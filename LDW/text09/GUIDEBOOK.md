# AI 면접 시뮬레이션 가이드북 (GUIDEBOOK)

## 1. 시뮬레이션 개요
본 프로젝트는 **웹 기반 AI 면접 시뮬레이션**으로, 사용자가 실제 면접과 유사한 환경에서 AI 면접관과 대화하며 면접 역량을 키울 수 있도록 돕습니다.
면접이 종료되면 AI가 지원자의 답변 내용, 태도 등을 종합적으로 분석하여 **루브릭(Rubric)** 기준에 따른 상세한 평가 리포트를 제공합니다.

### 주요 특징
- **실시간 대화**: STT(음성 인식)와 TTS(음성 합성) 기술을 활용하여 실제 사람과 대화하듯 면접을 진행합니다.
- **맞춤형 질문**: 지원한 직무와 자기소개서 내용을 바탕으로 생성형 AI가 심층 질문을 생성합니다.
- **루브릭 기반 평가**: 기술/직무, 문제해결, 의사소통, 태도/인성 4가지 영역을 5단계 척도로 정밀하게 평가합니다.
- **통합 분석 시스템**: 답변 내용뿐만 아니라 **영상 분석(MoveNet, DeepFace)**을 통해 지원자의 태도와 인성을 다각도로 분석하여 데이터베이스에 저장합니다.
- **웹 인터페이스**: 직관적인 웹 UI를 통해 누구나 쉽게 이용할 수 있습니다.

---

## 2. 시뮬레이션 환경
본 시뮬레이션은 다음과 같은 환경에서 실행되도록 개발되었습니다.

- **OS**: Windows (권장)
- **Python 버전**: Python 3.10 이상 권장
- **브라우저**: Chrome, Edge 등 최신 웹 브라우저
- **웹캠**: 영상 분석을 위한 필수 하드웨어
- **FFmpeg**: 오디오/비디오 처리를 위한 필수 라이브러리 (미설치 시 일부 분석 기능 제한)

---

## 3. 실행 방법
### 1단계: 가상환경 설정 및 패키지 설치
(최초 1회 실행)
```bash
# 가상환경 생성 (예시)
python -m venv venv

# 가상환경 활성화 (Windows)
venv\Scripts\activate

# 필수 패키지 설치
pip install -r requirements.txt
```

### 1-1단계: FFmpeg 설치 (필수)
`librosa` 및 오디오 분석 기능을 위해 **FFmpeg**가 필요합니다.
1. [FFmpeg 다운로드](https://www.gyan.dev/ffmpeg/builds/) 후 압축 해제
2. `bin` 폴더가 포함된 폴더를 `C:\ffmpeg` 로 이동 (즉, `C:\ffmpeg\bin\ffmpeg.exe` 가 되도록 설정)
3. **[자동 설정]** 본 시뮬레이션은 `C:\ffmpeg\bin` 경로를 자동으로 감지하여 시스템 경로에 추가합니다. 별도의 환경 변수 설정 없이도 `C:\ffmpeg` 위치에만 넣어두면 작동합니다.
4. 설치 확인: 포함된 `verify_ffmpeg_fix.py`를 실행하여 `[Success]` 메시지가 나오는지 확인하세요.

### 1-2단계: 환경 점검
설치가 잘 되었는지 확인하기 위해 검증 스크립트를 실행합니다.
```bash
python check_env.py
```

### 2단계: 서버 실행
`server.py` 파일을 실행하면 서버가 구동되고 자동으로 웹 브라우저가 열립니다.
```bash
python server.py
```
- 서버는 기본적으로 `http://localhost:5000` 에서 동작합니다.
- 브라우저가 자동으로 열리지 않을 경우, 주소창에 위 주소를 직접 입력하세요.

### 3단계: Docker를 이용한 실행 방법 (권장)
Docker가 설치되어 있다면, 다음 명령어로 간편하게 실행할 수 있습니다.

1. **컨테이너 빌드 및 실행**:
   ```bash
   docker-compose up --build
   ```
   > **주의사항**: 상위 폴더(`.../big20/`)에 `.env` 파일이 존재해야 정상적으로 실행됩니다. (API Key 등 필수 설정 포함)

2. **접속**:
   웹 브라우저를 열고 `http://localhost:5000`으로 접속합니다.
   (Docker 환경에서는 브라우저가 자동으로 실행되지 않을 수 있습니다.)
3. **종료**:
   `Ctrl + C`를 눌러 서버를 중지하거나, 다른 터미널에서 `docker-compose down`을 실행합니다.

---

## 4. 사용 모델 및 라이브러리 목록

## 4. 사용 모델 및 라이브러리 목록

### 핵심 기술 (AI & Backend)
- **FastAPI**: 고성능 비동기 웹 프레임워크 (백엔드 서버)
- **Google Gemini 2.0 Flash**: 면접 질문 생성, 답변 분석, 평가(LLM) 및 **음성 인식(STT)** 통합 모델 - **[NEW]** All-in-One AI 적용
- **LangChain**: LLM 오케스트레이션 및 프롬프트 관리
- **Uvicorn**: ASGI 웹 서버

### 음성 및 멀티미디어
- **Google Gemini (Multimodal)**: 고성능 음성 인식 (STT) - [유지]
- **OpenAI Whisper**: 음성 인식 (STT) 교차 검증용 모델 - **[NEW]** Gemini와 결과 비교
- **Python Levenshtein**: 두 STT 결과 간의 유사도 정밀 분석 **[NEW]**
- **Edge-TTS**: 자연스러운 음성 합성 (TTS)
- **PyAudio**: 오디오 입출력 처리
- **MoveNet Thunder (Google)**: 실시간 자세(Pose) 분석
- **DeepFace (Facebook) / OpenCV**: 표정 기반 감정 분석 (영상 분석 데이터로 활용)

### 데이터베이스 및 저장소
- **PostgreSQL / SQLite**: (설정에 따라) 면접 데이터 및 결과 저장
- **SQLAlchemy**: ORM(Object Relational Mapping) 데이터베이스 연동

---

## 5. 주요 기능 사용법

### [1] 로그인 및 직무 선택
1. 회원가입 후 로그인을 진행합니다.
2. 면접을 진행할 **직무(예: 웹 개발자, 마케팅 등)**를 선택하거나 입력합니다.
3. 자기소개서 파일을 업로드하거나 텍스트로 입력합니다.

### [2] 면접 진행
1. '면접 시작' 버튼을 누르면 AI 면접관이 첫 인사를 건넵니다.
2. **영상 분석**: 면접 진행 중 웹캠을 통해 사용자의 표정과 자세를 실시간으로 분석합니다. 이 데이터는 답변 제출 시 자동으로 서버에 전송되어 저장됩니다.
3. 마이크를 통해 답변을 말하면 AI가 이를 인식하고 꼬리물기 질문을 이어갑니다.
4. 설정된 질문 개수만큼 면접이 진행됩니다.

### [3] 결과 확인 (관리자/사용자)
1. 면접이 종료되면 잠시 후 **분석 결과 페이지**로 이동합니다.
2. **평가 루브릭**에 따라 4가지 항목(기술, 문제해결, 의사소통, 태도)에 대한 점수와 상세 피드백을 확인합니다.
3. **태도/인성 상세 분석**: 관리자는 지원자 상세 정보 페이지에서 `MoveNet`(자세) 및 `DeepFace`(표정) 분석 데이터를 포함한 종합 태도 평가 결과를 확인할 수 있습니다.
4. '합격/불합격' 여부와 개선점을 파악합니다.

---

## 6. 파일 구조 설명
```
C:\big20\big20_AI_Interview_simulation\LDW\text09\
├── app/
│   ├── main.py              # FastAPI 애플리케이션 진입점
│   ├── config.py            # 환경 변수 및 설정 관리 (Gemini 설정 추가)
│   ├── database.py          # 데이터베이스 연결 설정
│   ├── models.py            # Pydantic/SQLAlchemy 데이터 모델 정의
│   └── services/            # 핵심 비즈니스 로직
│       ├── analysis_service.py  # 면접 결과 분석 및 루브릭 평가 로직 (Gemini 적용) ★
│       ├── llm_service.py       # LLM 연동 (Gemini 2.0 Flash 적용) ★
│       ├── stt_service.py       # 음성 인식 (Gemini Multimodal 적용) ★ [NEW]
│       ├── tts_service.py       # 음성 합성
│       └── video_analysis_service.py # MoveNet, DeepFace 영상 분석 로직
├── static/                  # CSS, JS, 이미지 등 정적 파일
├── templates/               # HTML 템플릿 파일
├── requirements.txt         # 프로젝트 의존성 패키지 목록 (google-generativeai 추가)
├── server.py                # 서버 실행 및 브라우저 자동 실행 스크립트
├── check_env.py             # [NEW] 실행 환경(라이브러리, FFmpeg) 점검 스크립트
├── verify_fix.py            # [NEW] 오디오 처리 로직 검증 스크립트
├── scripts/                 # 유틸리티 스크립트 (모델 다운로드 등)
├── models/                  # AI 모델 저장소
├── tests/                   # 테스트 코드
│   └── test_gemini_integration.py # Gemini 연동 검증 스크립트
├── Dockerfile               # 도커 이미지 빌드 설정 파일
└── docker-compose.yml       # 도커 컨테이너 실행 설정 파일
```

---

## 7. 이번 작업으로 추가/변경된 기능
- **STT/LLM 모델 통합**: OpenAI Whisper 및 GPT-4o를 **Google Gemini 2.0 Flash**로 전면 교체하여 비용 효율성과 처리 속도를 개선했습니다.
- **음성 인식(STT) 강화**: Gemini Multimodal 기능을 활용하여 음성 파일의 유효성을 검사하고, 인식 실패 시 재시도하거나 명확한 에러 메시지를 반환하도록 개선했습니다.
- **질문 생성 로직 개선**: Gemini 2.0 Flash의 JSON 출력 안정성을 확보하기 위해 마크다운 정리 로직(`clean_json_string`)과 재시도 메커니즘을 추가했습니다.
- **Rate Limit 대응**: 무료 등급 사용 시 발생할 수 있는 할당량 초과(429 Error)에 대비하여 지수 백오프(Exponential Backoff) 기반의 재시도 로직을 구현했습니다.
- **테스트 스크립트 추가**: `scripts/test_stt_gemini.py` 및 `scripts/test_llm_gemini.py`를 통해 각 기능을 독립적으로 검증할 수 있습니다.
- **오디오 파일 저장 개선**: 면접 답변 음성 파일의 이름을 `YYYY-MM-DD-HH-MM-SS-{면접세션명}.webm` 형식으로 저장하여, 언제 어떤 면접에서 녹음된 파일인지 쉽게 식별할 수 있도록 개선했습니다.
- **STT 교차 검증 시스템 도입**: **Google Gemini (Multimodal)**와 **OpenAI Whisper**를 동시에 사용하여 음성 인식 정확도를 획기적으로 높였습니다. 두 모델의 인식 결과가 **95% 이상 일치(Levenshtein Distance)**할 경우에만 Gemini 결과를 채택하고, 그렇지 않을 경우 더 안정적인 Whisper 결과를 1순위로 사용하여 환각(Hallucination) 현상을 최소화했습니다.
- **비디오 심층 분석 통합**: `uploads/audio` 폴더에 저장된 `.webm` 영상 파일을 면접 종료 시 자동으로 스캔하여 **OpenCV**, **MoveNet Thunder**, **DeepFace**로 정밀 분석합니다. 프레임 단위로 감정과 자세를 분석하여 최종 태도/인성 평가에 반영합니다.
- **오디오 심층 분석 시스템 (Audio Deep Analysis)** **[NEW]**: 지원자의 목소리 데이터를 정밀 분석하여 비언어적 요소를 평가에 반영합니다.
    - **무음(Silence) 길이 측정**: 답변 도중의 침묵 시간을 측정하여 당황 여부를 판단합니다.
    - **Noise Reduction & Normalization**: `noisereduce`로 화이트 노이즈를 제거하고, 오디오 볼륨을 일정 수준으로 증폭하여 인식률을 극대화합니다.
    - **성량 및 자신감 (RMS Energy)**: `Librosa`를 활용해 목소리 크기의 변화를 분석하여 자신감을 측정합니다.
    - **목소리 떨림 (Pitch Jitter & Shimmer)**: `Parselmouth`를 사용하여 목소리의 미세한 떨림과 진폭 변동을 수치화해 긴장도를 분석합니다.
    - **말하기 속도 (Speech Rate)**: 전사된 글자 수와 시간을 비교하여 말이 너무 빠르거나 느린지 판단합니다.
- **STT 전사 로직 고도화** **[NEW]**:
    - `Whisper`와 `Gemini` 모두 **Temperature 0.0**으로 설정하여 일관성을 확보했습니다.
    - "음...", "어..." 같은 텍스트를 일부러 보존하도록 프롬프트를 강화하여 비언어적 습관까지 파악할 수 있게 했습니다.
    - **스마트 선택 로직**: 두 STT 모델의 결과 유사도가 낮을 경우(< 95%), LLM이 오디오 맥락상 더 자연스러운 텍스트를 스스로 판단하여 선택합니다.
    - **안정성 강화 (Bug Fix)**: 오디오 전처리 과정에서 오류가 발생하더라도 면접이 중단되지 않도록 예외 처리 로직을 강화했습니다. 특히 `librosa` 라이브러리의 오디오 길이 분석 시 발생할 수 있는 충돌을 방지하여 안정적인 답변 제출이 가능합니다.
- **오디오 처리 안정성 강화** **[NEW]**: 
    - `FFmpeg`가 설치되지 않은 환경에서도 서버가 중단되지 않도록 예외 처리(Fallback) 로직을 추가했습니다.
    - `check_env.py`를 통해 사용자가 손쉽게 실행 환경을 진단할 수 있습니다.
- **오디오 로딩 및 처리 오류 수정 (Critical Fix)** **[NEW]**:
    - **포맷 호환성 문제 해결**: `soundfile` 라이브러리가 `.webm` 저장을 지원하지 않아 발생하는 "No format specified" 오류를 해결하기 위해, 전처리된 중간 파일 포맷을 `.wav`로 변경했습니다.
    - **오디오 로딩 안정성 강화**: `librosa.load`가 실패할 경우 `soundfile`로 직접 읽기를 시도하는 2중 안전장치(Fallback)를 구현하여 오디오 파일 로딩 성공률을 높였습니다.
    - **FFmpeg 경로 자동 감지**: `C:\ffmpeg\bin` 경로를 자동으로 시스템 경로에 추가하여, FFmpeg 의존성이 있는 라이브러리들이 정상 작동하도록 보장합니다.
- **무한 로딩 방지 시스템 도입 (Critical Update)** **[NEW]**:
    - **API 타임아웃 적용**: Google Gemini 및 OpenAI API 호출 시 30초 타임아웃을 강제 적용하여, 네트워크 지연으로 인해 화면이 무한정 멈추는 현상을 원천 차단했습니다.
    - **예외 처리 강화**: 답변 분석 중 예상치 못한 오류가 발생하더라도 사용자에게 명확한 에러 메시지를 표시하고, 다음 질문으로 안전하게 넘어갈 수 있도록 복구 로직을 구현했습니다.


