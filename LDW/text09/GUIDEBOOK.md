# AI 면접 시뮬레이션 가이드북 (GUIDEBOOK)

## 1. 시뮬레이션 개요
본 프로젝트는 **웹 기반 AI 면접 시뮬레이션**으로, 사용자가 실제 면접과 유사한 환경에서 AI 면접관과 대화하며 면접 역량을 키울 수 있도록 돕습니다.
면접이 종료되면 AI가 지원자의 답변 내용, 태도 등을 종합적으로 분석하여 **루브릭(Rubric)** 기준에 따른 상세한 평가 리포트를 제공합니다.

### 주요 특징
- **실시간 대화**: STT(음성 인식)와 TTS(음성 합성) 기술을 활용하여 실제 사람과 대화하듯 면접을 진행합니다.
- **맞춤형 질문**: 지원한 직무와 자기소개서 내용을 바탕으로 생성형 AI가 심층 질문을 생성합니다.
- **루브릭 기반 평가**: 기술/직무, 문제해결, 의사소통, 태도/인성 4가지 영역을 5단계 척도로 정밀하게 평가합니다.
- **통합 분석 시스템**: 답변 내용뿐만 아니라 **영상 분석(MoveNet, DeepFace)**을 통해 지원자의 태도와 인성을 다각도로 분석하여 데이터베이스에 저장합니다.
- **웹 인터페이스**: 직관적인 웹 UI를 통해 누구나 쉽게 이용할 수 있습니다.

---

## 2. 시뮬레이션 환경
본 시뮬레이션은 다음과 같은 환경에서 실행되도록 개발되었습니다.

- **OS**: Windows (권장)
- **Python 버전**: Python 3.10 이상 권장
- **브라우저**: Chrome, Edge 등 최신 웹 브라우저
- **웹캠**: 영상 분석을 위한 필수 하드웨어
- **FFmpeg**: 오디오/비디오 처리를 위한 필수 라이브러리 (미설치 시 일부 분석 기능 제한)

---

## 3. 실행 방법
### 1단계: 가상환경 설정 및 패키지 설치
(최초 1회 실행)
```bash
# 가상환경 생성 (예시)
python -m venv venv

# 가상환경 활성화 (Windows)
venv\Scripts/activate

# 필수 패키지 설치 (numpy 1.26.2, pyannote-audio 3.1.1 고정 설치)
pip install -r requirements.txt
```

### 1-1단계: FFmpeg 설치 (필수)
`librosa` 및 오디오 분석 기능을 위해 **FFmpeg**가 필요합니다.
1. [FFmpeg 다운로드](https://www.gyan.dev/ffmpeg/builds/) 후 압축 해제
2. `bin` 폴더가 포함된 폴더를 `C:\ffmpeg` 로 이동 (즉, `C:\ffmpeg\bin\ffmpeg.exe` 가 되도록 설정)
3. **[자동 설정]** 본 시뮬레이션은 `C:\ffmpeg\bin` 경로를 자동으로 감지하여 시스템 경로에 추가합니다. 별도의 환경 변수 설정 없이도 `C:\ffmpeg` 위치에만 넣어두면 작동합니다.
4. 설치 확인: 포함된 `tests/manual_verification/verify_ffmpeg_fix.py`를 실행하여 `[Success]` 메시지가 나오는지 확인하세요.

### 1-2단계: 환경 점검
설치가 잘 되었는지 확인하기 위해 검증 스크립트를 실행합니다.
```bash
python scripts/check_env.py
```
결과에서 `[ERROR]` 항목이 없으면 정상입니다. `[WARN]` 항목은 해당 기능을 사용할 때만 문제가 됩니다.

### 1-3단계: 테스트 계정 초기화 (최초 1회)
PostgreSQL이 실행 중인 상태에서 아래 명령어로 테스트 계정을 생성합니다.
```bash
python scripts/setup_test_user.py
```
생성되는 계정:
| 구분 | ID | PW |
|------|----|---------|
| 지원자(시험용) | `test` | `test` |
| 관리자 | `admin` | `admin` |

### 2단계: 서버 실행
`server.py` 파일을 실행하면 서버가 구동되고 자동으로 **시스템 기본 웹 브라우저**가 열립니다.
(보안 및 성능을 위해 127.0.0.1:8000 접속을 기본으로 합니다.)
`server.py`는 서버 구동과 웹 브라우저 자동 실행 기능에만 집중하여 최적화되어 있으며, 모든 주석이 한국어로 제공됩니다.
```bash
python server.py
```
- 서버는 기본적으로 `http://127.0.0.1:8000` 에서 동작합니다.
- 브라우저가 자동으로 열리지 않을 경우, 주소창에 위 주소를 직접 입력하세요.

### 3단계: Docker를 이용한 실행 방법 (권장)
Docker가 설치되어 있다면, 다음 명령어로 간편하게 실행할 수 있습니다.

1. **컨테이너 빌드 및 실행**:
   ```bash
   docker-compose up --build
   ```
   > **주의사항**: 상위 폴더(`.../big20/`)에 `.env` 파일이 존재해야 정상적으로 실행됩니다. (API Key 등 필수 설정 포함)

2. **접속**:
   웹 브라우저를 열고 `http://localhost:5000`으로 접속합니다.
   (Docker 환경에서는 브라우저가 자동으로 실행되지 않을 수 있습니다.)
3. **종료**:
   `Ctrl + C`를 눌러 서버를 중지하거나, 다른 터미널에서 `docker-compose down`을 실행합니다.

---

## 4. 사용 모델 및 라이브러리 목록

### 핵심 기술 (AI & Backend)
- **FastAPI**: 고성능 비동기 웹 프레임워크 (백엔드 서버)
- **Google Gemini 2.0 Flash**: 면접 질문 생성, 답변 분석, 평가(LLM) 및 **음성 인식(STT)** 통합 모델 - **[NEW]** All-in-One AI 적용
- **LangChain**: LLM 오케스트레이션 및 프롬프트 관리
- **Uvicorn**: ASGI 웹 서버
- **numpy==1.26.2**: 특정 버전 고정 (바이너리 호환성 해결) **[UPDATED]**
- **pyannote-audio==3.1.1**: 특정 버전 고정 (의존성 충돌 해결) **[UPDATED]**
- **scipy==1.12.0**: numpy 1.26.2 ABI 호환 버전으로 다운그레이드 **[UPDATED]**
- **tensorflow==2.15.1**: numpy 1.26.2 바이너리 호환 버전으로 다운그레이드 **[UPDATED]**
- **onnx==1.15.0 / onnxruntime==1.17.1**: ml_dtypes 호환성 문제 해결을 위해 버전 고정 **[NEW]**

### 음성 및 멀티미디어
- **Google Gemini (Multimodal)**: 고성능 음성 인식 (STT) - [유지]
- **OpenAI Whisper**: 음성 인식 (STT) 교차 검증용 모델 - **[NEW]** Gemini와 결과 비교
- **Python Levenshtein**: 두 STT 결과 간의 유사도 정밀 분석 **[NEW]**
- **Edge-TTS**: 자연스러운 음성 합성 (TTS)
- **PyAudio**: 오디오 입출력 처리
- **MoveNet Thunder (Google)**: 실시간 자세(Pose) 분석
- **DeepFace (Facebook) / OpenCV**: 표정 기반 감정 분석 (영상 분석 데이터로 활용)

### 데이터베이스 및 저장소
- **PostgreSQL / SQLite**: (설정에 따라) 면접 데이터 및 결과 저장
- **SQLAlchemy**: ORM(Object Relational Mapping) 데이터베이스 연동

---

## 5. 주요 기능 사용법

### [1] 로그인 및 직무 선택
1. 회원가입 후 로그인을 진행합니다. (시험용 계정: `test` / `test`)
2. 면접을 진행할 **직무(예: 웹 개발자, 마케팅 등)**를 선택하거나 입력합니다.
3. 자기소개서 파일을 업로드하거나 텍스트로 입력합니다.

### [2] 면접 진행
1. '면접 시작' 버튼을 누르면 이력서 업로드 및 **환경 테스트(카메라, 마이크, 오디오)** 단계가 진행됩니다. **[UPDATED]**
2. **환경 테스트**: 카메라, 마이크뿐만 아니라 오디오(스피커) 연결 상태를 자동으로 확인하며, 연결된 실제 기기명을 '정상' 표시 옆에 `(기기명)` 형식으로 친절하게 알려줍니다. 모든 기기가 정상적으로 연결되어야 면접을 시작할 수 있습니다.
3. **화면 구성**: 면접관의 얼굴 위치를 고정하여 안정적인 면접 환경을 제공합니다. 면접관 얼굴 아래의 질문 말풍선은 제거되었으며, 질문 내용은 하단의 채팅 로그에서 실시간으로 확인할 수 있습니다.
4. **영상 분석**: 면접 진행 중 웹캠을 통해 사용자의 표정과 자세를 실시간으로 분석합니다. 이 데이터는 답변 제출 시 자동으로 서버에 전송되어 저장됩니다.
5. 마이크를 통해 답변을 말하면 AI가 이를 인식하고 꼬리물기 질문을 이어갑니다.
6. 설정된 질문 개수만큼 면접이 진행됩니다.

### [3] 결과 확인 (관리자/사용자)
1. 면접이 종료되면 잠시 후 **분석 결과 페이지**로 이동합니다.
2. **평가 루브릭**에 따라 4가지 항목(기술, 문제해결, 의사소통, 태도)에 대한 점수와 상세 피드백을 확인합니다.
3. **태도/인성 상세 분석**: 관리자는 지원자 상세 정보 페이지에서 `MoveNet`(자세) 및 `DeepFace`(표정) 분석 데이터를 포함한 종합 태도 평가 결과를 확인할 수 있습니다.
### [4] ID/PW 찾기 기능 **[UPDATED]**
1. 로그인 화면에서 'ID/PW 찾기' 링크를 클릭합니다.
2. **ID 찾기**: 이름과 이메일 주소를 입력하면 가입된 모든 아이디 목록을 확인할 수 있습니다. 검색 결과가 여러 개인 경우 모두 표시되며, 일정 시간 후 로그인 화면으로 돌아갑니다.
3. **PW 찾기**: 아이디를 입력하면 해당 계정의 이메일로 4자리 인증번호가 발송(시뮬레이션)됩니다. 발송된 인증번호를 입력하면 비밀번호(pw)를 즉시 확인할 수 있습니다.
---

## 6. 파일 구조 설명
```
C:\big20\big20_AI_Interview_simulation\LDW\text09\
├── app/                     # FastAPI 애플리케이션 진입점 및 로직
├── data/                    # 모델 입력 데이터, 백업 및 보관 데이터 (archive 포함)
├── db/                      # 데이터베이스 초기화 및 설정
├── db_data/                 # 데이터베이스 영구 데이터 저장소
├── LivePortrait/            # 실시간 포션 애니메이션 엔진
├── migration_package/       # DB 마이그레이션 패키지
├── models/                  # AI 모델 저장소
├── SadTalker/               # 아바타 생성 엔진
├── scripts/                 # 유틸리티 및 관리 스크립트
├── static/                  # 정적 파일 (HTML, JS, CSS)
├── test_uploads/            # 테스트용 업로드 파일
├── tests/                   # 기능 검증 및 유닛 테스트
├── uploads/                 # 실제 업로드 파일 저장소
├── Wav2Lip/                 # 립싱크 생성 엔진
├── docker-compose.yml       # 도커 오케스트레이션
├── Dockerfile               # 도커 빌드 설정
├── GUIDEBOOK.md             # 프로젝트 가이드북
├── requirements.txt         # 패키지 의존성 목록 (numpy==1.26.2, pyannote-audio==3.1.1 고정, 2026-02-24 최신화)
└── server.py                # 서버 실행 및 브라우저 자동 실행 (한국어 주석 적용)
```

---

## 7. 최근 추가/변경된 기능
- **STT/LLM 모델 통합**: OpenAI Whisper 및 GPT-4o를 **Google Gemini 2.0 Flash**로 전면 교체하여 비용 효율성과 처리 속도를 개선했습니다.
- **음성 인식(STT) 강화**: Gemini Multimodal 기능을 활용하여 음성 파일의 유효성을 검사하고, 인식 실패 시 재시도하거나 명확한 에러 메시지를 반환하도록 개선했습니다.
- **질문 생성 로직 개선**: Gemini 2.0 Flash의 JSON 출력 안정성을 확보하기 위해 마크다운 정리 로직(`clean_json_string`)과 재시도 메커니즘을 추가했습니다.
- **Rate Limit 대응**: 무료 등급 사용 시 발생할 수 있는 할당량 초과(429 Error)에 대비하여 지수 백오프(Exponential Backoff) 기반의 재시도 로직을 구현했습니다.
- **테스트 스크립트 정리**: `scripts/check_env.py` 및 `tests/manual_verification/` 하위 스크립트를 통해 각 기능을 독립적으로 검증할 수 있습니다.
- **오디오 파일 저장 개선**: 면접 답변 음성 파일의 이름을 `YYYY-MM-DD-HH-MM-SS-{면접세션명}.webm` 형식으로 저장하여, 언제 어떤 면접에서 녹음된 파일인지 쉽게 식별할 수 있도록 개선했습니다.
- **STT 교차 검증 시스템 도입**: **Google Gemini (Multimodal)**와 **OpenAI Whisper**를 동시에 사용하여 음성 인식 정확도를 획기적으로 높였습니다. 두 모델의 인식 결과가 **95% 이상 일치(Levenshtein Distance)**할 경우에만 Gemini 결과를 채택하고, 그렇지 않을 경우 더 안정적인 Whisper 결과를 1순위로 사용하여 환각(Hallucination) 현상을 최소화했습니다.
- **비디오 심층 분석 통합**: `uploads/audio` 폴더에 저장된 `.webm` 영상 파일을 면접 종료 시 자동으로 스캔하여 **OpenCV**, **MoveNet Thunder**, **DeepFace**로 정밀 분석합니다. 프레임 단위로 감정과 자세를 분석하여 최종 태도/인성 평가에 반영합니다.
- **오디오 심층 분석 시스템 (Audio Deep Analysis)** **[NEW]**: 지원자의 목소리 데이터를 정밀 분석하여 비언어적 요소를 평가에 반영합니다.
    - **초고속 무음 감지 (RMS & VAD)**: 전체 오디오의 에너지(RMS)와 사람 목소리 비율(WebRTC VAD)을 0.01초 내에 분석하여, 의미 없는 침묵이나 백색 소음만 있는 경우 즉시 "답변 없음"으로 처리합니다. 불필요한 STT 비용을 절감하고 빠른 피드백을 제공합니다.
    - **무음(Silence) 길이 측정**: 답변 도중의 침묵 시간을 측정하여 당황 여부를 판단합니다.
    - **Noise Reduction & Normalization**: `noisereduce`로 화이트 노이즈를 제거하고, 오디오 볼륨을 일정 수준으로 증폭하여 인식률을 극대화합니다.
    - **성량 및 자신감 (RMS Energy)**: `Librosa`를 활용해 목소리 크기의 변화를 분석하여 자신감을 측정합니다.
    - **목소리 떨림 (Pitch Jitter & Shimmer)**: `Parselmouth`를 사용하여 목소리의 미세한 떨림과 진폭 변동을 수치화해 긴장도를 분석합니다.
    - **말하기 속도 (Speech Rate)**: 전사된 글자 수와 시간을 비교하여 말이 너무 빠르거나 느린지 판단합니다.
- **STT 전사 로직 고도화** **[NEW]**:
    - `Whisper`와 `Gemini` 모두 **Temperature 0.0**으로 설정하여 일관성을 확보했습니다.
    - "음...", "어..." 같은 텍스트를 일부러 보존하도록 프롬프트를 강화하여 비언어적 습관까지 파악할 수 있게 했습니다.
    - **스마트 선택 로직**: 두 STT 모델의 결과 유사도가 낮을 경우(< 95%), **OpenAI Whisper 결과를 우선적으로 선택**하여 인식의 안정성과 간투사 보존율을 높였습니다. (기존 LLM 판단 로직에서 변경)
    - **안정성 강화 (Bug Fix)**: 오디오 전처리 과정에서 오류가 발생하더라도 면접이 중단되지 않도록 예외 처리 로직을 강화했습니다. 특히 `librosa` 라이브러리의 오디오 길이 분석 시 발생할 수 있는 충돌을 방지하여 안정적인 답변 제출이 가능합니다.
- **오디오 처리 안정성 강화** **[NEW]**: 
    - `FFmpeg`가 설치되지 않은 환경에서도 서버가 중단되지 않도록 예외 처리(Fallback) 로직을 추가했습니다.
    - `check_env.py`를 통해 사용자가 손쉽게 실행 환경을 진단할 수 있습니다.
- **오디오 로딩 및 처리 오류 수정 (Critical Fix)** **[NEW]**:
- **의존성 바이너리 호환성 해결 (2026-02-24)**: `ValueError: numpy.dtype size changed` 문제를 해결하기 위해 `numpy==1.26.2`와 `pyannote-audio==3.1.1` 버전을 고정하고, 추가로 `scipy==1.12.0`, `tensorflow==2.15.1`, `tensorflow-hub==0.15.0`, `tf-keras==2.15.1`, `onnx==1.15.0`, `onnxruntime==1.17.1`로 다운그레이드하여 전체 의존성 호환성을 확보했습니다.
- **서버 실행 스크립트 최적화**: `server.py`를 서버 구동 및 자동 브라우저 오픈 기능 전용으로 개편하고 한국어 주석을 추가했습니다.
- **프로젝트 구조 정량화**: 루트 디렉토리를 정리하여 관리 효율성을 높였습니다. (`diagnose_numpy.py` → `scripts/` 이동)

### 신규 업데이트 (STT 성능 개선 및 구조 최적화) **[2026-02-24]**
- **STT 전사 로직 고도화**: 유사도가 0.95 미만일 경우 Whisper 결과를 우선 사용하도록 로직을 단순화 및 강화했습니다.
- **노이즈 제거 기능 삭제**: 원본 음성의 뉘앙스와 작은 소리를 보존하기 위해 `noisereduce` 라이브러리를 통한 전처리를 제거했습니다.
- **전처리 최적화**: 오디오 품질(RMS)이 충분히 좋을 경우 정규화만 수행하고 변형을 최소화하는 로직을 적용했습니다.
- **감도 및 신호 탐색 개선**: 
    - RMS 임계값을 **0.001**로 하향 조정하여 아주 작은 목소리도 인식 가능하게 했습니다.
    - 특정 구간이 아닌 전체 구간에서 유의미한 신호를 찾는 방식으로 로직을 수정했습니다.
- **Gemini 프롬프트 절대 규칙 추가**: "어...", "그게..."와 같은 말더듬과 비문을 교정하지 않고 들리는 그대로 전사하도록 프롬프트를 강화했습니다.
- **Whisper 환각 필터 강화**: 특정 방송용 문구("MBC 뉴스" 등)가 포함된 짧은 텍스트를 더 엄격하게 필터링합니다.
- **의존성 최적화 및 고정**: `requirements.txt`에서 `noisereduce`를 제외하고, 요청된 `numpy==1.26.2` 및 `pyannote-audio==3.1.1` 버전을 고정하여 바이너리 호환성 문제를 해결했습니다.
- **서버 실행 스크립트 리팩토링**: `server.py`를 서버 구동 및 자동 웹 브라우저 실행 기능만 남겨 최적화하고, 모든 주석을 한국어로 변경했습니다.
- **루트 디렉토리 정리**: 프로젝트 루트에는 실행 필수 파일(`app`, `data`, `server.py` 등)만 남기고 나머지 작업 폴더들을 하위로 정리하여 관리 효율성을 높였습니다. `archive` 폴더는 `data` 하위로 이동되었습니다.
- **인터뷰 UI 개선 (2026-02-24)**: 면접관 얼굴 위치를 고정하고 얼굴 아래의 질문 텍스트 상자를 제거하여 시각적 복잡도를 낮췄습니다. 질문은 채팅 로그를 통해 확인할 수 있습니다.
- **환경 테스트 강화 (2026-02-24)** **[NEW]**: 
    - 면접 시작 전 '2. 환경 테스트' 단계에서 기존 카메라, 마이크 외에 **오디오(스피커)** 연결 확인 항목을 추가했습니다.
    - 현재 연결된 기기가 무엇인지 '정상' 표시 바로 옆에 `(기기명)` 형태로 표시하여 사용자가 어떤 장치로 면접을 진행하는지 명확히 인지할 수 있도록 개선했습니다.
    - 모든 기기가 정상적으로 탐지되어야 면접 시작 버튼이 활성화되도록 안정성을 높였습니다.

### 신규 업데이트 (관리자 공고 폼 구조 개선) **[NEW]**
- **관리자 공고 등록 필드 고도화**: 공고 등록 및 수정 시 기존 '직무 (Job)', '공고 내용', '마감일' 텍스트를 '채용 직무', '주요업무', '공고 마감일'로 각각 변경하여 직관성을 높였습니다.
- **상세 정보 입력란 추가**: '자격 요건', '우대사항', '복리후생', '채용절차', '채용인원' 총 5개의 새로운 항목을 입력 가능토록 추가하여, 데이터베이스(`interview_announcement`)에 상세 직무 관련 정보를 분리 저장하고 조회할 수 있습니다.

### 신규 업데이트 (이력서 시각화 및 구조화) **[NEW]**
- **이력서 미리보기(썸네일) 및 원본 보기(모달)**: '1. 이력서 업로드' 중 파일 선택 즉시 사용자 화면 우측에 이력서를 작은 크기(썸네일 형태)로 간편하게 미리 볼 수 있습니다. 썸네일을 클릭하면 원본 크기로 큰 모달 창을 띄워 상세히 확인할 수 있어 사용성이 개선되었습니다.
- **루트 디렉토리 정리**: `create_pdf.py` 및 `test_resume.pdf`와 같은 부가 스크립트와 파일들을 각각 `scripts/`, `test_uploads/` 폴더 등으로 옮겨 폴더 구조가 더 직관적으로 정리되었습니다.

### 신규 업데이트 (비디오 생성 안정성 개선) **[2026-02-24]**
- **비디오 생성 경로 설정 중앙화 (Critical Fix)**: `app/config.py`에 `WAV2LIP_OUTPUT_FOLDER`, `WAV2LIP_DIR`, `WAV2LIP_INFERENCE_SCRIPT`, `WAV2LIP_CHECKPOINT`, `WAV2LIP_FACE_IMAGE` 등 비디오 생성 관련 경로를 중앙 관리하도록 추가했습니다. 기존에는 `video_gen_service.py`에서 하드코딩된 경로를 사용하여 경로 불일치로 인한 비디오 생성 실패가 발생할 수 있었습니다.
- **비디오/오디오 폴백 로직 개선**: `tts_service.py`의 반환값을 `{"url": ..., "type": "video"|"audio"}` dict 형태로 변경하여, 프론트엔드가 비디오 생성 성공/실패 여부를 명확히 구분할 수 있게 했습니다.
- **프론트엔드 재생 안정화**: `app.js`의 `playAudio()` 함수를 수정하여, `.mp4` 파일은 `<video>` 태그로, `.mp3` 파일은 `<audio>` 요소로 재생합니다. 기존에는 비디오 생성 실패 시 mp3 파일을 `<video>` 태그에 로드하여 재생 오류가 발생했습니다.
- **인터뷰 라우터 응답 확장**: API 응답에 `audio_type` 필드를 추가하여 클라이언트가 미디어 타입을 인지하고 적절한 재생 방식을 선택할 수 있습니다.
- **의존성 최신화**: FastAPI 0.131.0, Uvicorn 0.41.0, Pydantic 2.12.5, SQLAlchemy 2.0.46 등 주요 라이브러리를 2026년 2월 기준 최신 안정 버전으로 업데이트했습니다. (`numpy==1.26.2`, `pyannote-audio==3.1.1` 고정 유지)

---

## 8. 데이터 백업 및 복원 (데이터 이관)

본 시뮬레이션은 데이터베이스에 저장된 면접 기록, 사용자 정보 등을 파일로 저장(내보내기)하고, 이를 다른 컴퓨터나 환경으로 복원(가져오기)할 수 있는 스크립트를 제공합니다.

### 8.1 데이터 내보내기 (Export)
현재 실행 중인 컨테이너 또는 로컬 데이터베이스의 데이터를 JSON 파일로 저장합니다.

1. **실행 방법**:
   ```bash
   python scripts/export_db.py
   ```
2. **결과**:
   - `data/interview_db_backup.json` 파일이 생성됩니다.
   - 이 파일에는 사용자 정보, 면접 기록, 채점 결과 등이 포함됩니다.
   - **주의**: 업로드된 파일(이력서, 오디오 등)은 `uploads` 폴더에 별도로 저장되므로, 이 폴더를 수동으로 복사해야 완벽한 백업이 가능합니다.

### 8.2 데이터 가져오기 (Import)
백업된 JSON 데이터를 새로운 데이터베이스에 복원합니다.

1. **준비 사항**:
   - `data/interview_db_backup.json` 파일을 새로운 환경의 `data` 폴더에 위치시킵니다.
   - 데이터베이스가 실행 중이어야 합니다.
2. **실행 방법**:
   ```bash
   python scripts/import_db.py
   ```
3. **결과**:
   - 기존 데이터를 유지하면서 백업된 데이터를 추가합니다.
   - 중복된 데이터(Primary Key 기준)는 무시하고 새로운 데이터만 추가됩니다 (ON CONFLICT DO NOTHING).

### 8.3 데이터 이관 시 주의사항
- **파일 이동**: 다른 컴퓨터로 이관할 경우, `data/interview_db_backup.json` 파일뿐만 아니라 `uploads/` 폴더 전체를 함께 이동시키는 것을 권장합니다.
- **환경 변수**: `scripts/.env` 또는 프로젝트 루트의 `.env` 파일 설정(DB 접속 정보 등)이 올바른지 확인하세요.

---

## 9. Wav2Lip 기반 립싱크 비디오 연동 (업데이트 노트)
본 프로젝트는 기존의 정적 AI 아바타에서 벗어나, 사용자의 답변과 TTS 오디오에 맞춰 동적으로 발화하는 **Wav2Lip-GAN** 기반 비디오 렌더링 기능을 지원합니다.

### 9.1 주요 통합 내용
*   **비디오 출력 지원**: 프론트엔드(`static/index.html` 및 `static/app.js`)의 비디오 플레이어(`<video id="ai-video">`)를 통해 256x256 크기의 립싱크 비디오를 지원합니다.
*   **Wav2Lip 비디오 생성 모듈**: `app/services/video_gen_service.py`를 통해 TTS 오디오(`uploads/tts_audio/*.mp3`)와 지정된 이미지(`data/man.png`)를 합성한 후 `uploads/Wav2Lip_mp4/*.mp4`를 생성합니다. 경로 설정은 `app/config.py`에서 `WAV2LIP_OUTPUT_FOLDER`, `WAV2LIP_CHECKPOINT`, `WAV2LIP_FACE_IMAGE` 등으로 중앙 관리됩니다. **[UPDATED]**
*   **비디오/오디오 자동 폴백**: 비디오 생성에 실패할 경우 TTS 오디오(mp3)만 재생하며, 프론트엔드가 `.mp4`/`.mp3` 확장자를 자동 감지하여 적절한 방식(`<video>` 또는 `<audio>`)으로 재생합니다. **[NEW]**
*   **의존성 최신화**: 립싱크 처리에 필요한 `ffmpeg-python`, `moviepy`, `tqdm`, `torchvision`, `torchaudio`, `librosa` 모듈이 요구사항(`requirements.txt`)에 반영되었습니다.

### 9.2 참고 사항
* 해당 기능을 원활하게 구동하기 위해서는 사전에 `C:\big20\big20_AI_Interview_simulation\LDW\text09\Wav2Lip\checkpoints` 내에 `wav2lip_gan.pth` 가중치 파일과 모델 구조가 필요합니다.
* 실행 전 반드시 `pip install ffmpeg-python moviepy librosa torchvision torchaudio`가 완료되어 있는지 점검하십시오.



