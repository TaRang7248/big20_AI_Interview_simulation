# AI 면접 시뮬레이션 가이드북 (GUIDEBOOK)

## 1. 시뮬레이션 개요
본 프로젝트는 **웹 기반 AI 면접 시뮬레이션**으로, 사용자가 실제 면접과 유사한 환경에서 AI 면접관과 대화하며 면접 역량을 키울 수 있도록 돕습니다.
면접이 종료되면 AI가 지원자의 답변 내용, 태도 등을 종합적으로 분석하여 **루브릭(Rubric)** 기준에 따른 상세한 평가 리포트를 제공합니다.

### 주요 특징
- **실시간 대화**: STT(음성 인식)와 TTS(음성 합성) 기술을 활용하여 실제 사람과 대화하듯 면접을 진행합니다.
- **맞춤형 질문**: 지원한 직무와 자기소개서 내용을 바탕으로 생성형 AI가 심층 질문을 생성합니다.
- **루브릭 기반 평가**: 기술/직무, 문제해결, 의사소통, 태도/인성 4가지 영역을 5단계 척도로 정밀하게 평가합니다.
- **통합 분석 시스템**: 답변 내용뿐만 아니라 **영상 분석(MoveNet, DeepFace)**을 통해 지원자의 태도와 인성을 다각도로 분석하여 데이터베이스에 저장합니다.
- **웹 인터페이스**: 직관적인 웹 UI를 통해 누구나 쉽게 이용할 수 있습니다.

---

## 4. 주요 업데이트 내용 (2026-02-25)

### 4.1 오디오 및 미디어 장치 고정 기능 개선
면접 시작 시 오디오/비디오 기기가 예기치 않게 변경되는 문제를 해결하기 위해 다음과 같은 기능이 추가되었습니다.
- **기기 정보 저장**: [환경 테스트] 단계에서 사용자가 확인한 카메라, 마이크, 스피커의 장치 ID를 브라우저 세션 동안 저장합니다.
- **면접 적용**: 면접이 시작될 때 저장된 장치 ID를 사용하여 녹음 및 재생 장치를 강제로 고정합니다.
- **스피커(오디오 출력) 고정**: 지원 브라우저에서 `setSinkId`를 통해 사용자가 선택한 스피커로 AI의 질문이 출력되도록 보장합니다.

### 4.2 서버 스크립트 최적화
- `server.py`는 서버 구동 및 웹 브라우저 자동 실행이라는 본연의 기능에 충실하도록 코드를 정리하고 모든 안내 메시지를 한국어로 표준화하였습니다.

### 4.3 코드베이스 주석 전면 한글화
- 백엔드우터(`app/routers/`), 서비스(`app/services/`), 핵심 설정 파일(`main.py`, `models.py` 등) 및 프론트엔드(`static/app.js`, `index.html`, `styles.css`)의 모든 영문 주석을 한국어로 번역하여 시스템 가독성과 유지보수성을 대폭 향상시켰습니다.

---

## 2. 시뮬레이션 환경
본 시뮬레이션은 다음과 같은 환경에서 실행되도록 개발되었습니다.

- **OS**: Windows (권장)
- **Python 버전**: Python 3.10 이상 권장
- **브라우저**: Chrome, Edge 등 최신 웹 브라우저
- **웹캠**: 영상 분석을 위한 필수 하드웨어
- **FFmpeg**: 오디오/비디오 처리를 위한 필수 라이브러리 (미설치 시 일부 분석 기능 제한)

---

## 3. 실행 방법
### 1단계: 가상환경 설정 및 패키지 설치
(최초 1회 실행)
```bash
# 가상환경 생성 (예시)
python -m venv venv

# 가상환경 활성화 (Windows)
venv\Scripts/activate

# 필수 패키지 설치 (numpy 1.26.2, pyannote-audio 3.1.1 고정 설치)
pip install -r requirements.txt
```

### 1-1단계: FFmpeg 설치 (필수)
`librosa` 및 오디오 분석 기능을 위해 **FFmpeg**가 필요합니다.
1. [FFmpeg 다운로드](https://www.gyan.dev/ffmpeg/builds/) 후 압축 해제
2. `bin` 폴더가 포함된 폴더를 `C:\ffmpeg` 로 이동 (즉, `C:\ffmpeg\bin\ffmpeg.exe` 가 되도록 설정)
3. **[자동 설정]** 본 시뮬레이션은 `C:\ffmpeg\bin` 경로를 자동으로 감지하여 시스템 경로에 추가합니다. 별도의 환경 변수 설정 없이도 `C:\ffmpeg` 위치에만 넣어두면 작동합니다.
4. 설치 확인: 포함된 `tests/manual_verification/verify_ffmpeg_fix.py`를 실행하여 `[Success]` 메시지가 나오는지 확인하세요.

### 1-2단계: 환경 점검
설치가 잘 되었는지 확인하기 위해 검증 스크립트를 실행합니다.
```bash
python scripts/check_env.py
```
결과에서 `[ERROR]` 항목이 없으면 정상입니다. `[WARN]` 항목은 해당 기능을 사용할 때만 문제가 됩니다.

### 1-3단계: 테스트 계정 초기화 (최초 1회)
PostgreSQL이 실행 중인 상태에서 아래 명령어로 테스트 계정을 생성합니다.
```bash
python scripts/setup_test_user.py
```
생성되는 계정:
| 구분 | ID | PW |
|------|----|---------|
| 지원자(시험용) | `test` | `test` |
| 관리자 | `admin` | `admin` |

### 2단계: 서버 실행
`server.py` 파일을 실행하면 서버가 구동되고 자동으로 **시스템 기본 웹 브라우저**가 열립니다.
(보안 및 성능을 위해 127.0.0.1:8000 접속을 기본으로 합니다.)
`server.py`는 서버 구동과 웹 브라우저 자동 실행 기능에만 집중하여 최적화되어 있으며, 모든 주석이 한국어로 제공됩니다.

```bash
# 작업 디렉토리로 이동
cd 'C:\big20\big20_AI_Interview_simulation\LDW\text09'

# 서버 실행
python server.py
```
- 서버는 기본적으로 `http://127.0.0.1:8000` 에서 동작합니다.
- 브라우저가 자동으로 열리지 않을 경우, 주소창에 위 주소를 직접 입력하세요.

### 3단계: Docker를 이용한 실행 방법 (권장)
Docker가 설치되어 있다면, 다음 명령어로 간편하게 실행할 수 있습니다.

1. **컨테이너 빌드 및 실행**:
   ```bash
   docker-compose up --build
   ```
   > **주의사항**: 상위 폴더(`.../big20/`)에 `.env` 파일이 존재해야 정상적으로 실행됩니다. (API Key 등 필수 설정 포함)

2. **접속**:
   웹 브라우저를 열고 `http://localhost:5000`으로 접속합니다.
   (Docker 환경에서는 브라우저가 자동으로 실행되지 않을 수 있습니다.)
3. **종료**:
   `Ctrl + C`를 눌러 서버를 중지하거나, 다른 터미널에서 `docker-compose down`을 실행합니다.

---

## 4. 사용 모델 및 라이브러리 목록

### 핵심 기술 (AI & Backend)
- **FastAPI**: 고성능 비동기 웹 프레임워크 (백엔드 서버)
- **Google Gemini 2.0 Flash**: 면접 질문 생성, 답변 분석, 평가(LLM) 및 **음성 인식(STT)** 통합 모델 - **[NEW]** All-in-One AI 적용
- **LangChain**: LLM 오케스트레이션 및 프롬프트 관리
- **Uvicorn**: ASGI 웹 서버
- **numpy==1.26.2**: 특정 버전 고정 (바이너리 호환성 해결) **[UPDATED]**
- **pyannote-audio==3.1.1**: 특정 버전 고정 (의존성 충돌 해결) **[UPDATED]**
- **scipy==1.12.0**: numpy 1.26.2 ABI 호환 버전으로 다운그레이드 **[UPDATED]**
- **tensorflow==2.15.1**: numpy 1.26.2 바이너리 호환 버전으로 다운그레이드 **[UPDATED]**
- **onnx==1.15.0 / onnxruntime==1.17.1**: ml_dtypes 호환성 문제 해결을 위해 버전 고정 **[NEW]**

### 음성 및 멀티미디어
- **Google Gemini (Multimodal)**: 고성능 음성 인식 (STT) - [유지]
- **OpenAI Whisper**: 음성 인식 (STT) 교차 검증용 모델 - **[NEW]** Gemini와 결과 비교
- **Python Levenshtein**: 두 STT 결과 간의 유사도 정밀 분석 **[NEW]**
- **Edge-TTS**: 자연스러운 음성 합성 (TTS)
- **PyAudio**: 오디오 입출력 처리
- **MoveNet Thunder (Google)**: 실시간 자세(Pose) 분석
- **DeepFace (Facebook) / OpenCV**: 표정 기반 감정 분석 (영상 분석 데이터로 활용)

### 데이터베이스 및 저장소
- **PostgreSQL / SQLite**: (설정에 따라) 면접 데이터 및 결과 저장
- **SQLAlchemy**: ORM(Object Relational Mapping) 데이터베이스 연동

---

## 5. 주요 기능 사용법

### [1] 로그인 및 직무 선택
1. 회원가입 후 로그인을 진행합니다. (시험용 계정: `test` / `test`)
2. 면접을 진행할 **직무(예: 웹 개발자, 마케팅 등)**를 선택하거나 입력합니다.
3. 자기소개서 파일을 업로드하거나 텍스트로 입력합니다.

### [2] 면접 진행
1. '면접 시작' 버튼을 누르면 이력서 업로드 및 **환경 테스트(카메라, 마이크, 오디오)** 단계가 진행됩니다. **[UPDATED]**
2. **환경 테스트**: 카메라, 마이크뿐만 아니라 오디오(스피커) 연결 상태를 자동으로 확인하며, 연결된 실제 기기명을 '정상' 표시 옆에 `(기기명)` 형식으로 친절하게 알려줍니다. 모든 기기가 정상적으로 연결되어야 면접을 시작할 수 있습니다.
3. **화면 구성**: 면접관의 얼굴 위치를 고정하여 안정적인 면접 환경을 제공합니다. 면접관 얼굴 아래의 질문 말풍선은 제거되었으며, 질문 내용은 하단의 채팅 로그에서 실시간으로 확인할 수 있습니다.
4. **영상 분석**: 면접 진행 중 웹캠을 통해 사용자의 표정과 자세를 실시간으로 분석합니다. 이 데이터는 답변 제출 시 자동으로 서버에 전송되어 저장됩니다.
5. 마이크를 통해 답변을 말하면 AI가 이를 인식하고 꼬리물기 질문을 이어갑니다.
6. 설정된 질문 개수만큼 면접이 진행됩니다.

### [3] 결과 확인 (관리자/사용자)
1. 면접이 종료되면 잠시 후 **분석 결과 페이지**로 이동합니다.
2. **평가 루브릭**에 따라 4가지 항목(기술, 문제해결, 의사소통, 태도)에 대한 점수와 상세 피드백을 확인합니다.
3. **태도/인성 상세 분석**: 관리자는 지원자 상세 정보 페이지에서 `MoveNet`(자세) 및 `DeepFace`(표정) 분석 데이터를 포함한 종합 태도 평가 결과를 확인할 수 있습니다.
### [4] ID/PW 찾기 기능 **[UPDATED]**
1. 로그인 화면에서 'ID/PW 찾기' 링크를 클릭합니다.
2. **ID 찾기**: 이름과 이메일 주소를 입력하면 가입된 모든 아이디 목록을 확인할 수 있습니다. 검색 결과가 여러 개인 경우 모두 표시되며, 일정 시간 후 로그인 화면으로 돌아갑니다.
3. **PW 찾기**: 아이디를 입력하면 해당 계정의 이메일로 4자리 인증번호가 발송(시뮬레이션)됩니다. 발송된 인증번호를 입력하면 비밀번호(pw)를 즉시 확인할 수 있습니다.
---

## 6. 상세 파일 구조 설명 (File Structure)

본 프로젝트는 관리 효율성을 위해 계층화된 디렉토리 구조를 따릅니다.

### [루트 디렉토리 파일]
- **`server.py`**: 서비스의 메인 엔트리 포인트입니다. FastAPI 서버(uvicorn)를 구동함과 동시에 사용자의 기본 웹 브라우저를 자동으로 열어줍니다. 한국어 주석이 적용되어 있으며 서버 실행과 브라우저 호출 기능에 집중되어 있습니다.
- **`requirements.txt`**: 프로젝트 실행을 위한 패키지 의존성 목록입니다. `numpy==1.26.2`, `pyannote-audio==3.1.1` 등 호환성이 중요한 라이브러리의 버전이 고정되어 있습니다.
- **`GUIDEBOOK.md`**: 본 문서로, 프로젝트의 개요, 설치 방법, 상세 파일 기능 및 최근 변경 사항을 포함합니다.
- **`Dockerfile` / `docker-compose.yml`**: Docker 컨테이너 환경에서 서비스를 실행하기 위한 빌드 및 오케스트레이션 설정 파일입니다.

### [주요 디렉토리 및 구성 파일]
- **`app/`**: 애플리케이션의 핵심 로직이 위치합니다.
  - **`main.py`**: FastAPI 인스턴스 생성, CORS 설정, 라우터 등록 등 앱의 진입점 역할을 합니다.
  - **`config.py`**: API 키, 파일 경로, 서버 설정 등 전역 설정을 중앙 관리합니다.
  - **`database.py`**: SQLAlchemy를 이용한 DB 연결 엔진 및 세션 설정을 담당합니다.
  - **`models.py`**: 유저, 면접 세션, 질문, 분석 결과 등 DB 테이블 스키마를 정의합니다.
  - **`routers/`**: 기능별 API 엔드포인트를 정의합니다 (`auth`, `user`, `interview`, `admin`, `job`, `video_router`).
  - **`services/`**: 핵심 비즈니스 기능을 수행합니다.
    - `stt_service.py`: Gemini 및 Whisper를 활용한 음성 인식 로직.
    - `tts_service.py`: Edge-TTS를 활용한 음성 합성 및 비디오 폴백 처리.
    - `llm_service.py`: Gemini를 이용한 면접 질문 생성 및 답변 분석.
    - `analysis_service.py`: 음성 데이터(성량, 피치 등) 정밀 분석.
    - `video_analysis_service.py`: 실시간 자세 및 표정 분석 데이터 처리.
    - `video_gen_service.py`: Wav2Lip 기반 립싱크 비디오 생성.
    - `vad_service.py`: 목소리 활동 감지(VAD)를 통한 답변 유무 판단.
- **`scripts/`**: 데이터 관리 및 환경 설정을 위한 유틸리티 폴더입니다.
  - `check_env.py`: 실행 환경(FFmpeg, 패키지 등) 사전 점검.
  - `setup_test_user.py`: 초기 테스트 계정(`test`/`test`) 생성.
  - `export_db.py` / `import_db.py`: 데이터베이스 백업 및 복구.
  - `diagnose_numpy.py`: numpy 버전 및 바이너리 호환성 진단.
- **`static/`**: 웹 프론트엔드 자산입니다.
  - `index.html`: 메인 웹 페이지 구조.
  - `app.js`: 실시간 면접 진행, 미디어 처리, API 통신 등 클라이언트 로직.
  - `style.css`: UI 디자인 및 레이아웃 스타일.
- **`tests/`**: 시스템 안정성 검증을 위한 테스트 코드입니다.
  - `manual_verification/`: 주요 기능(FFmpeg, 오디오 인식 등)의 수동 검증 스크립트.
  - `test_stt_verification.py`, `test_video_gen.py` 등 각 모듈별 유닛 테스트.
- **`data/`**: 시스템에서 사용하는 공통 리소스와 백업 데이터 저장소입니다.
- **`models/`**: AI 모델 실행에 필요한 각종 가중치(pth, bin) 파일이 위치합니다.
- **`uploads/`**: 사용자가 업로드한 파일(resume) 및 녹음된 원본 데이터(audio, Wav2Lip_mp4)가 저장됩니다.
- **`LivePortrait`, `SadTalker`, `Wav2Lip`**: 외부 AI 이미지/비디오 생성 엔진 소스 코드 및 관련 모듈입니다.
- **`db_data/`**: PostgreSQL 등 컨테이너 환경의 데이터 영구 보관용 폴더입니다.

---

## 7. 최근 추가/변경된 기능
- **STT/LLM 모델 통합**: OpenAI Whisper 및 GPT-4o를 **Google Gemini 2.0 Flash**로 전면 교체하여 비용 효율성과 처리 속도를 개선했습니다.
- **음성 인식(STT) 강화**: Gemini Multimodal 기능을 활용하여 음성 파일의 유효성을 검사하고, 인식 실패 시 재시도하거나 명확한 에러 메시지를 반환하도록 개선했습니다.
- **질문 생성 로직 개선**: Gemini 2.0 Flash의 JSON 출력 안정성을 확보하기 위해 마크다운 정리 로직(`clean_json_string`)과 재시도 메커니즘을 추가했습니다.
- **Rate Limit 대응**: 무료 등급 사용 시 발생할 수 있는 할당량 초과(429 Error)에 대비하여 지수 백오프(Exponential Backoff) 기반의 재시도 로직을 구현했습니다.
- **테스트 스크립트 정리**: `scripts/check_env.py` 및 `tests/manual_verification/` 하위 스크립트를 통해 각 기능을 독립적으로 검증할 수 있습니다.
- **오디오 파일 저장 개선**: 면접 답변 음성 파일의 이름을 `YYYY-MM-DD-HH-MM-SS-{면접세션명}.webm` 형식으로 저장하여, 언제 어떤 면접에서 녹음된 파일인지 쉽게 식별할 수 있도록 개선했습니다.
- **STT 교차 검증 시스템 도입**: **Google Gemini (Multimodal)**와 **OpenAI Whisper**를 동시에 사용하여 음성 인식 정확도를 획기적으로 높였습니다. 두 모델의 인식 결과가 **95% 이상 일치(Levenshtein Distance)**할 경우에만 Gemini 결과를 채택하고, 그렇지 않을 경우 더 안정적인 Whisper 결과를 1순위로 사용하여 환각(Hallucination) 현상을 최소화했습니다.
- **비디오 심층 분석 통합**: `uploads/audio` 폴더에 저장된 `.webm` 영상 파일을 면접 종료 시 자동으로 스캔하여 **OpenCV**, **MoveNet Thunder**, **DeepFace**로 정밀 분석합니다. 프레임 단위로 감정과 자세를 분석하여 최종 태도/인성 평가에 반영합니다.
- **오디오 심층 분석 시스템 (Audio Deep Analysis)** **[NEW]**: 지원자의 목소리 데이터를 정밀 분석하여 비언어적 요소를 평가에 반영합니다.
    - **초고속 무음 감지 (RMS & VAD)**: 전체 오디오의 에너지(RMS)와 사람 목소리 비율(WebRTC VAD)을 0.01초 내에 분석하여, 의미 없는 침묵이나 백색 소음만 있는 경우 즉시 "답변 없음"으로 처리합니다. 불필요한 STT 비용을 절감하고 빠른 피드백을 제공합니다.
    - **무음(Silence) 길이 측정**: 답변 도중의 침묵 시간을 측정하여 당황 여부를 판단합니다.
    - **Noise Reduction & Normalization**: `noisereduce`로 화이트 노이즈를 제거하고, 오디오 볼륨을 일정 수준으로 증폭하여 인식률을 극대화합니다.
    - **성량 및 자신감 (RMS Energy)**: `Librosa`를 활용해 목소리 크기의 변화를 분석하여 자신감을 측정합니다.
    - **목소리 떨림 (Pitch Jitter & Shimmer)**: `Parselmouth`를 사용하여 목소리의 미세한 떨림과 진폭 변동을 수치화해 긴장도를 분석합니다.
    - **말하기 속도 (Speech Rate)**: 전사된 글자 수와 시간을 비교하여 말이 너무 빠르거나 느린지 판단합니다.
- **STT 전사 로직 고도화**: 
    - Gemini와 Whisper 결과를 비교하여 최적의 답변을 선택하는 스마트 엔진을 적용했습니다.
    - **간투사 보존 우선**: "어", "음", "그" 등 면접의 생동감을 나타내는 간투사를 더 많이 포함한 결과를 우선적으로 선택합니다.
    - **답변 없음 처리**: 두 모델 모두 결과가 없거나 "답변 없음"인 경우를 정밀하게 판별합니다.
    - **정보량 위주 선택**: 포함 관계가 성립하거나 길이가 10자 미만인 짧은 결과에 대해 패널티를 부여하고 더 상세한(긴) 결과를 선택합니다.
    - **서버 스크립트 최적화**: `server.py`는 서버 구동 및 자동 웹 브라우저 실행 기능만 전용으로 개편하고 모든 주석을 한국어로 변경했습니다.
- **프로젝트 구조 정량화 및 정돈**: 루트 디렉토리를 정리하여 관리 효율성을 높였으며, 지정된 폴더(`app`, `data`, `db`, `models` 등) 외의 파일들을 정리했습니다.

### 신규 업데이트 (관리자 공고 폼 구조 개선) **[NEW]**
- **관리자 공고 등록 필드 고도화**: 공고 등록 및 수정 시 기존 '직무 (Job)', '공고 내용', '마감일' 텍스트를 '채용 직무', '주요업무', '공고 마감일'로 각각 변경하여 직관성을 높였습니다.
- **상세 정보 입력란 추가**: '자격 요건', '우대사항', '복리후생', '채용절차', '채용인원' 총 5개의 새로운 항목을 입력 가능토록 추가하여, 데이터베이스(`interview_announcement`)에 상세 직무 관련 정보를 분리 저장하고 조회할 수 있습니다.

### 신규 업데이트 (이력서 시각화 및 구조화) **[NEW]**
- **이력서 미리보기(썸네일) 및 원본 보기(모달)**: '1. 이력서 업로드' 중 파일 선택 즉시 사용자 화면 우측에 이력서를 작은 크기(썸네일 형태)로 간편하게 미리 볼 수 있습니다. 썸네일을 클릭하면 원본 크기로 큰 모달 창을 띄워 상세히 확인할 수 있어 사용성이 개선되었습니다.
- **루트 디렉토리 정리**: `create_pdf.py` 및 `test_resume.pdf`와 같은 부가 스크립트와 파일들을 각각 `scripts/`, `test_uploads/` 폴더 등으로 옮겨 폴더 구조가 더 직관적으로 정리되었습니다.

### 신규 업데이트 (비디오 생성 안정성 개선) **[2026-02-24]**
- **비디오 생성 경로 설정 중앙화 (Critical Fix)**: `app/config.py`에 `WAV2LIP_OUTPUT_FOLDER`, `WAV2LIP_DIR`, `WAV2LIP_INFERENCE_SCRIPT`, `WAV2LIP_CHECKPOINT`, `WAV2LIP_FACE_IMAGE` 등 비디오 생성 관련 경로를 중앙 관리하도록 추가했습니다. 기존에는 `video_gen_service.py`에서 하드코딩된 경로를 사용하여 경로 불일치로 인한 비디오 생성 실패가 발생할 수 있었습니다.
- **비디오/오디오 폴백 로직 개선**: `tts_service.py`의 반환값을 `{"url": ..., "type": "video"|"audio"}` dict 형태로 변경하여, 프론트엔드가 비디오 생성 성공/실패 여부를 명확히 구분할 수 있게 했습니다.
- **프론트엔드 재생 안정화**: `app.js`의 `playAudio()` 함수를 수정하여, `.mp4` 파일은 `<video>` 태그로, `.mp3` 파일은 `<audio>` 요소로 재생합니다. 기존에는 비디오 생성 실패 시 mp3 파일을 `<video>` 태그에 로드하여 재생 오류가 발생했습니다.
- **인터뷰 라우터 응답 확장**: API 응답에 `audio_type` 필드를 추가하여 클라이언트가 미디어 타입을 인지하고 적절한 재생 방식을 선택할 수 있습니다.
- **의존성 최신화**: FastAPI 0.131.0, Uvicorn 0.41.0, Pydantic 2.12.5, SQLAlchemy 2.0.46 등 주요 라이브러리를 2026년 2월 기준 최신 안정 버전으로 업데이트했습니다. (`numpy==1.26.2`, `pyannote-audio==3.1.1` 고정 유지)

---

## 8. 데이터 백업 및 복원 (데이터 이관)

본 시뮬레이션은 데이터베이스에 저장된 면접 기록, 사용자 정보 등을 파일로 저장(내보내기)하고, 이를 다른 컴퓨터나 환경으로 복원(가져오기)할 수 있는 스크립트를 제공합니다.

### 8.1 데이터 내보내기 (Export)
현재 실행 중인 컨테이너 또는 로컬 데이터베이스의 데이터를 JSON 파일로 저장합니다.

1. **실행 방법**:
   ```bash
   python scripts/export_db.py
   ```
2. **결과**:
   - `data/interview_db_backup.json` 파일이 생성됩니다.
   - 이 파일에는 사용자 정보, 면접 기록, 채점 결과 등이 포함됩니다.
   - **주의**: 업로드된 파일(이력서, 오디오 등)은 `uploads` 폴더에 별도로 저장되므로, 이 폴더를 수동으로 복사해야 완벽한 백업이 가능합니다.

### 8.2 데이터 가져오기 (Import)
백업된 JSON 데이터를 새로운 데이터베이스에 복원합니다.

1. **준비 사항**:
   - `data/interview_db_backup.json` 파일을 새로운 환경의 `data` 폴더에 위치시킵니다.
   - 데이터베이스가 실행 중이어야 합니다.
2. **실행 방법**:
   ```bash
   python scripts/import_db.py
   ```
3. **결과**:
   - 기존 데이터를 유지하면서 백업된 데이터를 추가합니다.
   - 중복된 데이터(Primary Key 기준)는 무시하고 새로운 데이터만 추가됩니다 (ON CONFLICT DO NOTHING).

### 8.3 데이터 이관 시 주의사항
- **파일 이동**: 다른 컴퓨터로 이관할 경우, `data/interview_db_backup.json` 파일뿐만 아니라 `uploads/` 폴더 전체를 함께 이동시키는 것을 권장합니다.
- **환경 변수**: `scripts/.env` 또는 프로젝트 루트의 `.env` 파일 설정(DB 접속 정보 등)이 올바른지 확인하세요.

---

## 9. Wav2Lip 기반 립싱크 비디오 연동 (업데이트 노트)
본 프로젝트는 기존의 정적 AI 아바타에서 벗어나, 사용자의 답변과 TTS 오디오에 맞춰 동적으로 발화하는 **Wav2Lip-GAN** 기반 비디오 렌더링 기능을 지원합니다.

### 9.1 주요 통합 내용
*   **비디오 출력 지원**: 프론트엔드(`static/index.html` 및 `static/app.js`)의 비디오 플레이어(`<video id="ai-video">`)를 통해 256x256 크기의 립싱크 비디오를 지원합니다.
*   **Wav2Lip 비디오 생성 모듈**: `app/services/video_gen_service.py`를 통해 TTS 오디오(`uploads/tts_audio/*.mp3`)와 지정된 이미지(`data/man.png`)를 합성한 후 `uploads/Wav2Lip_mp4/*.mp4`를 생성합니다. 경로 설정은 `app/config.py`에서 `WAV2LIP_OUTPUT_FOLDER`, `WAV2LIP_CHECKPOINT`, `WAV2LIP_FACE_IMAGE` 등으로 중앙 관리됩니다. **[UPDATED]**
*   **비디오/오디오 자동 폴백**: 비디오 생성에 실패할 경우 TTS 오디오(mp3)만 재생하며, 프론트엔드가 `.mp4`/`.mp3` 확장자를 자동 감지하여 적절한 방식(`<video>` 또는 `<audio>`)으로 재생합니다. **[NEW]**
*   **의존성 최신화**: 립싱크 처리에 필요한 `ffmpeg-python`, `moviepy`, `tqdm`, `torchvision`, `torchaudio`, `librosa` 모듈이 요구사항(`requirements.txt`)에 반영되었습니다.

### 9.2 참고 사항
* 해당 기능을 원활하게 구동하기 위해서는 사전에 `C:\big20\big20_AI_Interview_simulation\LDW\text09\Wav2Lip\checkpoints` 내에 `wav2lip_gan.pth` 가중치 파일과 모델 구조가 필요합니다.
* 실행 전 반드시 `pip install ffmpeg-python moviepy librosa torchvision torchaudio`가 완료되어 있는지 점검하십시오.



