
# **\[회의록\] AI 모의면접 시스템 개발 로드맵 및 R\&R 정의**

**일시:** 2026년 1월 28일 (수)

**안건:** AI 모의면접 시스템 단계별 개발 전략 수립 및 역할 분담

## **1\. 회의 개요 및 핵심 전략**

**핵심 전략:** '통합 규약' 선행 후, **기능 단위의 수직적 통합(Vertical Slice)** 방식 채택

**개발 방향:** 텍스트 기반의 기본 파이프라인(MVP)을 우선 완성하여 "서비스 가능한 상태"를 만든 뒤, 음성(STT/TTS) → 실시간(WebRTC) → 심화(Vision) 순으로 확장

**팀 구성:** 총 6명

---

## **2\. 상세 개발 로드맵**

### **\[0단계\] 통합 규약 확정 (소요: 0.5 \~ 1일)**

**목표:** 개발 착수 전, 변경 시 리스크가 큰 핵심 규약 3종 선행 확정

**주요 결정 사항:** 6명 전원이 합의하여 문서화 완료 후 개발 시작

**세션 상태(Session State) 정의:** session\_id, interview\_id, current\_q\_index, end\_condition, turn\_status 등

**이벤트 스키마(Event Schema) 정의:** 공통 JSON 구조 설계

필드: event\_type, timestamp\_ms, payload

타입: stt\_partial, stt\_final, llm\_question, tts\_audio, vision\_emotion, eval\_score 등

**API 계약(OpenAPI Spec):** /auth, /session/start, /session/event, /session/end, /report 등 엔드포인트 명세

**Note:** 이 3가지 규약은 추후 변경 시 팀 전체의 개발 비용(Refactoring cost)이 급증하므로 초기에 확실히 잡아야 함.

### **\[1단계\] 텍스트 면접 파이프라인 (Vertical Slice) (소요: 2 \~ 3일)**

**목표:** 영상/음성을 제외한 가장 단순한 파이프라인의 End-to-End 연결 (서비스 구동 확인)

**워크플로우:** 프론트(UI) → FastAPI → LLM → FastAPI → 프론트 질문 출력 → 사용자 텍스트 응답 → LLM 평가 JSON 반환 → DB 저장

**Role & Responsibility (6명 동시 진행)**  
| 담당 인원 | 역할 및 상세 업무 |  
| :--- | :--- |  
| **1명** | **Backend Core:** FastAPI 기본 뼈대 구축, 세션 관리 및 DB 모델링 |  
| **1명** | **AI Logic:** LLM 프롬프트 엔지니어링 및 구조화 출력(JSON) 최적화 |  
| **1명** | **Frontend:** 텍스트 채팅 기반의 기본 면접 UI 구현 |  
| **1명** | **Evaluation:** 면접 평가 루브릭(최소 3개 항목) 및 평가 로직 JSON 설계 |  
| **1명** | **Data/Report:** 대화 내용 저장/리포트 엔드포인트 구현 및 결과 조회 UI |  
| **1명** | **QA/Test:** 시나리오 기반 테스트(pytest), 더미 데이터 생성 및 검증 |

### **\[2단계\] STT(음성 인식) 통합 (소요: 3 \~ 5일)**

**목표:** 텍스트 파이프라인에 단방향 음성 입력 기능 추가 (WebRTC 제외)

**구현 방식:** 마이크 입력(또는 오디오 파일) → Deepgram/Whisper → 텍스트 변환 → 기존 로직 처리

**핵심 과제:**

Interim(중간 결과) / Final(최종 결과) 처리 로직

발화 단위 Segmentation (문장 끊기)

Timestamp가 포함된 Transcript 저장 구조 마련

### **\[3단계\] TTS(음성 합성) 및 인터럽트 구현 (소요: 5 \~ 7일)**

**목표:** 양방향 음성 대화 구현 및 자연스러운 턴 테이킹(Turn-taking)

**구현 방식:** LLM 질문 생성 → TTS 오디오 스트림 → 프론트엔드 재생

**핵심 과제:**

**Interrupt(끼어들기):** 사용자 발화 감지 시(또는 버튼 클릭) TTS 즉시 중단

오디오 스트리밍(Chunk) 처리 및 중단/재시작 컨트롤

모든 상태 변화를 세션 이벤트로 관리

### **\[4단계\] WebRTC/LiveKit 실시간 면접장 (소요: 1 \~ 2주)**

**목표:** 브라우저 기반 실시간 저지연 통신 환경 구축 (본격적인 면접 환경)

**구현 내용:**

서버 사이드 오디오 스트림 수신 및 STT 파이프라인 연결 (Streaming)

처리 결과를 WebSocket 등을 통해 프론트로 실시간 Push

**리소스 배분:** 스트리밍 담당 1인으로는 부족하며, **최소 2\~3명을 투입**하여 집중 개발 필요

### **\[5단계\] Vision AI 및 리포트 고도화 (확장 단계)**

**목표:** 멀티모달 분석 기능 추가 (MVP 완성 후 병렬 진행)

**구현 내용:**

초당 1\~2 프레임 샘플링하여 Vision AI 분석

감정/시선/얼굴 검출 이벤트를 발행하여 면접 질문 타임라인과 동기화

---

## **3\. Action Items (다음 할 일)**

**\[전원\]** 금일 중 **0단계(통합 규약)** 회의 진행 및 문서 3종(세션, 이벤트, API) 확정

**\[PM/PL\]** 1단계 R\&R의 구체적인 담당자 지정 및 지라(Jira)/칸반 보드 티켓 생성

---

