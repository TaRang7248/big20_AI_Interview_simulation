# (í•µì‹¬) LangGraph ì›Œí¬í”Œë¡œìš° ì •ì˜


import os
# [ì¶”ê°€] í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from dotenv import load_dotenv
# [ì¶”ê°€] .env íŒŒì¼ ì¦‰ì‹œ ë¡œë“œ (ì´ ì½”ë“œê°€ llm ì´ˆê¸°í™”ë³´ë‹¤ ë¨¼ì € ì‹¤í–‰ë˜ì–´ì•¼ í•¨)
load_dotenv()

from typing import Annotated, Literal, TypedDict, List
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, BaseMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder # 26.02.05 ì¶”ê°€(500 error)
# [ìˆ˜ì • 1] pydanticì—ì„œ ì§ì ‘ import í•©ë‹ˆë‹¤.
from pydantic import BaseModel, Field 
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
# [ì¶”ê°€] ë©”ëª¨ë¦¬ ì €ì¥ì„ ìœ„í•œ ì²´í¬í¬ì¸í„°
from langgraph.checkpoint.memory import MemorySaver 

# RAG ì²´ì¸ í•¨ìˆ˜ ì„í¬íŠ¸ (ê²½ë¡œ ì£¼ì˜)
from YJH.chains.rag_chain import retrieve_interview_context


# --- 1. ìƒíƒœ(State) ì •ì˜ ---
class InterviewState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    phase: str
    question_count: int
    last_assessment: dict 

# --- 2. êµ¬ì¡°í™”ëœ ì¶œë ¥(Structured Output) ì •ì˜ ---
class AnswerAssessment(BaseModel):
    """ì§€ì›ì ë‹µë³€ í‰ê°€ ëª¨ë¸"""
    relevance: int = Field(description="ë‹µë³€ì´ ì§ˆë¬¸ ì˜ë„ì— ì–¼ë§ˆë‚˜ ë¶€í•©í•˜ëŠ”ì§€ (1-5ì )")
    technical_accuracy: int = Field(description="ê¸°ìˆ ì  ì •í™•ì„± (1-5ì )")
    completeness: bool = Field(description="ë‹µë³€ì´ ì¶©ë¶„íˆ ì™„ë£Œë˜ì—ˆëŠ”ì§€ ì—¬ë¶€")
    follow_up_needed: bool = Field(description="ì‹¬ì¸µ ì§ˆë¬¸(ê¼¬ë¦¬ë¬¼ê¸°)ì´ í•„ìš”í•œì§€ ì—¬ë¶€")
    reasoning: str = Field(description="í‰ê°€ ì´ìœ  ë° ê´€ì°° ë‚´ìš©")

# --- 3. ëª¨ë¸ ì´ˆê¸°í™” ---
llm = ChatOpenAI(
    model="gpt-4o",
    temperature=0.7
)

# --- 4. ë…¸ë“œ(Node) í•¨ìˆ˜ ì •ì˜ ---

def node_analyze_answer(state: InterviewState):
    """
    ì§€ì›ìì˜ ë‹µë³€ì„ ë¶„ì„í•˜ê³  í‰ê°€í•©ë‹ˆë‹¤.
    (ìˆ˜ì •ì‚¬í•­: ì§§ì€ ì¸ì‚¬ë§ì´ë‚˜ ì´ˆê¸° ë‹¨ê³„ëŠ” í‰ê°€ë¥¼ ê±´ë„ˆë›°ì–´ ë¬´í•œ ë£¨í”„ ë°©ì§€)
    """
    print("--- ë…¸ë“œ ì‹¤í–‰: ë‹µë³€ í‰ê°€ (Analyze Answer) ---")
    
    messages = state["messages"]
    
    # 1. ë©”ì‹œì§€ê°€ ì—†ê±°ë‚˜ ì‹œìŠ¤í…œ ë©”ì‹œì§€ì¸ ê²½ìš° ê±´ë„ˆëœ€
    if not messages or isinstance(messages[-1], SystemMessage):
        return {"last_assessment": {}}

    user_answer = messages[-1].content
    
    # ------------------------------------------------------------------
    # [í•µì‹¬ ìˆ˜ì •] "ì•ˆë…•í•˜ì„¸ìš”" ê°™ì€ ì§§ì€ ì¸ì‚¬ëŠ” í‰ê°€í•˜ì§€ ì•Šê³  ë°”ë¡œ í†µê³¼ì‹œí‚µë‹ˆë‹¤.
    # ì´ ë¶€ë¶„ì´ ì—†ìœ¼ë©´ AIê°€ ì¸ì‚¬ë¥¼ ê¸°ìˆ ì ìœ¼ë¡œ í‰ê°€í•˜ë ¤ë‹¤ ì—ëŸ¬(í† í° ì´ˆê³¼)ê°€ ë‚©ë‹ˆë‹¤.
    # ------------------------------------------------------------------
    if len(user_answer) < 20: 
        print(f"â© [Skip] ë‹µë³€ ê¸¸ì´({len(user_answer)}ì)ê°€ ì§§ì•„ ì •ë°€ í‰ê°€ë¥¼ ìƒëµí•©ë‹ˆë‹¤.")
        # ê°€ì§œ(Dummy) í‰ê°€ ë°ì´í„°ë¥¼ ë°˜í™˜í•˜ì—¬ ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ê¹€
        return {
            "last_assessment": {
                "technical_accuracy": 5,   # ê¸°ë³¸ ì ìˆ˜ ë¶€ì—¬
                "logic": 5,
                "communication": 5,
                "feedback": "ì¸ì‚¬ ë° ë„ì… ë‹¨ê³„ì…ë‹ˆë‹¤.",
                "follow_up_needed": False
            }
        }

    # 2. í‰ê°€ í”„ë¡¬í”„íŠ¸ ì„¤ì • (ê¸´ ë‹µë³€ì¼ ê²½ìš°ì—ë§Œ ì‹¤í–‰ë¨)
    evaluator_prompt = SystemMessage(content="""
    ë‹¹ì‹ ì€ 15ë…„ ì°¨ ì‹œë‹ˆì–´ í…Œí¬ë‹ˆì»¬ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. 
    ì§€ì›ìì˜ ë‹µë³€ì„ ë“£ê³  ê¸°ìˆ ì  ì •í™•ì„±ê³¼ ë…¼ë¦¬ì„±ì„ ëƒ‰ì² í•˜ê²Œ í‰ê°€í•˜ì‹­ì‹œì˜¤.
    ë‹µë³€ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ëª¨í˜¸í•˜ë©´ 'follow_up_needed'ë¥¼ trueë¡œ ì„¤ì •í•˜ì„¸ìš”.
    """)
    
    # 3. LLM í˜¸ì¶œ
    # (ì£¼ì˜: AnswerAssessment ëª¨ë¸ì´ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤)
    try:
        structured_llm = llm.with_structured_output(AnswerAssessment)
        # ìµœê·¼ 5ê°œ í„´ë§Œ ë¶„ì„
        response = structured_llm.invoke([evaluator_prompt] + messages[-5:]) 
        
        return {"last_assessment": response.model_dump()}
        
    except Exception as e:
        print(f"âŒ í‰ê°€ ë¡œì§ ì—ëŸ¬: {e}")
        # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ë©ˆì¶”ì§€ ì•Šë„ë¡ ê¸°ë³¸ê°’ ë°˜í™˜
        return {
            "last_assessment": {
                "feedback": "í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ë„˜ì–´ê°‘ë‹ˆë‹¤.",
                "follow_up_needed": False
            }
        }



# [ìˆ˜ì •] node_generate_question í•¨ìˆ˜ ì „ì²´ êµì²´ (26.02.05)

def node_generate_question(state: InterviewState):
    """
    í˜„ì¬ ë©´ì ‘ ë‹¨ê³„ì— ë”°ë¼ ì ì ˆí•œ ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    - intro: í™˜ì˜ ì¸ì‚¬ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    - technical_interview: ì´ë ¥ì„œ ê¸°ë°˜ ê°•ì œ ì§ˆë¬¸ (Strict Mode ì ìš©)
    """
    print("--- ë…¸ë“œ ì‹¤í–‰: ì§ˆë¬¸ ìƒì„± (Generate Question) ---")

    phase = state.get("phase", "intro")
    messages = state["messages"]
    q_count = state.get("question_count", 0)

    # --- [Phase 1] ë„ì…ë¶€ (Intro) ---
    # ì•„ì§ ë©´ì ‘ì´ ì‹œì‘ë˜ì§€ ì•Šì•˜ê±°ë‚˜, ì²« ì¸ì‚¬ë¥¼ í•´ì•¼ í•  ë•Œ
    if phase == "intro":
        print("ğŸ‘‹ [Phase: Intro] í™˜ì˜ ì¸ì‚¬ ìƒì„±")
        system_prompt = """
        ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ AI ë©´ì ‘ê´€ì…ë‹ˆë‹¤. 
        ì§€ì›ìê°€ ë©´ì ‘ì¥ì— ì²˜ìŒ ë“¤ì–´ì˜¨ ìƒí™©ì…ë‹ˆë‹¤. 
        ê¸´ì¥ì„ í’€ì–´ì£¼ë©° ì •ì¤‘í•˜ê²Œ í™˜ì˜ ì¸ì‚¬ë¥¼ ê±´ë„¤ê³ , ê°„ë‹¨í•œ ìê¸°ì†Œê°œë¥¼ ìš”ì²­í•˜ì„¸ìš”.
        (ì•„ì§ ê¸°ìˆ  ì§ˆë¬¸ì€ í•˜ì§€ ë§ˆì„¸ìš”.)
        """
        
        # ê°€ì§œ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë„£ì–´ AIì˜ ì²« ë§ˆë””ë¥¼ ìœ ë„
        msg = llm.invoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content="ë©´ì ‘ê´€ë‹˜ ì•ˆë…•í•˜ì„¸ìš”, ë©´ì ‘ ë³´ëŸ¬ ì™”ìŠµë‹ˆë‹¤.") 
        ])
        
        # ì¸ì‚¬ê°€ ëë‚¬ìœ¼ë¯€ë¡œ ë‹¤ìŒ í„´ë¶€í„°ëŠ” 'technical_interview'ë¡œ ì „í™˜
        return {
            "messages": [msg], 
            "phase": "technical_interview", 
            "question_count": q_count 
        }

    # --- [Phase 2] ê¸°ìˆ  ë©´ì ‘ (Technical) - Strict Mode ì ìš© ---
    
    # 1. ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ë°œì–¸ ë‚´ìš© í™•ì¸ (main_yjh.pyì—ì„œ ì£¼ì…í•œ í”„ë¡¬í”„íŠ¸ê°€ ìˆëŠ”ì§€)
    last_user_msg = messages[-1].content if messages else ""
    
    # ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì¼ë°˜ ëª¨ë“œ)
    system_instruction = f"""
    ë‹¹ì‹ ì€ 15ë…„ ì°¨ ì‹œë‹ˆì–´ ê¸°ìˆ  ë©´ì ‘ê´€ì…ë‹ˆë‹¤. (ì§ˆë¬¸ íšŸìˆ˜: {q_count + 1}ë²ˆì§¸)
    ì§€ì›ìì˜ ë‹µë³€ì„ ë“£ê³  ì´ì–´ì§€ëŠ” ê¸°ìˆ  ì§ˆë¬¸(ê¼¬ë¦¬ ì§ˆë¬¸)ì„ í•˜ë‚˜ ë˜ì§€ì„¸ìš”.
    """

    # 2. [Strict Mode ê°ì§€] ì´ë ¥ì„œ ì»¨í…ìŠ¤íŠ¸ê°€ ì£¼ì…ë˜ì—ˆëŠ”ì§€ í™•ì¸
    # main_yjh.pyì—ì„œ "Resume Context"ë¼ëŠ” ë‹¨ì–´ë¥¼ í¬í•¨í•´ì„œ ë³´ëƒˆë‹¤ë©´ ì´ ëª¨ë“œê°€ ë°œë™ë©ë‹ˆë‹¤.
    if "Resume Context" in last_user_msg or "System Instruction" in last_user_msg:
        print("ğŸ”’ [Strict Mode] ì´ë ¥ì„œ ê¸°ë°˜ ì§ˆë¬¸ ëª¨ë“œ ë°œë™ (ë”´ì†Œë¦¬ ì°¨ë‹¨)")
        system_instruction = """
        [Role]
        ë‹¹ì‹ ì€ ì§€ì›ìì˜ 'ì´ë ¥ì„œ(Resume)'ë¥¼ ê²€ì¦í•˜ëŠ” ê¹ê¹í•œ ë©´ì ‘ê´€ì…ë‹ˆë‹¤.

        [Critical Rules]
        1. ë°˜ë“œì‹œ ì‚¬ìš©ìê°€ ë°©ê¸ˆ ì œê³µí•œ [Resume Context] ë‚´ìš© ì•ˆì—ì„œë§Œ ì§ˆë¬¸í•˜ì‹­ì‹œì˜¤.
        2. ì´ë ¥ì„œì— ì—†ëŠ” 'ê°•í™”í•™ìŠµ(RL)', 'NLP', 'AI', 'ë”¥ëŸ¬ë‹' ì§ˆë¬¸ì€ **ì ˆëŒ€ ê¸ˆì§€**ì…ë‹ˆë‹¤.
        3. ì§€ì›ìëŠ” 'ë°±ì—”ë“œ(Java/Python/AWS)' ê°œë°œìì…ë‹ˆë‹¤. DB, API, ë°°í¬, ë§ˆì´ê·¸ë ˆì´ì…˜ ê´€ë ¨ ì§ˆë¬¸ë§Œ í•˜ì‹­ì‹œì˜¤.
        4. "ì´ì „ ë‹µë³€ì—ì„œ ì–¸ê¸‰í•˜ì‹ ..." ê°™ì€ ì„œë‘ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—°ê²°ì„±ì„ ê°•ì¡°í•˜ì‹­ì‹œì˜¤.
        5. ì§ˆë¬¸ì€ í•œ ë²ˆì— í•˜ë‚˜ë§Œ í•˜ì„¸ìš”.
        """
    
    # 3. ì§ˆë¬¸ ìƒì„±
    # ê¸°ì¡´ ì½”ë“œì˜ 'retrieve_interview_context'ëŠ” ì‚­ì œí–ˆìŠµë‹ˆë‹¤. (ì´ë¯¸ ë©”ì‹œì§€ ì•ˆì— ì •ë³´ê°€ ìˆìœ¼ë¯€ë¡œ ì¤‘ë³µ ì œê±°)
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_instruction),
        MessagesPlaceholder(variable_name="messages")
    ])
    
    chain = prompt | llm
    
    try:
        response = chain.invoke({"messages": messages})
        
        return {
            "messages": [response],
            "question_count": q_count + 1
        }
    except Exception as e:
        print(f"âŒ ì§ˆë¬¸ ìƒì„± ì¤‘ ì—ëŸ¬: {e}")
        return {
            "messages": [AIMessage(content="ì£„ì†¡í•©ë‹ˆë‹¤. ì ì‹œ í†µì‹  ì˜¤ë¥˜ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ í•œ ë²ˆ í”„ë¡œì íŠ¸ ê²½í—˜ì„ ë§ì”€í•´ ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?")]
        }



# --- 5. ê·¸ë˜í”„ êµ¬ì„± (Workflow) ---

workflow = StateGraph(InterviewState)

workflow.add_node("analyze_answer", node_analyze_answer)
workflow.add_node("generate_question", node_generate_question)

# ì‹œì‘ì  ì„¤ì •
workflow.set_entry_point("analyze_answer")

# ì—£ì§€ ì—°ê²°
workflow.add_edge("analyze_answer", "generate_question")
workflow.add_edge("generate_question", END)

# [ì¶”ê°€] ì²´í¬í¬ì¸í„° ì„¤ì • (ëŒ€í™” ê¸°ì–µ ìœ ì§€ìš©)
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)