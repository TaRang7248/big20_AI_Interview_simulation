import os
import sys
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage

# Load environment variables
load_dotenv()

class TextInterview:
    def __init__(self):
        self.api_key = os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            print("Error: OPENAI_API_KEY not found in .env file.")
            sys.exit(1)
        
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)
        self.history = []
        
        # Define the persona and system prompt
        self.system_prompt = (
            "You are a professional technical interviewer."
            "You are conducting a mock interview with a candidate for a software engineering position."
            "Ask relevant technical questions, follow up on their answers, and provide constructive feedback if asked."
            "Keep the interaction professional but encouraging."
            "Start by welcoming the candidate and asking them to introduce themselves."
        )

        self.prompt = ChatPromptTemplate.from_messages([
            ("system", self.system_prompt),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{input}")
        ])

        self.chain = self.prompt | self.llm

    def start_interview(self):
        print("--- AI Mock Interview (Text Mode) ---")
        print("Type 'exit' or 'quit' to end the interview.\n")

        # Initial greeting is generated by the AI based on the system prompt instruction
        # to "Start by welcoming..." but to trigger it we might need an initial empty trigger or just custom text.
        # Let's force an initial greeting from the AI by sending a hidden 'start' signal or just printing a standard welcome.
        # Ideally, we want the AI to speak first.
        
        start_msg = "Please start the interview by welcoming the candidate."
        response = self.chain.invoke({
            "history": self.history,
            "input": start_msg
        })
        
        print(f"Interviewer: {response.content}")
        self.history.append(HumanMessage(content=start_msg)) # We track the trigger? 
        # Actually, better to treat the system prompt instructions as sufficient, 
        # but usually LLMs respond to a user message.
        # Let's not add the 'instruction' to history visible to user context if possible, 
        # or just assume the user speaks first? 
        # The requirements said "System acts as interviewer".
        # Let's just track the AI response.
        self.history.append(AIMessage(content=response.content))

        self.run_loop()

    def run_loop(self):
        while True:
            try:
                user_input = input("\nYou: ")
                if user_input.lower() in ["exit", "quit"]:
                    print("Exiting interview. Good luck!")
                    break
                
                self.process_input(user_input)
            except KeyboardInterrupt:
                print("\nInterrupted.")
                break

    def process_input(self, user_text):
        # Update history with user input
        # Note: We don't add the user_text to history HERE before invoke if we pass it as 'input'
        # The chain invoke will use 'input' to generate response, but for the NEXT turn we need it in history.
        # LangChain's runnable with history is complex, manual management is simpler for this script.
        
        response = self.chain.invoke({
            "history": self.history,
            "input": user_text
        })

        print(f"Interviewer: {response.content}")

        # Update history
        self.history.append(HumanMessage(content=user_text))
        self.history.append(AIMessage(content=response.content))

if __name__ == "__main__":
    app = TextInterview()
    app.start_interview()
