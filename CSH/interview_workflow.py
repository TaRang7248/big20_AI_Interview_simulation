"""
LangGraph ê¸°ë°˜ ë©´ì ‘ ì›Œí¬í”Œë¡œìš° ìƒíƒœë¨¸ì‹ 
=======================================
ì›Œí¬í”Œë¡œìš° ìƒíƒœë¨¸ì‹  êµ¬í˜„

ê¸°ëŠ¥:
  1. ì¡°ê±´ë¶€ ë¶„ê¸°  â€” add_conditional_edges ë¡œ ê°ì •Â·ì ìˆ˜ ê¸°ë°˜ ë¼ìš°íŒ…
  2. ë£¨í”„ ì œì–´    â€” ê¼¬ë¦¬ì§ˆë¬¸ 2íšŒ ì œí•œ, MAX_QUESTIONS ì²´í¬
  3. ì²´í¬í¬ì¸íŠ¸   â€” MemorySaver ë¡œ ì„¸ì…˜ ì¤‘ë‹¨Â·ì¬ê°œ ì§€ì›
  4. ë³‘ë ¬ ì²˜ë¦¬    â€” ë‹µë³€ í‰ê°€ + ê°ì • ë¶„ì„ ë™ì‹œ ì‹¤í–‰ (asyncio.gather)
  5. ì‹œê°í™”/ê°ì‚¬  â€” Mermaid ë‹¤ì´ì–´ê·¸ë¨ + ì‹¤í–‰ ì¶”ì  ë¡œê·¸

Usage:
    from interview_workflow import InterviewWorkflow, WorkflowState, get_workflow_instance

    wf = get_workflow_instance()
    result = await wf.run(session_id, user_input="[START]")
    # result = {"response": "...", "phase": "greeting", ...}
"""

from __future__ import annotations

import asyncio
import time
import traceback
from datetime import datetime
from enum import Enum
from typing import Dict, List, Optional, TypedDict

from langgraph.checkpoint.memory import MemorySaver

# â”€â”€ LangGraph â”€â”€
from langgraph.graph import END, START, StateGraph

# â”€â”€ Thinking ëª¨ë¸ ì¶”ë¡  í† í° ì œê±° ìœ í‹¸ë¦¬í‹° (EXAONE Deep: <thought>, qwen3: <think>) â”€â”€
# integrated_interview_server.pyì— ì •ì˜ëœ strip_think_tokensë¥¼ importí•©ë‹ˆë‹¤.
# ìˆœí™˜ import ë°©ì§€ë¥¼ ìœ„í•´ lazy import íŒ¨í„´ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
_strip_think_tokens = None


def _get_strip_think_tokens():
    """ìŠ¤íŠ¸ë¦½ í•¨ìˆ˜ë¥¼ ì§€ì—° ë¡œë“œ (ìˆœí™˜ import ë°©ì§€)"""
    global _strip_think_tokens
    if _strip_think_tokens is None:
        try:
            from integrated_interview_server import strip_think_tokens

            _strip_think_tokens = strip_think_tokens
        except ImportError:
            import re as _re

            # í´ë°±: ì§ì ‘ êµ¬í˜„ (EXAONE Deep <thought> + qwen3 <think> ëª¨ë‘ ì§€ì›)
            def _fallback(text: str) -> str:
                cleaned = _re.sub(r"<think>.*?</think>", "", text, flags=_re.DOTALL)
                cleaned = _re.sub(r"<think>.*$", "", cleaned, flags=_re.DOTALL)
                cleaned = _re.sub(
                    r"<thought>.*?</thought>", "", cleaned, flags=_re.DOTALL
                )
                cleaned = _re.sub(r"<thought>.*$", "", cleaned, flags=_re.DOTALL)
                return cleaned.strip()

            _strip_think_tokens = _fallback
    return _strip_think_tokens


# ========================================================================== #
#                            Phase & State ì •ì˜                               #
# ========================================================================== #


class InterviewPhase(str, Enum):
    """ë©´ì ‘ ë‹¨ê³„ ì—´ê±°í˜•"""

    IDLE = "idle"  # ì´ˆê¸° ìƒíƒœ (ì„¸ì…˜ ìƒì„± ì§í›„)
    GREETING = "greeting"  # ì¸ì‚¬ / ìê¸°ì†Œê°œ ìš”ì²­
    GENERATE_QUESTION = "generate_question"  # LLM ì§ˆë¬¸ ìƒì„±
    WAIT_ANSWER = "wait_answer"  # ì‚¬ìš©ì ë‹µë³€ ëŒ€ê¸°
    PROCESS_ANSWER = "process_answer"  # ë‹µë³€ ìˆ˜ì‹  í›„ ì „ì²˜ë¦¬
    EVALUATE = "evaluate"  # ë‹µë³€ í‰ê°€ (ë³‘ë ¬: í‰ê°€ + ê°ì •)
    ROUTE_NEXT = "route_next"  # ë‹¤ìŒ ë‹¨ê³„ ë¼ìš°íŒ…
    FOLLOW_UP = "follow_up"  # ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±
    COMPLETE = "complete"  # ë©´ì ‘ ì¢…ë£Œ + ë³´ê³ ì„œ ìƒì„±
    ERROR = "error"  # ì˜¤ë¥˜ ë³µêµ¬


class WorkflowState(TypedDict, total=False):
    """LangGraph ìƒíƒœ â€” ë©´ì ‘ ì „ì²´ ì»¨í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë”•ì…”ë„ˆë¦¬ë¡œ ê´€ë¦¬"""

    # â”€â”€ ì„¸ì…˜ ì‹ë³„ â”€â”€
    session_id: str
    phase: str  # InterviewPhase ê°’

    # â”€â”€ ì…ë ¥ / ì¶œë ¥ â”€â”€
    user_input: str  # í˜„ì¬ í„´ì˜ ì‚¬ìš©ì ì…ë ¥
    response: str  # AI ë©´ì ‘ê´€ ì‘ë‹µ (ìµœì¢… ì¶œë ¥)

    # â”€â”€ ì§ˆë¬¸ ì¶”ì  â”€â”€
    question_count: int  # í˜„ì¬ê¹Œì§€ ì§ˆë¬¸ ìˆ˜ (1ë¶€í„°)
    max_questions: int  # ìµœëŒ€ ì§ˆë¬¸ ìˆ˜ (ê¸°ë³¸ 10)
    current_topic: str  # í˜„ì¬ ì§ˆë¬¸ ì£¼ì œ
    topic_question_count: int  # í•´ë‹¹ ì£¼ì œ ë‚´ ì§ˆë¬¸ ìˆ˜
    topic_history: List[Dict]  # ì£¼ì œ ë³€ê²½ ì´ë ¥

    # â”€â”€ ê¼¬ë¦¬ì§ˆë¬¸ â”€â”€
    follow_up_mode: bool
    follow_up_reason: str
    needs_follow_up: bool

    # â”€â”€ í‰ê°€ â”€â”€
    last_evaluation: Optional[Dict]  # ì§ì „ ë‹µë³€ í‰ê°€ ê²°ê³¼
    evaluations: List[Dict]  # ëˆ„ì  í‰ê°€
    pending_eval_task_id: Optional[str]  # Celery í‰ê°€ íƒœìŠ¤í¬ ID

    # â”€â”€ ê°ì • â”€â”€
    last_emotion: Optional[Dict]  # ì§ì „ ê°ì • ë¶„ì„ ê²°ê³¼ (DeepFace)
    emotion_history: List[Dict]  # ê°ì • ë³€í™” ì´ë ¥
    emotion_adaptive_mode: str  # "normal" | "encouraging" | "challenging"

    # â”€â”€ ìŒì„± ê°ì • (Hume Prosody) â”€â”€
    last_prosody: Optional[Dict]  # ì§ì „ Prosody ë¶„ì„ ê²°ê³¼
    prosody_history: List[Dict]  # Prosody ë³€í™” ì´ë ¥

    # â”€â”€ ëŒ€í™” ê¸°ë¡ â”€â”€
    chat_history: List[Dict]

    # â”€â”€ RAG ì‚¬ì „ ì¡°íšŒ ê²°ê³¼ (evaluate ë…¸ë“œì—ì„œ ë³‘ë ¬ ì¡°íšŒ â†’ generate_questionì—ì„œ ì‚¬ìš©) â”€â”€
    rag_resume_context: str  # ì´ë ¥ì„œ RAG ê²€ìƒ‰ ê²°ê³¼ â†’ LLM í”„ë¡¬í”„íŠ¸ì— ì£¼ì…
    rag_qa_context: str  # Q&A ì°¸ì¡° RAG ê²€ìƒ‰ ê²°ê³¼ â†’ LLM í”„ë¡¬í”„íŠ¸ì— ì£¼ì…

    # â”€â”€ ê°ì‚¬ / ì¶”ì  â”€â”€
    trace: List[Dict]  # [{node, timestamp, duration_ms, details}]
    error_info: Optional[str]

    # â”€â”€ ì™¸ë¶€ ì—°ê²° í”Œë˜ê·¸ â”€â”€
    use_rag: bool
    celery_available: bool
    llm_available: bool


# ========================================================================== #
#                          Trace / Audit Helpers                              #
# ========================================================================== #


def _trace_entry(node: str, details: str = "", duration_ms: float = 0) -> Dict:
    """ê°ì‚¬ ì¶”ì  ì—”íŠ¸ë¦¬ ìƒì„±"""
    return {
        "node": node,
        "timestamp": datetime.now().isoformat(),
        "duration_ms": round(duration_ms, 2),
        "details": details,
    }


# ========================================================================== #
#                            Node í•¨ìˆ˜ ì •ì˜                                    #
# ========================================================================== #


class InterviewNodes:
    """
    ê° grpah ë…¸ë“œì—ì„œ ì‹¤í–‰ë˜ëŠ” ìˆœìˆ˜ í•¨ìˆ˜(ë˜ëŠ” ì½”ë£¨í‹´)ë“¤ì„ ëª¨ì€ í´ë˜ìŠ¤.
    ì™¸ë¶€ ì„œë¹„ìŠ¤(interviewer, state ë“±)ëŠ” run-timeì— ì£¼ì…ë©ë‹ˆë‹¤.
    """

    def __init__(self, server_state, interviewer_instance, event_bus=None):
        """
        Parameters
        ----------
        server_state : InterviewState   â€” ì„œë²„ ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬ì
        interviewer_instance : AIInterviewer â€” LLM ë©´ì ‘ê´€
        event_bus : EventBus | None     â€” ì´ë²¤íŠ¸ í¼ë¸”ë¦¬ì…”
        """
        self._state_mgr = server_state
        self._interviewer = interviewer_instance
        self._event_bus = event_bus

    # ------------------------------------------------------------------ #
    #  1. greeting â€” ì¸ì‚¬ / ìê¸°ì†Œê°œ ìš”ì²­                                    #
    # ------------------------------------------------------------------ #
    async def greeting(self, ws: WorkflowState) -> Dict:
        """ìµœì´ˆ [START] ì‹ í˜¸ ì‹œ ì¸ì‚¬ë§ ë°˜í™˜"""
        t0 = time.perf_counter()
        session_id = ws["session_id"]

        greeting_msg = self._interviewer.get_initial_greeting()

        # ì„œë²„ ì„¸ì…˜ì—ë„ ë°˜ì˜
        session = self._state_mgr.get_session(session_id)
        if session:
            chat_history = session.get("chat_history", [])
            chat_history.append({"role": "assistant", "content": greeting_msg})
            self._state_mgr.update_session(
                session_id,
                {
                    "chat_history": chat_history,
                    "question_count": 1,
                },
            )

        elapsed = (time.perf_counter() - t0) * 1000
        trace = ws.get("trace", [])
        trace.append(_trace_entry("greeting", "ì´ˆê¸° ì¸ì‚¬ë§ ìƒì„±", elapsed))

        return {
            "response": greeting_msg,
            "phase": InterviewPhase.GREETING.value,
            "question_count": 1,
            "trace": trace,
        }

    # ------------------------------------------------------------------ #
    #  2. process_answer â€” ì‚¬ìš©ì ë‹µë³€ ìˆ˜ì‹  ë° ì „ì²˜ë¦¬                         #
    # ------------------------------------------------------------------ #
    async def process_answer(self, ws: WorkflowState) -> Dict:
        """ì‚¬ìš©ì ë‹µë³€ì„ ì„¸ì…˜ì— ê¸°ë¡í•˜ê³ , ì´ì „ ì§ˆë¬¸ ì •ë³´ë¥¼ ì¶”ì¶œ"""
        t0 = time.perf_counter()
        session_id = ws["session_id"]
        user_input = ws.get("user_input", "")

        session = self._state_mgr.get_session(session_id)
        if not session:
            return {"error_info": "ì„¸ì…˜ ì—†ìŒ", "phase": InterviewPhase.ERROR.value}

        chat_history = session.get("chat_history", [])
        chat_history.append({"role": "user", "content": user_input})
        self._state_mgr.update_session(session_id, {"chat_history": chat_history})

        # ì´ì „ ì§ˆë¬¸ ì°¾ê¸° (í‰ê°€ìš©)
        previous_question = ""
        for msg in reversed(chat_history[:-1]):
            if msg["role"] == "assistant":
                previous_question = msg["content"]
                break

        elapsed = (time.perf_counter() - t0) * 1000
        trace = ws.get("trace", [])
        trace.append(
            _trace_entry(
                "process_answer",
                f"ë‹µë³€ ê¸¸ì´={len(user_input)}ì, ì´ì „ì§ˆë¬¸ ì¡´ì¬={bool(previous_question)}",
                elapsed,
            )
        )

        return {
            "phase": InterviewPhase.PROCESS_ANSWER.value,
            "chat_history": chat_history,
            "trace": trace,
        }

    # ------------------------------------------------------------------ #
    #  3. evaluate â€” ë³‘ë ¬: ë‹µë³€ í‰ê°€ + ê°ì • ë¶„ì„                             #
    # ------------------------------------------------------------------ #
    async def evaluate(self, ws: WorkflowState) -> Dict:
        """ë‹µë³€ í‰ê°€ + ê°ì • ë¶„ì„ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰"""
        t0 = time.perf_counter()
        session_id = ws["session_id"]
        user_input = ws.get("user_input", "")

        session = self._state_mgr.get_session(session_id)
        if not session:
            return {"error_info": "ì„¸ì…˜ ì—†ìŒ", "phase": InterviewPhase.ERROR.value}

        chat_history = session.get("chat_history", [])

        # ì´ì „ ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°
        previous_question = ""
        for msg in reversed(chat_history):
            if msg["role"] == "assistant":
                previous_question = msg["content"]
                break

        # â”€â”€ ë³‘ë ¬ íƒœìŠ¤í¬ êµ¬ì„± â”€â”€
        eval_result: Optional[Dict] = None
        emotion_result: Optional[Dict] = None
        prosody_result: Optional[Dict] = None
        pending_task_id: Optional[str] = None

        async def _run_evaluation():
            nonlocal eval_result, pending_task_id
            try:
                # Celeryê°€ ê°€ìš©í•˜ë©´ ë¹„ë™ê¸° ì˜¤í”„ë¡œë“œ
                if ws.get("celery_available") and previous_question:
                    try:
                        # ë™ì  import (ìˆœí™˜ ë°©ì§€)
                        from celery_tasks import evaluate_answer_task

                        task = evaluate_answer_task.delay(
                            session_id, previous_question, user_input, ""
                        )
                        pending_task_id = task.id
                        # íƒœìŠ¤í¬ IDë¥¼ ì„¸ì…˜ì— ì €ì¥
                        pending_tasks = session.get("pending_eval_tasks", [])
                        pending_tasks.append(
                            {
                                "task_id": task.id,
                                "question": previous_question,
                                "answer": user_input,
                                "submitted_at": time.time(),
                            }
                        )
                        self._state_mgr.update_session(
                            session_id,
                            {
                                "pending_eval_tasks": pending_tasks,
                            },
                        )
                    except Exception as e:
                        print(f"âš ï¸ [Workflow] Celery í‰ê°€ ì˜¤í”„ë¡œë“œ ì‹¤íŒ¨: {e}")
                # Celery ë¶ˆê°€ ì‹œ â†’ ì¸ë¼ì¸ í‰ê°€ ìŠ¤í‚µ (LLM ì´ì¤‘ í˜¸ì¶œ ë°©ì§€)
                # í‰ê°€ëŠ” ë©´ì ‘ ì¢…ë£Œ í›„ collect_celery_evaluations ë˜ëŠ”
                # start_interview_completion_workflowì—ì„œ ì¼ê´„ ì²˜ë¦¬
                elif previous_question and self._interviewer.llm:
                    # evaluate_answerë¥¼ ì—¬ê¸°ì„œ í˜¸ì¶œí•˜ë©´ generate_questionê³¼ í•©ì³
                    # LLM 2íšŒ ìˆœì°¨ í˜¸ì¶œ â†’ ì‘ë‹µ ì‹œê°„ 2ë°° ì¦ê°€ (GPU ê²½í•©)
                    # ëŒ€ì‹  ì„¸ì…˜ì— pending ì •ë³´ë§Œ ê¸°ë¡í•˜ê³  ìŠ¤í‚µ
                    pending_tasks = session.get("pending_eval_tasks", [])
                    pending_tasks.append(
                        {
                            "task_id": f"deferred_{time.time()}",
                            "question": previous_question,
                            "answer": user_input,
                            "submitted_at": time.time(),
                            "deferred": True,  # Celery ë¯¸ì‚¬ìš©, ë‚˜ì¤‘ì— ì¼ê´„ í‰ê°€ í‘œì‹œ
                        }
                    )
                    self._state_mgr.update_session(
                        session_id,
                        {
                            "pending_eval_tasks": pending_tasks,
                        },
                    )
                    print("ğŸ“‹ [Workflow] í‰ê°€ ì§€ì—° ì €ì¥ (Celery ë¯¸ê°€ìš©, LLM ê²½í•© ë°©ì§€)")
            except Exception as e:
                print(f"âš ï¸ [Workflow] í‰ê°€ ì˜¤ë¥˜: {e}")

        async def _run_emotion():
            nonlocal emotion_result
            try:
                # ì„¸ì…˜ì— ì €ì¥ëœ ë§ˆì§€ë§‰ ê°ì • ë°ì´í„° í™œìš©
                last_emotion = self._state_mgr.last_emotion
                if last_emotion:
                    emotion_result = last_emotion
            except Exception:
                pass

        async def _run_prosody():
            """Hume Prosody ìŒì„± ê°ì • ë°ì´í„° ìˆ˜ì§‘"""
            nonlocal prosody_result
            try:
                last_prosody = self._state_mgr.last_prosody
                if last_prosody:
                    prosody_result = last_prosody
            except Exception:
                pass

        # â”€â”€ RAG ì‚¬ì „ ì¡°íšŒ (ì§ˆë¬¸ ìƒì„± ë…¸ë“œì—ì„œ ì‚¬ìš©í•  ì»¨í…ìŠ¤íŠ¸) â”€â”€
        # âš¡ GPU ê²½í•© ë°©ì§€: RAG ì„ë² ë”©(nomic-embed-text)ì´ Ollamaë¥¼ í˜¸ì¶œí•˜ë¯€ë¡œ
        #    LLMê³¼ ë™ì‹œì— ì‹¤í–‰í•˜ë©´ GPU ì§ë ¬ íì‰ìœ¼ë¡œ íƒ€ì„ì•„ì›ƒ ë°œìƒ.
        #    RAGë¥¼ ë¨¼ì € ì™„ë£Œí•œ ë’¤ í‰ê°€(Celery/LLM)ë¥¼ ì‹¤í–‰í•˜ì—¬ GPU ê²½í•©ì„ íšŒí”¼í•©ë‹ˆë‹¤.
        #    generate_question ì‹œì ì—ëŠ” ì´ë¯¸ Stateì— RAG ê²°ê³¼ê°€ ì¤€ë¹„ë˜ì–´ LLM í˜¸ì¶œë§Œ ìˆ˜í–‰
        rag_resume_ctx = ""
        rag_qa_ctx = ""

        try:
            rag_resume_ctx, rag_qa_ctx = await self._interviewer.fetch_rag_contexts(
                session_id, user_input
            )
        except Exception as e:
            print(f"âš ï¸ [Workflow] RAG ì‚¬ì „ ì¡°íšŒ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")

        # ë³‘ë ¬ ì‹¤í–‰ (í‰ê°€ + ê°ì • + Prosody) â€” RAGëŠ” ìœ„ì—ì„œ ì´ë¯¸ ì™„ë£Œë¨
        await asyncio.gather(_run_evaluation(), _run_emotion(), _run_prosody())

        # â”€â”€ ê°ì • ê¸°ë°˜ ì ì‘ ëª¨ë“œ ê²°ì • (â˜… ë©€í‹°ëª¨ë‹¬ ìœµí•©: Prosody + DeepFace ë™ì‹œ) â”€â”€
        emotion_adaptive_mode = ws.get("emotion_adaptive_mode", "normal")
        multimodal_fusion: Optional[Dict] = None

        if prosody_result and emotion_result:
            # â”€â”€ ì–‘ìª½ ë‹¤ ìˆì„ ë•Œ: ë©€í‹°ëª¨ë‹¬ ê°€ì¤‘ ìœµí•© (Prosody 60% + DeepFace 40%) â”€â”€
            try:
                from hume_prosody_service import get_prosody_service

                svc = get_prosody_service()
                if svc:
                    prosody_indicators = prosody_result.get("interview_indicators", {})
                    fusion = svc.merge_with_deepface(
                        prosody_indicators=prosody_indicators,
                        deepface_emotion=emotion_result,
                        prosody_weight=0.5,
                    )
                    emotion_adaptive_mode = fusion.get(
                        "emotion_adaptive_mode", "normal"
                    )
                    multimodal_fusion = fusion
            except Exception:
                # ìœµí•© ì‹¤íŒ¨ ì‹œ Prosody ë‹¨ë… ì‚¬ìš©
                emotion_adaptive_mode = prosody_result.get("adaptive_mode", "normal")
        elif prosody_result:
            # â”€â”€ Prosodyë§Œ ìˆì„ ë•Œ: 48ê°ì • 10ì§€í‘œ ê¸°ë°˜ ê²°ì • â”€â”€
            emotion_adaptive_mode = prosody_result.get("adaptive_mode", "normal")
        elif emotion_result:
            # â”€â”€ DeepFaceë§Œ ìˆì„ ë•Œ: 7ê°ì • ê¸°ë°˜ ê²°ì • (í´ë°±) â”€â”€
            dominant = emotion_result.get("dominant_emotion", "neutral")
            if dominant in ("sad", "fear", "disgust"):
                emotion_adaptive_mode = "encouraging"
            elif dominant in ("happy", "surprise"):
                emotion_adaptive_mode = "challenging"
            else:
                emotion_adaptive_mode = "normal"

        # ê°ì • ì´ë ¥ ì—…ë°ì´íŠ¸
        emotion_history = ws.get("emotion_history", [])
        if emotion_result:
            emotion_history.append(
                {
                    "timestamp": datetime.now().isoformat(),
                    "emotion": emotion_result,
                }
            )

        # Prosody ì´ë ¥ ì—…ë°ì´íŠ¸
        prosody_history = ws.get("prosody_history", [])
        if prosody_result:
            prosody_history.append(
                {
                    "timestamp": datetime.now().isoformat(),
                    "prosody": prosody_result,
                }
            )

        # í‰ê°€ ëˆ„ì 
        evaluations = ws.get("evaluations", [])
        if eval_result:
            evaluations.append(eval_result)

        elapsed = (time.perf_counter() - t0) * 1000
        trace = ws.get("trace", [])
        trace.append(
            _trace_entry(
                "evaluate",
                f"celery_task={pending_task_id or 'N/A'}, "
                f"emotion={emotion_adaptive_mode}, "
                f"prosody={prosody_result.get('dominant_indicator') if prosody_result else 'N/A'}, "
                f"fusion={'yes' if multimodal_fusion else 'no'}, "
                f"eval_score={eval_result.get('total_score') if eval_result else 'pending'}",
                elapsed,
            )
        )

        return {
            "phase": InterviewPhase.EVALUATE.value,
            "last_evaluation": eval_result,
            "evaluations": evaluations,
            "last_emotion": emotion_result,
            "emotion_history": emotion_history,
            "emotion_adaptive_mode": emotion_adaptive_mode,
            "last_prosody": prosody_result,
            "prosody_history": prosody_history,
            "pending_eval_task_id": pending_task_id,
            "rag_resume_context": rag_resume_ctx,
            "rag_qa_context": rag_qa_ctx,
            "trace": trace,
        }

    # ------------------------------------------------------------------ #
    #  4. route_next â€” ì¡°ê±´ë¶€ ë¶„ê¸° ê²°ì •                                      #
    # ------------------------------------------------------------------ #
    async def route_next(self, ws: WorkflowState) -> Dict:
        """
        ë‹¤ìŒ ì•¡ì…˜ì„ ê²°ì •í•˜ëŠ” ë¼ìš°íŒ… ë…¸ë“œ.
        - MAX_QUESTIONS ë„ë‹¬ â†’ complete
        - ê¼¬ë¦¬ì§ˆë¬¸ í•„ìš” â†’ follow_up
        - ê·¸ ì™¸ â†’ generate_question
        """
        t0 = time.perf_counter()
        session_id = ws["session_id"]
        user_input = ws.get("user_input", "")

        session = self._state_mgr.get_session(session_id)
        question_count = ws.get(
            "question_count", session.get("question_count", 1) if session else 1
        )
        max_questions = ws.get("max_questions", 10)

        # â”€â”€ ê¼¬ë¦¬ì§ˆë¬¸ íŒë‹¨ â”€â”€
        needs_follow_up = False
        follow_up_reason = ""
        topic_count = ws.get("topic_question_count", 0)

        if user_input and user_input not in ("[START]", "[NEXT]"):
            needs_follow_up, follow_up_reason = self._interviewer.should_follow_up(
                session_id, user_input
            )
            # ì£¼ì œë‹¹ 2íšŒ ì´ìƒì´ë©´ ê¼¬ë¦¬ì§ˆë¬¸ ì¤‘ë‹¨
            if topic_count >= 2:
                needs_follow_up = False
                follow_up_reason = "ì£¼ì œ ì „í™˜ í•„ìš”"

        # LangGraphì˜ ë…¸ë“œ(Node) ë‚´ë¶€ì—ì„œ ì‹¤í–‰ë˜ëŠ” ë¡œì§ìœ¼ë¡œ, ì‚¬ìš©ìì˜ ê°ì • ìƒíƒœì— ë”°ë¼ ê·¸ë˜í”„ì˜ íë¦„(ë‹¤ìŒ ë™ì‘)ì„ ê²°ì •í•˜ëŠ” ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì˜ í•µì‹¬ ë¡œì§
        # â”€â”€ ê°ì • ê¸°ë°˜ ì ì‘ (ì¡°ê±´ë¶€ ë¶„ê¸° í™•ì¥) â”€â”€
        emotion_mode = ws.get("emotion_adaptive_mode", "normal")
        # ë¶ˆì•ˆ/ê³µí¬ ê°ì§€ ì‹œ â†’ ê¼¬ë¦¬ì§ˆë¬¸/ì••ë°• ì§ˆë¬¸ ì™„í™”
        if emotion_mode == "encouraging" and needs_follow_up:
            needs_follow_up = False
            follow_up_reason = "ê°ì • ì ì‘: ê²©ë ¤ ëª¨ë“œ (ê¼¬ë¦¬ì§ˆë¬¸ ì™„í™”)"

        elapsed = (time.perf_counter() - t0) * 1000
        trace = ws.get("trace", [])
        trace.append(
            _trace_entry(
                "route_next",
                f"q={question_count}/{max_questions}, follow_up={needs_follow_up}, "
                f"emotion_mode={emotion_mode}, reason={follow_up_reason}",
                elapsed,
            )
        )

        return {
            "phase": InterviewPhase.ROUTE_NEXT.value,
            "needs_follow_up": needs_follow_up,
            "follow_up_reason": follow_up_reason,
            "follow_up_mode": needs_follow_up,
            "trace": trace,
        }

    # ------------------------------------------------------------------ #
    #  5. generate_question â€” LLM ì§ˆë¬¸ ìƒì„± (ë…ë¦½ì  ìƒíƒœ ë³€í™˜)              #
    # ------------------------------------------------------------------ #
    async def generate_question(self, ws: WorkflowState) -> Dict:
        """Stateì—ì„œ ì¡°ë¦½ëœ ë°ì´í„°ë¡œ LLM ì§ˆë¬¸ ìƒì„± (ë…ë¦½ì  ìƒíƒœ ë³€í™˜)

        ì´ì „ Thin Wrapper ë°©ì‹ê³¼ ë‹¬ë¦¬, ê° ë…¸ë“œê°€ ë‹´ë‹¹í•˜ëŠ” ë°ì´í„°ë¥¼ Stateì—ì„œ
        ì§ì ‘ ìˆ˜ì‹ í•˜ì—¬ build_and_call_llm()ì— ì „ë‹¬í•©ë‹ˆë‹¤:
          - RAG ì»¨í…ìŠ¤íŠ¸: evaluate ë…¸ë“œì—ì„œ ì‚¬ì „ ì¡°íšŒ â†’ State ê²½ìœ 
          - ê°ì • ì ì‘: evaluate ë…¸ë“œì—ì„œ ê²°ì • â†’ State ê²½ìœ 
          - ê¼¬ë¦¬ì§ˆë¬¸ ì •ë³´: route_next ë…¸ë“œì—ì„œ íŒë‹¨ â†’ State ê²½ìœ 
          - ì£¼ì œ ì¶”ì /ì§ˆë¬¸ ì¹´ìš´íŠ¸: ì´ ë…¸ë“œì—ì„œë§Œ 1íšŒ ì—…ë°ì´íŠ¸ (ì¤‘ë³µ ì œê±°)
        """
        t0 = time.perf_counter()
        session_id = ws["session_id"]
        user_input = ws.get("user_input", "")

        session = self._state_mgr.get_session(session_id)
        if not session:
            return {"error_info": "ì„¸ì…˜ ì—†ìŒ", "phase": InterviewPhase.ERROR.value}

        # â”€â”€ Stateì—ì„œ ì‚¬ì „ ì¡°íšŒëœ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì‹  (evaluate ë…¸ë“œì—ì„œ ì¤€ë¹„) â”€â”€
        resume_context = ws.get("rag_resume_context", "")
        qa_context = ws.get("rag_qa_context", "")
        emotion_mode = ws.get("emotion_adaptive_mode", "normal")
        needs_follow_up = ws.get("needs_follow_up", False)
        follow_up_reason = ws.get("follow_up_reason", "")
        current_topic = ws.get("current_topic", "general")
        topic_count = ws.get("topic_question_count", 0)

        # â”€â”€ ê°ì • ì ì‘ ëª¨ë“œ ì„¸ì…˜ì— ë™ê¸°í™” â”€â”€
        if emotion_mode != "normal":
            self._state_mgr.update_session(
                session_id, {"emotion_adaptive_mode": emotion_mode}
            )

        # â”€â”€ LLM ì§ˆë¬¸ ìƒì„± (build_and_call_llm: ìˆœìˆ˜ í”„ë¡¬í”„íŠ¸ ì¡°ë¦½ + LLM í˜¸ì¶œ) â”€â”€
        # íŒë‹¨ ë¡œì§(should_follow_up, RAG, topic_tracking)ì€ ì´ë¯¸ ê° ë…¸ë“œì—ì„œ ì²˜ë¦¬ë¨
        question = await self._interviewer.build_and_call_llm(
            session_id,
            user_input,
            resume_context=resume_context,
            qa_context=qa_context,
            needs_follow_up=needs_follow_up,
            follow_up_reason=follow_up_reason,
            current_topic=current_topic,
            topic_count=topic_count,
            emotion_mode=emotion_mode,
        )

        # â”€â”€ ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸ â”€â”€
        session = self._state_mgr.get_session(session_id)
        chat_history = session.get("chat_history", [])
        chat_history.append({"role": "assistant", "content": question})

        # â”€â”€ ì£¼ì œ ì¶”ì  ì—…ë°ì´íŠ¸ (ì´ ë…¸ë“œì—ì„œë§Œ 1íšŒ í˜¸ì¶œ â€” ì¤‘ë³µ ì œê±°) â”€â”€
        if user_input and user_input not in ("[START]", "[NEXT]"):
            is_follow_up = ws.get("follow_up_mode", False)
            self._interviewer.update_topic_tracking(
                session_id, user_input, is_follow_up
            )

        # â”€â”€ ì§ˆë¬¸ ì¹´ìš´íŠ¸ ì¦ê°€ (ì´ ë…¸ë“œì—ì„œë§Œ 1íšŒ ì²˜ë¦¬ â€” ì¤‘ë³µ ì œê±°) â”€â”€
        question_count = session.get("question_count", 1)
        new_q_count = question_count + 1
        self._state_mgr.update_session(
            session_id,
            {
                "chat_history": chat_history,
                "question_count": new_q_count,
            },
        )

        session = self._state_mgr.get_session(session_id)

        elapsed = (time.perf_counter() - t0) * 1000
        trace = ws.get("trace", [])
        trace.append(
            _trace_entry(
                "generate_question",
                f"ì§ˆë¬¸ ê¸¸ì´={len(question)}ì, emotion_mode={emotion_mode}, "
                f"q_count={new_q_count}, rag_resume={bool(resume_context)}, "
                f"rag_qa={bool(qa_context)}",
                elapsed,
            )
        )

        return {
            "response": question,
            "phase": InterviewPhase.GENERATE_QUESTION.value,
            "question_count": new_q_count,
            "current_topic": session.get("current_topic", "general"),
            "topic_question_count": session.get("topic_question_count", 0),
            "trace": trace,
        }

    # ------------------------------------------------------------------ #
    #  6. follow_up â€” ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„± (ë…ë¦½ì  ìƒíƒœ ë³€í™˜)                       #
    # ------------------------------------------------------------------ #
    async def follow_up(self, ws: WorkflowState) -> Dict:
        """ê¼¬ë¦¬ì§ˆë¬¸ ì „ìš© ë…¸ë“œ â€” Stateì—ì„œ ì¡°ë¦½ëœ ë°ì´í„°ë¡œ LLM í˜¸ì¶œ

        generate_questionê³¼ ë™ì¼í•˜ê²Œ build_and_call_llm()ì„ ì‚¬ìš©í•˜ë˜,
        needs_follow_up=Trueë¥¼ ê°•ì œ ì „ë‹¬í•˜ì—¬ ê¼¬ë¦¬ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸ë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤.
        ì£¼ì œ ì¶”ì  ì‹œ is_follow_up=Trueë¡œ ê¸°ë¡í•˜ì—¬ ì¶”ì  í†µê³„ì— ë°˜ì˜í•©ë‹ˆë‹¤.
        """
        t0 = time.perf_counter()
        session_id = ws["session_id"]
        user_input = ws.get("user_input", "")

        # â”€â”€ Stateì—ì„œ ì‚¬ì „ ì¡°íšŒëœ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì‹  â”€â”€
        resume_context = ws.get("rag_resume_context", "")
        qa_context = ws.get("rag_qa_context", "")
        emotion_mode = ws.get("emotion_adaptive_mode", "normal")
        follow_up_reason = ws.get("follow_up_reason", "")
        current_topic = ws.get("current_topic", "general")
        topic_count = ws.get("topic_question_count", 0)

        # follow_up_mode ì„¸íŒ…
        self._state_mgr.update_session(session_id, {"follow_up_mode": True})

        # â”€â”€ LLM ì§ˆë¬¸ ìƒì„± (ê¼¬ë¦¬ì§ˆë¬¸ ëª¨ë“œ: needs_follow_up=True ê°•ì œ) â”€â”€
        question = await self._interviewer.build_and_call_llm(
            session_id,
            user_input,
            resume_context=resume_context,
            qa_context=qa_context,
            needs_follow_up=True,
            follow_up_reason=follow_up_reason,
            current_topic=current_topic,
            topic_count=topic_count,
            emotion_mode=emotion_mode,
        )

        # â”€â”€ ëŒ€í™” ê¸°ë¡ + ì£¼ì œ ì¶”ì  + ì§ˆë¬¸ ì¹´ìš´íŠ¸ â”€â”€
        session = self._state_mgr.get_session(session_id)
        chat_history = session.get("chat_history", [])
        chat_history.append({"role": "assistant", "content": question})

        # ì£¼ì œ ì¶”ì  (ê¼¬ë¦¬ì§ˆë¬¸: is_follow_up=True)
        self._interviewer.update_topic_tracking(session_id, user_input, True)

        # ì§ˆë¬¸ ì¹´ìš´íŠ¸ ì¦ê°€
        question_count = session.get("question_count", 1)
        new_q_count = question_count + 1
        self._state_mgr.update_session(
            session_id,
            {
                "chat_history": chat_history,
                "question_count": new_q_count,
            },
        )

        session = self._state_mgr.get_session(session_id)

        elapsed = (time.perf_counter() - t0) * 1000
        trace = ws.get("trace", [])
        trace.append(
            _trace_entry(
                "follow_up",
                f"ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±, ê¸¸ì´={len(question)}ì, reason={follow_up_reason}, "
                f"rag_resume={bool(resume_context)}, rag_qa={bool(qa_context)}",
                elapsed,
            )
        )

        return {
            "response": question,
            "phase": InterviewPhase.FOLLOW_UP.value,
            "question_count": new_q_count,
            "current_topic": session.get("current_topic", "general"),
            "topic_question_count": session.get("topic_question_count", 0),
            "follow_up_mode": True,
            "trace": trace,
        }

    # ------------------------------------------------------------------ #
    #  7. complete â€” ë©´ì ‘ ì¢…ë£Œ + ì›Œí¬í”Œë¡œìš° ì‹œì‘                              #
    # ------------------------------------------------------------------ #
    async def complete(self, ws: WorkflowState) -> Dict:
        """ë©´ì ‘ ì¢…ë£Œ ì²˜ë¦¬: ì¢…ë£Œ ë©”ì‹œì§€ + ë¦¬í¬íŠ¸ ìƒì„± ì›Œí¬í”Œë¡œìš° íŠ¸ë¦¬ê±°"""
        t0 = time.perf_counter()
        session_id = ws["session_id"]

        closing_msg = (
            "ë©´ì ‘ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤. ê²°ê³¼ ë³´ê³ ì„œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
        )

        # ì„œë²„ ì„¸ì…˜ ì—…ë°ì´íŠ¸
        session = self._state_mgr.get_session(session_id)
        if session:
            chat_history = session.get("chat_history", [])
            chat_history.append({"role": "assistant", "content": closing_msg})
            self._state_mgr.update_session(
                session_id,
                {
                    "chat_history": chat_history,
                    "status": "completed",
                },
            )

        # ë°±ê·¸ë¼ìš´ë“œ ë¦¬í¬íŠ¸ ì›Œí¬í”Œë¡œìš° ì‹œì‘
        try:
            await self._interviewer.start_interview_completion_workflow(session_id)
        except Exception as e:
            print(f"âš ï¸ [Workflow] ì™„ë£Œ ì›Œí¬í”Œë¡œìš° ì‹œì‘ ì‹¤íŒ¨: {e}")

        # ì´ë²¤íŠ¸ ë°œí–‰
        if self._event_bus:
            try:
                from events import EventType as AppEventType

                await self._event_bus.publish(
                    AppEventType.SESSION_ENDED,
                    session_id=session_id,
                    data={"reason": "max_questions_reached"},
                    source="interview_workflow",
                )
            except Exception:
                pass

        elapsed = (time.perf_counter() - t0) * 1000
        trace = ws.get("trace", [])
        trace.append(
            _trace_entry("complete", "ë©´ì ‘ ì¢…ë£Œ + ë¦¬í¬íŠ¸ ì›Œí¬í”Œë¡œìš° ì‹œì‘", elapsed)
        )

        return {
            "response": closing_msg,
            "phase": InterviewPhase.COMPLETE.value,
            "trace": trace,
        }

    # ------------------------------------------------------------------ #
    #  8. error â€” ì˜¤ë¥˜ ë³µêµ¬                                                #
    # ------------------------------------------------------------------ #
    async def error_recovery(self, ws: WorkflowState) -> Dict:
        """ì˜¤ë¥˜ ë°œìƒ ì‹œ í´ë°± ì‘ë‹µ"""
        t0 = time.perf_counter()
        error_info = ws.get("error_info", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")

        fallback_msg = "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

        trace = ws.get("trace", [])
        trace.append(
            _trace_entry(
                "error_recovery",
                f"ì˜¤ë¥˜: {error_info}",
                (time.perf_counter() - t0) * 1000,
            )
        )

        return {
            "response": fallback_msg,
            "phase": InterviewPhase.ERROR.value,
            "trace": trace,
        }


# ========================================================================== #
#                         Edge ë¼ìš°íŒ… í•¨ìˆ˜                                     #
# ========================================================================== #


def route_after_process(state: WorkflowState) -> str:
    """process_answer í›„ â†’ evaluate ë¡œ ì´ë™"""
    if state.get("error_info"):
        return "error_recovery"
    return "evaluate"


def route_after_evaluate(state: WorkflowState) -> str:
    """evaluate í›„ â†’ route_next ë¡œ ì´ë™"""
    if state.get("error_info"):
        return "error_recovery"
    return "route_next"


def route_after_routing(state: WorkflowState) -> str:
    """
    route_next í›„ ì¡°ê±´ë¶€ ë¶„ê¸°:
      - MAX ë„ë‹¬ â†’ complete
      - ê¼¬ë¦¬ì§ˆë¬¸  â†’ follow_up
      - ì¼ë°˜      â†’ generate_question
    """
    session_id = state.get("session_id", "")
    question_count = state.get("question_count", 1)
    max_questions = state.get("max_questions", 10)

    # ë©´ì ‘ ì¢…ë£Œ ì¡°ê±´
    if question_count >= max_questions:
        return "complete"

    # ê¼¬ë¦¬ì§ˆë¬¸ ë¶„ê¸°
    if state.get("needs_follow_up", False):
        return "follow_up"

    # ì¼ë°˜ ì§ˆë¬¸
    return "generate_question"


def route_initial(state: WorkflowState) -> str:
    """
    ìµœì´ˆ ì…ë ¥ ë¼ìš°íŒ…:
      - [START]  â†’ greeting
      - [NEXT]   â†’ generate_question
      - ì¼ë°˜ ë‹µë³€ â†’ process_answer
    """
    user_input = state.get("user_input", "")
    if user_input == "[START]":
        return "greeting"
    elif user_input == "[NEXT]":
        return "generate_question"
    else:
        return "process_answer"


# ========================================================================== #
#                     InterviewWorkflow â€” ê·¸ë˜í”„ ë¹Œë”                          #
# ========================================================================== #


class InterviewWorkflow:
    """
    LangGraph StateGraph ë¥¼ ë¹Œë“œí•˜ê³  ì‹¤í–‰í•˜ëŠ” ë©”ì¸ í´ë˜ìŠ¤.
    MemorySaver ì²´í¬í¬ì¸í„°ë¡œ ì„¸ì…˜ë³„ ìƒíƒœ ì¤‘ë‹¨Â·ì¬ê°œë¥¼ ì§€ì›í•©ë‹ˆë‹¤.
    """

    def __init__(self, server_state, interviewer_instance, event_bus=None):
        self._nodes = InterviewNodes(server_state, interviewer_instance, event_bus)
        self._checkpointer = MemorySaver()
        self._graph = self._build_graph()
        self._compiled = self._graph.compile(checkpointer=self._checkpointer)

        # ì‹¤í–‰ ì¶”ì  ì €ì¥ì†Œ (session_id â†’ List[WorkflowState snapshots])
        self._execution_traces: Dict[str, List[Dict]] = {}

        print("âœ… LangGraph InterviewWorkflow ë¹Œë“œ ì™„ë£Œ (StateGraph + MemorySaver)")

    # ------------------------------------------------------------------ #
    #  ê·¸ë˜í”„ êµ¬ì¶•                                                          #
    # ------------------------------------------------------------------ #
    def _build_graph(self) -> StateGraph:
        """StateGraph ì •ì˜: ë…¸ë“œ + ì—£ì§€ + ì¡°ê±´ë¶€ ì—£ì§€"""
        builder = StateGraph(WorkflowState)

        # â”€â”€ ë…¸ë“œ ë“±ë¡ â”€â”€
        builder.add_node("greeting", self._nodes.greeting)
        builder.add_node("process_answer", self._nodes.process_answer)
        builder.add_node("evaluate", self._nodes.evaluate)
        builder.add_node("route_next", self._nodes.route_next)
        builder.add_node("generate_question", self._nodes.generate_question)
        builder.add_node("follow_up", self._nodes.follow_up)
        builder.add_node("complete", self._nodes.complete)
        builder.add_node("error_recovery", self._nodes.error_recovery)

        # â”€â”€ ì‹œì‘ â†’ ì´ˆê¸° ë¼ìš°íŒ… (ì¡°ê±´ë¶€) â”€â”€
        builder.add_conditional_edges(
            START,
            route_initial,
            {
                "greeting": "greeting",
                "generate_question": "generate_question",
                "process_answer": "process_answer",
            },
        )

        # â”€â”€ greeting â†’ END (í„´ ì¢…ë£Œ, ë‹¤ìŒ í˜¸ì¶œ ëŒ€ê¸°) â”€â”€
        builder.add_edge("greeting", END)

        # â”€â”€ process_answer â†’ evaluate (ì¡°ê±´ë¶€: ì˜¤ë¥˜ ì‹œ error_recovery) â”€â”€
        builder.add_conditional_edges(
            "process_answer",
            route_after_process,
            {
                "evaluate": "evaluate",
                "error_recovery": "error_recovery",
            },
        )

        # â”€â”€ evaluate â†’ route_next (ì¡°ê±´ë¶€: ì˜¤ë¥˜ ì‹œ error_recovery) â”€â”€
        builder.add_conditional_edges(
            "evaluate",
            route_after_evaluate,
            {
                "route_next": "route_next",
                "error_recovery": "error_recovery",
            },
        )

        # â”€â”€ route_next â†’ {complete | follow_up | generate_question} (ì¡°ê±´ë¶€ ë¶„ê¸°) â”€â”€
        builder.add_conditional_edges(
            "route_next",
            route_after_routing,
            {
                "complete": "complete",
                "follow_up": "follow_up",
                "generate_question": "generate_question",
            },
        )

        # â”€â”€ generate_question â†’ END (í„´ ì¢…ë£Œ) â”€â”€
        builder.add_edge("generate_question", END)

        # â”€â”€ follow_up â†’ END (í„´ ì¢…ë£Œ) â”€â”€
        builder.add_edge("follow_up", END)

        # â”€â”€ complete â†’ END â”€â”€
        builder.add_edge("complete", END)

        # â”€â”€ error_recovery â†’ END â”€â”€
        builder.add_edge("error_recovery", END)

        return builder

    # ------------------------------------------------------------------ #
    #  ì‹¤í–‰                                                                #
    # ------------------------------------------------------------------ #
    async def run(
        self,
        session_id: str,
        user_input: str,
        *,
        use_rag: bool = True,
        celery_available: bool = False,
        llm_available: bool = True,
        extra_state: Optional[Dict] = None,
    ) -> Dict:
        """
        í•œ í„´ì˜ ë©´ì ‘ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

        Parameters
        ----------
        session_id : str â€” ë©´ì ‘ ì„¸ì…˜ ID
        user_input : str â€” ì‚¬ìš©ì ì…ë ¥ ("[START]", "[NEXT]", ë˜ëŠ” ë‹µë³€ í…ìŠ¤íŠ¸)
        use_rag    : bool â€” RAG ì‚¬ìš© ì—¬ë¶€
        celery_available : bool â€” Celery ê°€ìš© ì—¬ë¶€
        llm_available : bool â€” LLM ê°€ìš© ì—¬ë¶€
        extra_state : dict â€” ì¶”ê°€ ì´ˆê¸° ìƒíƒœ

        Returns
        -------
        Dict â€” ìµœì¢… WorkflowState (response, phase, trace ë“± í¬í•¨)
        """
        # â”€â”€ ì´ˆê¸° ìƒíƒœ êµ¬ì„± â”€â”€
        # ì„œë²„ ì„¸ì…˜ì—ì„œ ê¸°ì¡´ ìƒíƒœ ë³µì›
        session = self._nodes._state_mgr.get_session(session_id)
        current_question_count = 1
        current_topic = "general"
        topic_question_count = 0
        emotion_adaptive_mode = "normal"

        if session:
            current_question_count = session.get("question_count", 1)
            current_topic = session.get("current_topic", "general")
            topic_question_count = session.get("topic_question_count", 0)
            emotion_adaptive_mode = session.get("emotion_adaptive_mode", "normal")

        initial_state: WorkflowState = {
            "session_id": session_id,
            "user_input": user_input,
            "phase": InterviewPhase.IDLE.value,
            "response": "",
            "question_count": current_question_count,
            "max_questions": 10,
            "current_topic": current_topic,
            "topic_question_count": topic_question_count,
            "topic_history": session.get("topic_history", []) if session else [],
            "follow_up_mode": session.get("follow_up_mode", False)
            if session
            else False,
            "follow_up_reason": "",
            "needs_follow_up": False,
            "last_evaluation": None,
            "evaluations": session.get("evaluations", []) if session else [],
            "pending_eval_task_id": None,
            "last_emotion": None,
            "emotion_history": [],
            "emotion_adaptive_mode": emotion_adaptive_mode,
            "chat_history": session.get("chat_history", []) if session else [],
            "rag_resume_context": "",  # evaluate ë…¸ë“œì—ì„œ RAG ë³‘ë ¬ ì¡°íšŒ í›„ ì €ì¥
            "rag_qa_context": "",  # evaluate ë…¸ë“œì—ì„œ RAG ë³‘ë ¬ ì¡°íšŒ í›„ ì €ì¥
            "trace": [],
            "error_info": None,
            "use_rag": use_rag,
            "celery_available": celery_available,
            "llm_available": llm_available,
        }

        if extra_state:
            initial_state.update(extra_state)

        # â”€â”€ LangGraph ì‹¤í–‰ (ì²´í¬í¬ì¸íŠ¸ ì„¤ì •) â”€â”€
        config = {
            "configurable": {
                "thread_id": session_id,
            }
        }

        try:
            result = await self._compiled.ainvoke(initial_state, config=config)
        except Exception as e:
            print(f"âŒ [Workflow] ê·¸ë˜í”„ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            traceback.print_exc()
            result = {
                "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "phase": InterviewPhase.ERROR.value,
                "trace": [_trace_entry("graph_error", str(e))],
                "error_info": str(e),
            }

        # â”€â”€ ì‹¤í–‰ ì¶”ì  ì €ì¥ â”€â”€
        if session_id not in self._execution_traces:
            self._execution_traces[session_id] = []
        self._execution_traces[session_id].append(
            {
                "turn": len(self._execution_traces[session_id]) + 1,
                "user_input": user_input[:100],
                "phase": result.get("phase", "unknown"),
                "response_preview": result.get("response", "")[:100],
                "trace": result.get("trace", []),
                "timestamp": datetime.now().isoformat(),
            }
        )

        return result

    # ------------------------------------------------------------------ #
    #  ì²´í¬í¬ì¸íŠ¸ ê´€ë ¨                                                       #
    # ------------------------------------------------------------------ #
    def get_checkpoint(self, session_id: str) -> Optional[Dict]:
        """ì„¸ì…˜ì˜ ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ ìƒíƒœë¥¼ ë°˜í™˜"""
        config = {"configurable": {"thread_id": session_id}}
        try:
            checkpoint = self._checkpointer.get(config)
            if checkpoint:
                return {
                    "session_id": session_id,
                    "checkpoint_id": checkpoint.get("id"),
                    "timestamp": checkpoint.get("ts"),
                    "channel_values": checkpoint.get("channel_values", {}),
                }
        except Exception as e:
            print(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return None

    def list_checkpoints(self, session_id: str, limit: int = 10) -> List[Dict]:
        """ì„¸ì…˜ì˜ ì²´í¬í¬ì¸íŠ¸ ì´ë ¥ì„ ë°˜í™˜"""
        config = {"configurable": {"thread_id": session_id}}
        results = []
        try:
            for cp in self._checkpointer.list(config, limit=limit):
                results.append(
                    {
                        "checkpoint_id": cp.get("id"),
                        "timestamp": cp.get("ts"),
                        "metadata": cp.get("metadata", {}),
                    }
                )
        except Exception as e:
            print(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return results

    # ------------------------------------------------------------------ #
    #  ì‹œê°í™” / ê°ì‚¬                                                        #
    # ------------------------------------------------------------------ #
    def get_graph_mermaid(self) -> str:
        """Mermaid ë‹¤ì´ì–´ê·¸ë¨ ë¬¸ìì—´ì„ ë°˜í™˜"""
        try:
            return self._compiled.get_graph().draw_mermaid()
        except Exception as e:
            print(f"âš ï¸ Mermaid ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: ìˆ˜ë™ Mermaid
            return self._fallback_mermaid()

    def _fallback_mermaid(self) -> str:
        """draw_mermaid() ì‹¤íŒ¨ ì‹œ ìˆ˜ë™ Mermaid ë‹¤ì´ì–´ê·¸ë¨"""
        return """graph TD
    __start__([START]) --> |"[START]"| greeting
    __start__ --> |"[NEXT]"| generate_question
    __start__ --> |"ë‹µë³€"| process_answer
    greeting --> __end__([END])
    process_answer --> evaluate
    process_answer --> |"ì˜¤ë¥˜"| error_recovery
    evaluate --> route_next
    evaluate --> |"ì˜¤ë¥˜"| error_recovery
    route_next --> |"MAX ë„ë‹¬"| complete
    route_next --> |"ê¼¬ë¦¬ì§ˆë¬¸"| follow_up
    route_next --> |"ì¼ë°˜"| generate_question
    generate_question --> __end__
    follow_up --> __end__
    complete --> __end__
    error_recovery --> __end__

    style greeting fill:#4ade80,color:#000
    style evaluate fill:#60a5fa,color:#000
    style route_next fill:#fbbf24,color:#000
    style follow_up fill:#f97316,color:#000
    style complete fill:#a78bfa,color:#000
    style error_recovery fill:#f87171,color:#000
"""

    def get_execution_trace(self, session_id: str) -> List[Dict]:
        """ì„¸ì…˜ì˜ ì „ì²´ ì‹¤í–‰ ì¶”ì  ì´ë ¥ì„ ë°˜í™˜"""
        return self._execution_traces.get(session_id, [])

    def get_current_state_summary(self, session_id: str) -> Dict:
        """ì„¸ì…˜ì˜ í˜„ì¬ ì›Œí¬í”Œë¡œìš° ìƒíƒœ ìš”ì•½"""
        checkpoint = self.get_checkpoint(session_id)
        traces = self.get_execution_trace(session_id)

        return {
            "session_id": session_id,
            "total_turns": len(traces),
            "last_phase": traces[-1]["phase"] if traces else "idle",
            "last_timestamp": traces[-1]["timestamp"] if traces else None,
            "checkpoint_available": checkpoint is not None,
            "traces": traces,
        }

    def get_graph_definition(self) -> Dict:
        """ì •ì  ê·¸ë˜í”„ êµ¬ì¡° ì •ë³´ë¥¼ ë°˜í™˜"""
        return {
            "nodes": [
                {"id": "greeting", "description": "ì¸ì‚¬ / ìê¸°ì†Œê°œ ìš”ì²­"},
                {"id": "process_answer", "description": "ì‚¬ìš©ì ë‹µë³€ ìˆ˜ì‹  ë° ì „ì²˜ë¦¬"},
                {"id": "evaluate", "description": "ë‹µë³€ í‰ê°€ + ê°ì • ë¶„ì„ (ë³‘ë ¬)"},
                {"id": "route_next", "description": "ì¡°ê±´ë¶€ ë¶„ê¸° ê²°ì •"},
                {"id": "generate_question", "description": "LLM ì§ˆë¬¸ ìƒì„±"},
                {"id": "follow_up", "description": "ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±"},
                {"id": "complete", "description": "ë©´ì ‘ ì¢…ë£Œ + ë¦¬í¬íŠ¸ ìƒì„±"},
                {"id": "error_recovery", "description": "ì˜¤ë¥˜ ë³µêµ¬"},
            ],
            "edges": [
                {
                    "from": "START",
                    "to": "greeting",
                    "condition": "user_input == '[START]'",
                },
                {
                    "from": "START",
                    "to": "generate_question",
                    "condition": "user_input == '[NEXT]'",
                },
                {"from": "START", "to": "process_answer", "condition": "ì¼ë°˜ ë‹µë³€"},
                {"from": "greeting", "to": "END", "condition": None},
                {"from": "process_answer", "to": "evaluate", "condition": "ì •ìƒ"},
                {"from": "process_answer", "to": "error_recovery", "condition": "ì˜¤ë¥˜"},
                {"from": "evaluate", "to": "route_next", "condition": "ì •ìƒ"},
                {"from": "evaluate", "to": "error_recovery", "condition": "ì˜¤ë¥˜"},
                {
                    "from": "route_next",
                    "to": "complete",
                    "condition": "question_count >= max_questions",
                },
                {
                    "from": "route_next",
                    "to": "follow_up",
                    "condition": "needs_follow_up && topic_count < 2",
                },
                {"from": "route_next", "to": "generate_question", "condition": "ê·¸ ì™¸"},
                {"from": "generate_question", "to": "END", "condition": None},
                {"from": "follow_up", "to": "END", "condition": None},
                {"from": "complete", "to": "END", "condition": None},
                {"from": "error_recovery", "to": "END", "condition": None},
            ],
            "conditional_features": [
                "ê°ì • ê¸°ë°˜ ì ì‘ ëª¨ë“œ (encouraging/challenging/normal)",
                "ê¼¬ë¦¬ì§ˆë¬¸ ë£¨í”„ ì œì–´ (ì£¼ì œë‹¹ 2íšŒ ì œí•œ)",
                "MAX_QUESTIONS ë„ë‹¬ ì‹œ ìë™ ì¢…ë£Œ",
            ],
            "checkpointer": "MemorySaver (ì„¸ì…˜ë³„ ìƒíƒœ ì¤‘ë‹¨Â·ì¬ê°œ)",
            "parallel_processing": "evaluate ë…¸ë“œì—ì„œ ë‹µë³€ í‰ê°€ + ê°ì • ë¶„ì„ ë™ì‹œ ì‹¤í–‰",
        }


# ========================================================================== #
#                         Singleton / Factory                                 #
# ========================================================================== #

_workflow_instance: Optional[InterviewWorkflow] = None


def init_workflow(
    server_state, interviewer_instance, event_bus=None
) -> InterviewWorkflow:
    """ì›Œí¬í”Œë¡œìš° ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•˜ê³  ë°˜í™˜"""
    global _workflow_instance
    _workflow_instance = InterviewWorkflow(
        server_state, interviewer_instance, event_bus
    )
    return _workflow_instance


def get_workflow_instance() -> Optional[InterviewWorkflow]:
    """ì´ˆê¸°í™”ëœ ì›Œí¬í”Œë¡œìš° ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜"""
    return _workflow_instance
