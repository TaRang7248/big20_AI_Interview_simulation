# Llama 3ì„ í™œìš©í•œ ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ AI ë©´ì ‘ í”„ë¡œê·¸ë¨
# LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ Llama 3 ëª¨ë¸ê³¼ ëŒ€í™”í•˜ë©°, ë©´ì ‘ê´€ í˜ë¥´ì†Œë‚˜ë¥¼ ê°€ì§„ AIê°€ ì§ˆë¬¸ì„ í•˜ê³  ì‚¬ìš©ìê°€ ë‹µí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬í˜„

# ìš´ì˜ì²´ì œ(OS)ì˜ ê¸°ëŠ¥ì„ íŒŒì´ì¬ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ëª¨ë“ˆ. ì£¼ë¡œ API í‚¤ì™€ ê°™ì€ í™˜ê²½ ë³€ìˆ˜ë¥¼ .env íŒŒì¼ì—ì„œ ê°€ì ¸ì˜¬ ë•Œ ì‚¬ìš©
import os
# íŒŒì´ì¬ ì¸í„°í”„ë¦¬í„°ì™€ ì‹œìŠ¤í…œ ê´€ë ¨ ì„¤ì •ì„ ì œì–´
import sys
# .env íŒŒì¼ì— ì €ì¥ëœ ë¹„ë°€ ì •ë³´(OpenAI API í‚¤ ë“±)ë¥¼ ì½ì–´ì™€ì„œ ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ë¡œ ë“±ë¡í•´ ì£¼ëŠ” ë„êµ¬
from dotenv import load_dotenv
# LangChainì—ì„œ ì œê³µí•˜ëŠ” Ollama ì „ìš© ì±„íŒ… ëª¨ë¸ ì—°ê²° ë„êµ¬. ì´ë¥¼ í†µí•´ Llama 3 ê°™ì€ ëª¨ë¸ê³¼ ëŒ€í™”í•  ìˆ˜ ìˆë‹¤.
from langchain_ollama import ChatOllama
# RAG ê¸°ëŠ¥ì„ ìœ„í•œ ëª¨ë“ˆ ì„í¬íŠ¸
from resume_rag import ResumeRAG
# AI ëŒ€í™”ì— ì“°ì´ëŠ” ë©”ì‹œì§€ íƒ€ì…ì„ ì •ì˜
# HumanMessage: ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë©”ì‹œì§€
# AIMessage: AIê°€ ìƒì„±í•œ ë©”ì‹œì§€
# SystemMessage: AIì˜ ì¸ê²©(í˜ë¥´ì†Œë‚˜)ê³¼ ê·œì¹™ì„ ë¶€ì—¬í•˜ëŠ” ë©”ì‹œì§€
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

# ì •ê·œ í‘œí˜„ì‹(Regular Expression)ì„ ì‚¬ìš©í•˜ëŠ” ë„êµ¬
import re
# ë‚ ì§œì™€ ì‹œê°„ì„ ë‹¤ë£¨ëŠ” ë„êµ¬
from datetime import datetime
# ë¦¬ìŠ¤íŠ¸ ì•ˆì˜ ë‹¨ì–´ ë¹ˆë„ ìˆ˜ë¥¼ ì„¸ëŠ” ë„êµ¬
from collections import Counter

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ .env íŒŒì¼ì„ ì°¾ê¸° ìœ„í•´ ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.dirname(current_dir)
sys.path.append(root_dir)

# í”„ë¡œì íŠ¸ í´ë”ì— ìˆëŠ” .env íŒŒì¼ì— ì íŒ ì„¤ì •ê°’ë“¤ì„ ì½ì–´ì„œ íŒŒì´ì¬ í”„ë¡œê·¸ë¨ì´ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í™˜ê²½ ë³€ìˆ˜ë¡œ ë“±ë¡í•´ì£¼ëŠ” í•¨ìˆ˜
load_dotenv()

# LLM ëª¨ë¸ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ë¡œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥)
DEFAULT_LLM_MODEL = os.getenv("LLM_MODEL", "llama3")
DEFAULT_LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.7")) 


class InterviewReportGenerator:
    """
    ë©´ì ‘ ì¢…ë£Œ í›„ STAR ê¸°ë²• ê¸°ë°˜ ì¢…í•© ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤
    - STAR ê¸°ë²•(Situation, Task, Action, Result) ë¶„ì„
    - í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
    - ë‹µë³€ êµ¬ì¡° í‰ê°€
    - ë°œí™” ì†ë„/ë°œìŒ ëª…í™•ì„±/ì‹œì„  ì²˜ë¦¬ ë“± ë¹„ì–¸ì–´ì  ìš”ì†Œ ë¶„ì„ (í™”ìƒ ë©´ì ‘ ë‚´ìš© ë°”íƒ•)
    """
    
    def __init__(self, llm):
        self.llm = llm
        # STAR ê¸°ë²• ê´€ë ¨ í‚¤ì›Œë“œ ì •ì˜
        self.star_keywords = {
            'situation': ['ìƒí™©', 'ë°°ê²½', 'ë‹¹ì‹œ', 'ê·¸ë•Œ', 'í™˜ê²½', 'ìƒíƒœ', 'ë¬¸ì œ', 'ì´ìŠˆ', 'ê³¼ì œ'],
            'task': ['ëª©í‘œ', 'ê³¼ì œ', 'ì„ë¬´', 'ì—­í• ', 'ë‹´ë‹¹', 'ì±…ì„', 'í•´ì•¼ í• ', 'ëª©ì ', 'ë¯¸ì…˜'],
            'action': ['í–‰ë™', 'ìˆ˜í–‰', 'ì‹¤í–‰', 'ì²˜ë¦¬', 'í•´ê²°', 'ê°œë°œ', 'êµ¬í˜„', 'ì ìš©', 'ì§„í–‰', 'ì‹œë„', 'ë…¸ë ¥'],
            'result': ['ê²°ê³¼', 'ì„±ê³¼', 'ë‹¬ì„±', 'ì™„ë£Œ', 'ê°œì„ ', 'í–¥ìƒ', 'ì¦ê°€', 'ê°ì†Œ', 'íš¨ê³¼', 'ì„±ê³µ', 'ì‹¤íŒ¨ì—ì„œ ë°°ìš´']
        }
        # IT ê´€ë ¨ í•µì‹¬ í‚¤ì›Œë“œ (ê¸°ìˆ  ìŠ¤íƒ)
        self.tech_keywords = [
            'python', 'java', 'javascript', 'typescript', 'react', 'vue', 'angular', 'node',
            'django', 'flask', 'spring', 'aws', 'azure', 'gcp', 'docker', 'kubernetes',
            'sql', 'nosql', 'mongodb', 'postgresql', 'mysql', 'redis', 'kafka',
            'git', 'ci/cd', 'devops', 'agile', 'scrum', 'api', 'rest', 'graphql',
            'machine learning', 'deep learning', 'ai', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 'ì¸ê³µì§€ëŠ¥',
            'tensorflow', 'pytorch', 'pandas', 'numpy', 'scikit-learn',
            'ë°ì´í„°', 'ë¶„ì„', 'ëª¨ë¸', 'ì•Œê³ ë¦¬ì¦˜', 'ìµœì í™”', 'í…ŒìŠ¤íŠ¸', 'ë°°í¬', 'LLM', 'RAG', 'LangChain', 'Spark', 'Hadoop',
            'Terraform', 'Linux', 'Prometheus', 'Grafana', 'Flutter', 'Swift', 'Kotlin', 'React Native', 
            'Next.js', 'Tailwind', 'Svelte', 'Redux', 'Go', 'C++', 'PHP', 'Ruby', 'FastAPI'
        ]
    
    def extract_user_answers(self, chat_history: list) -> list:
        """ëŒ€í™” ê¸°ë¡ì—ì„œ ì§€ì›ìì˜ ë‹µë³€ë§Œ ì¶”ì¶œ"""
        answers = []
        for msg in chat_history:
            if isinstance(msg, HumanMessage):
                answers.append(msg.content)
        return answers
    
    def analyze_star_structure(self, answers: list) -> dict:
        """
        STAR ê¸°ë²•ì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€ êµ¬ì¡°ë¥¼ ë¶„ì„
        ê° ë‹µë³€ì—ì„œ S, T, A, R ìš”ì†Œê°€ ì–¼ë§ˆë‚˜ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í‰ê°€
        """
        star_analysis = {
            'situation': {'count': 0, 'examples': []},
            'task': {'count': 0, 'examples': []},
            'action': {'count': 0, 'examples': []},
            'result': {'count': 0, 'examples': []}
        }
        
        for answer in answers:
            answer_lower = answer.lower()
            for star_element, keywords in self.star_keywords.items():
                for keyword in keywords:
                    if keyword in answer_lower:
                        star_analysis[star_element]['count'] += 1
                        # í‚¤ì›Œë“œ ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìµœëŒ€ 50ì)
                        idx = answer_lower.find(keyword)
                        start = max(0, idx - 20)
                        end = min(len(answer), idx + len(keyword) + 30)
                        context = answer[start:end]
                        if context not in star_analysis[star_element]['examples']:
                            star_analysis[star_element]['examples'].append(f"...{context}...")
                        break  # í•˜ë‚˜ì˜ í‚¤ì›Œë“œë§Œ ì¹´ìš´íŠ¸
        
        return star_analysis
    
    def extract_keywords(self, answers: list) -> dict:
        """ë‹µë³€ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        all_text = ' '.join(answers).lower()
        
        # ê¸°ìˆ  í‚¤ì›Œë“œ ì¶”ì¶œ
        found_tech_keywords = []
        for keyword in self.tech_keywords:
            if keyword.lower() in all_text:
                count = all_text.count(keyword.lower())
                found_tech_keywords.append((keyword, count))
        
        # ë¹ˆë„ìˆœ ì •ë ¬
        found_tech_keywords.sort(key=lambda x: x[1], reverse=True)
        
        # ë‹µë³€ ì „ì²´ì—ì„œ 2ê¸€ì ì´ìƒì˜ í•œê¸€ ë‹¨ì–´ë§Œ ëª¨ë‘ ê³¨ë¼ë‚¸ë‹¤
        korean_words = re.findall(r'[ê°€-í£]{2,}', all_text)
        # ê³¨ë¼ë‚¸ í•œê¸€ ë‹¨ì–´ë“¤ì´ ê°ê° ëª‡ ë²ˆì”© ë‚˜ì™”ëŠ”ì§€ ìë™ìœ¼ë¡œ ê³„ì‚°
        word_freq = Counter(korean_words)
        
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = ['ê·¸ë˜ì„œ', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ°ë°', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ê·¸ê²ƒ', 'ìˆìŠµë‹ˆë‹¤', 
                     'í–ˆìŠµë‹ˆë‹¤', 'í•©ë‹ˆë‹¤', 'ì…ë‹ˆë‹¤', 'ìŠµë‹ˆë‹¤', 'ê²ƒì…ë‹ˆë‹¤', 'ì˜€ìŠµë‹ˆë‹¤', 'ë©ë‹ˆë‹¤']
        for sw in stopwords:
            if sw in word_freq:
                del word_freq[sw]
        
        return {
            'tech_keywords': found_tech_keywords[:10],  # ìƒìœ„ 10ê°œ
            'general_keywords': word_freq.most_common(15)  # ìƒìœ„ 15ê°œ
        }
    
    # ì§€ì›ìê°€ ë‹µë³€ì„ ì–¼ë§ˆë‚˜ ì„±ì‹¤í•˜ê³  ê¸¸ê²Œ ì‘ì„±í–ˆëŠ”ì§€ 'ì–‘ì ì¸ ì¸¡ë©´'ì—ì„œ ë¶„ì„í•˜ëŠ” ê¸°ëŠ¥
    # ì´ ìˆ˜ì¹˜ë“¤ì€ ë‹¨ìˆœí•œ ìˆ«ìê°€ ì•„ë‹ˆë¼ ì§€ì›ìì˜ 'íƒœë„'ë¥¼ ë³´ì—¬ì£¼ëŠ” ë°ì´í„°ê°€ ëœë‹¤
    def calculate_answer_metrics(self, answers: list) -> dict:
        """ë‹µë³€ ê´€ë ¨ ê¸°ë³¸ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        if not answers:
            return {'total_answers': 0, 'avg_length': 0, 'total_chars': 0}
        
        total_chars = sum(len(a) for a in answers)
        avg_length = total_chars / len(answers)
        
        # ë‹µë³€ ê¸¸ì´ ë¶„í¬
        short_answers = sum(1 for a in answers if len(a) < 50)
        medium_answers = sum(1 for a in answers if 50 <= len(a) < 200)
        long_answers = sum(1 for a in answers if len(a) >= 200)
        
        return {
            'total_answers': len(answers), # ì´ ë‹µë³€ ê°œìˆ˜
            'avg_length': round(avg_length, 1), # í‰ê·  ê¸¸ì´ë¥¼ ì†Œìˆ˜ì  ì²«ì§¸ ìë¦¬ê¹Œì§€ ë°˜ì˜¬ë¦¼
            'total_chars': total_chars, # ì „ì²´ ê¸€ì ìˆ˜
            'short_answers': short_answers, # ì§§ì€ ë‹µë³€ ê°œìˆ˜
            'medium_answers': medium_answers, # ì¤‘ê°„ ë‹µë³€ ê°œìˆ˜
            'long_answers': long_answers # ê¸´ ë‹µë³€ ê°œìˆ˜
        }
    
    def generate_star_feedback(self, star_analysis: dict) -> str:
        """STAR ë¶„ì„ ê²°ê³¼ì— ê¸°ë°˜í•œ í”¼ë“œë°± ìƒì„±"""
        feedback = []
        
        total_elements = sum(star_analysis[k]['count'] for k in star_analysis)
        
        if total_elements == 0:
            return "âš ï¸ STAR ê¸°ë²• ìš”ì†Œê°€ ê±°ì˜ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. êµ¬ì²´ì ì¸ ìƒí™©, ê³¼ì œ, í–‰ë™, ê²°ê³¼ë¥¼ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ë©´ ë” íš¨ê³¼ì ì…ë‹ˆë‹¤."
        
        # ê° ìš”ì†Œë³„ í”¼ë“œë°±
        element_names = {
            'situation': ('ìƒí™©(Situation)', 'ë‹¹ì‹œ ìƒí™©ì´ë‚˜ ë°°ê²½'),
            'task': ('ê³¼ì œ(Task)', 'ë§¡ì€ ì—­í• ì´ë‚˜ í•´ê²°í•´ì•¼ í•  ëª©í‘œ'),
            'action': ('í–‰ë™(Action)', 'êµ¬ì²´ì ìœ¼ë¡œ ìˆ˜í–‰í•œ í–‰ë™'),
            'result': ('ê²°ê³¼(Result)', 'ë‹¬ì„±í•œ ì„±ê³¼ë‚˜ ë°°ìš´ ì ')
        }
        
        weak_elements = []
        strong_elements = []
        
        for element, (name, desc) in element_names.items():
            count = star_analysis[element]['count']
            if count == 0:
                weak_elements.append(f"{name}")
            elif count >= 3:
                strong_elements.append(f"{name}")
        
        if strong_elements:
            feedback.append(f"âœ… ê°•ì : {', '.join(strong_elements)} ìš”ì†Œê°€ ì˜ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        
        if weak_elements:
            feedback.append(f"ğŸ“ ê°œì„  í•„ìš”: {', '.join(weak_elements)} ìš”ì†Œë¥¼ ë” ë³´ì™„í•˜ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤.")
        
        return '\n'.join(feedback)
    
    def generate_ai_evaluation(self, chat_history: list, answers: list) -> str:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ì¢…í•© í‰ê°€ ìƒì„±
        ëŒ€í™” ê¸°ë¡(chat_history)ê³¼ ì§€ì›ìì˜ ë‹µë³€ ë¦¬ìŠ¤íŠ¸(answers)ë¥¼ ë°›ì•„ì„œ ìµœì¢… í‰ê°€ ê¸€(ë¬¸ìì—´)ì„ ë‚´ë†“ëŠ” í•¨ìˆ˜
        """
        if not answers:
            return "ë‹µë³€ì´ ì—†ì–´ í‰ê°€ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ëŒ€í™” ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        conversation_text = ""
        for msg in chat_history[1:]:  # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì œì™¸
            if isinstance(msg, AIMessage):
                conversation_text += f"ë©´ì ‘ê´€: {msg.content}\n"
            elif isinstance(msg, HumanMessage):
                conversation_text += f"ì§€ì›ì: {msg.content}\n"
        
        evaluation_prompt = f"""ë‹¤ìŒì€ ë©´ì ‘ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤. ì§€ì›ìì˜ ë‹µë³€ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.

[ë©´ì ‘ ëŒ€í™”]
{conversation_text}

[í‰ê°€ ê¸°ì¤€]
1. STAR ê¸°ë²• í™œìš©ë„ (ìƒí™©-ê³¼ì œ-í–‰ë™-ê²°ê³¼ êµ¬ì¡°)
2. ë‹µë³€ì˜ êµ¬ì²´ì„±ê³¼ ë…¼ë¦¬ì„±
3. ê¸°ìˆ ì  ì—­ëŸ‰ í‘œí˜„
4. ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ëŠ¥ë ¥
5. ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„

ìœ„ ê¸°ì¤€ì— ë”°ë¼ ì§€ì›ìì˜ ë©´ì ‘ ë‹µë³€ì„ í‰ê°€í•˜ê³ , ê° í•­ëª©ë³„ë¡œ 1~5ì  ì²™ë„ë¡œ ì ìˆ˜ë¥¼ ë§¤ê²¨ì£¼ì„¸ìš”. í‰ê°€í•œ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•©ê²© í˜¹ì€ ë¶ˆí•©ê²© ì—¬ë¶€ë„ íŒë‹¨í•´ì£¼ì„¸ìš”."""

        try:
            response = self.llm.invoke([HumanMessage(content=evaluation_prompt)])
            return response.content
        except Exception as e:
            return f"AI í‰ê°€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
    
    def generate_report(self, chat_history: list, video_metrics: dict = None) -> str:
        """
        ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
        
        Args:
            chat_history: ë©´ì ‘ ëŒ€í™” ê¸°ë¡
            video_metrics: ë¹„ë””ì˜¤ ë©´ì ‘ ì‹œ ë°œí™” ì†ë„, ë°œìŒ ëª…í™•ì„±, ì‹œì„  ì²˜ë¦¬ ë°ì´í„°
        """
        print("\n" + "="*60)
        print("ğŸ“Š ë©´ì ‘ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        print("="*60)
        
        # ì§€ì›ì ë‹µë³€ ì¶”ì¶œ
        answers = self.extract_user_answers(chat_history)
        
        if not answers:
            return "ë¶„ì„í•  ë‹µë³€ì´ ì—†ìŠµë‹ˆë‹¤."
        
        # ë¶„ì„ ìˆ˜í–‰
        star_analysis = self.analyze_star_structure(answers)
        keywords = self.extract_keywords(answers)
        metrics = self.calculate_answer_metrics(answers)
        star_feedback = self.generate_star_feedback(star_analysis)
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = []
        report.append("\n" + "="*60)
        report.append("ğŸ“‹ AI ëª¨ì˜ë©´ì ‘ ì¢…í•© ë¦¬í¬íŠ¸")
        report.append(f"ğŸ“… ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("="*60)
        
        # 1. ê¸°ë³¸ í†µê³„
        report.append("\n[1] ğŸ“ˆ ë‹µë³€ ê¸°ë³¸ í†µê³„")
        report.append("-" * 40)
        report.append(f"  â€¢ ì´ ë‹µë³€ ìˆ˜: {metrics['total_answers']}íšŒ")
        report.append(f"  â€¢ í‰ê·  ë‹µë³€ ê¸¸ì´: {metrics['avg_length']}ì")
        report.append(f"  â€¢ ì´ ë‹µë³€ ë¶„ëŸ‰: {metrics['total_chars']}ì")
        report.append("  â€¢ ë‹µë³€ ê¸¸ì´ ë¶„í¬:")
        report.append(f"    - ì§§ì€ ë‹µë³€(~50ì): {metrics['short_answers']}íšŒ")
        report.append(f"    - ì¤‘ê°„ ë‹µë³€(50~200ì): {metrics['medium_answers']}íšŒ")
        report.append(f"    - ê¸´ ë‹µë³€(200ì~): {metrics['long_answers']}íšŒ")
        
        # 2. STAR ê¸°ë²• ë¶„ì„
        report.append("\n[2] â­ STAR ê¸°ë²• ë¶„ì„")
        report.append("-" * 40)
        for element in ['situation', 'task', 'action', 'result']:
            element_kr = {'situation': 'ìƒí™©(S)', 'task': 'ê³¼ì œ(T)', 
                         'action': 'í–‰ë™(A)', 'result': 'ê²°ê³¼(R)'}[element]
            count = star_analysis[element]['count']
            bar = 'â–ˆ' * min(count, 10) + 'â–‘' * (10 - min(count, 10))
            report.append(f"  â€¢ {element_kr}: [{bar}] {count}íšŒ")
        
        report.append("\n  ğŸ’¡ STAR í”¼ë“œë°±:")
        for line in star_feedback.split('\n'):
            report.append(f"     {line}")
        
        # 3. í•µì‹¬ í‚¤ì›Œë“œ ë¶„ì„
        report.append("\n[3] ğŸ”‘ í•µì‹¬ í‚¤ì›Œë“œ ë¶„ì„")
        report.append("-" * 40)
        
        if keywords['tech_keywords']:
            report.append("  â€¢ ê¸°ìˆ  í‚¤ì›Œë“œ:")
            tech_str = ", ".join([f"{kw}({cnt}íšŒ)" for kw, cnt in keywords['tech_keywords'][:5]])
            report.append(f"    {tech_str}")
        
        if keywords['general_keywords']:
            report.append("  â€¢ ì£¼ìš” í‘œí˜„:")
            general_str = ", ".join([f"{kw}({cnt}íšŒ)" for kw, cnt in keywords['general_keywords'][:8]])
            report.append(f"    {general_str}")
        
        # 4. ë¹„ë””ì˜¤ ë©´ì ‘ ë©”íŠ¸ë¦­ (ì œê³µëœ ê²½ìš°)
        report.append("\n[4] ğŸ¥ ë¹„ì–¸ì–´ì  ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ë¶„ì„")
        report.append("-" * 40)
        if video_metrics:
            report.append(f"  â€¢ ë°œí™” ì†ë„: {video_metrics.get('speech_rate', 'N/A')}")
            report.append(f"  â€¢ ë°œìŒ ëª…í™•ì„±: {video_metrics.get('pronunciation_clarity', 'N/A')}")
            report.append(f"  â€¢ ì‹œì„  ì²˜ë¦¬: {video_metrics.get('eye_contact', 'N/A')}")
            report.append(f"  â€¢ í‘œì • ë¶„ì„: {video_metrics.get('facial_expression', 'N/A')}")
        else:
            report.append("  â„¹ï¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ ë©´ì ‘ìœ¼ë¡œ ë¹„ì–¸ì–´ì  ë¶„ì„ì´ ì œê³µë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            report.append("  ğŸ’¡ ë¹„ë””ì˜¤ ë©´ì ‘ ëª¨ë“œì—ì„œ ë°œí™” ì†ë„, ë°œìŒ ëª…í™•ì„±, ì‹œì„  ì²˜ë¦¬ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        
        # 5. AI ì¢…í•© í‰ê°€
        report.append("\n[5] ğŸ¤– AI ì¢…í•© í‰ê°€")
        report.append("-" * 40)
        print("  (AIê°€ ë©´ì ‘ ë‚´ìš©ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...)")
        ai_evaluation = self.generate_ai_evaluation(chat_history, answers)
        for line in ai_evaluation.split('\n'):
            report.append(f"  {line}")
        
        # 6. ê°œì„  ì œì•ˆ
        report.append("\n[6] ğŸ“ ê°œì„  ì œì•ˆ")
        report.append("-" * 40)
        
        suggestions = []
        if metrics['short_answers'] > metrics['long_answers']:
            suggestions.append("â€¢ ë‹µë³€ì„ ë” êµ¬ì²´ì ì´ê³  ìƒì„¸í•˜ê²Œ ì‘ì„±í•´ë³´ì„¸ìš”.")
        if star_analysis['result']['count'] < 2:
            suggestions.append("â€¢ ê²½í—˜ì˜ 'ê²°ê³¼'ì™€ 'ì„±ê³¼'ë¥¼ ë” ê°•ì¡°í•´ë³´ì„¸ìš”.")
        if star_analysis['action']['count'] < 2:
            suggestions.append("â€¢ ë³¸ì¸ì´ ì§ì ‘ ìˆ˜í–‰í•œ 'í–‰ë™'ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ë³´ì„¸ìš”.")
        if not keywords['tech_keywords']:
            suggestions.append("â€¢ ê¸°ìˆ ì ì¸ ìš©ì–´ì™€ ë„êµ¬ë¥¼ ë” í™œìš©í•´ë³´ì„¸ìš”.")
        
        if suggestions:
            for suggestion in suggestions:
                report.append(f"  {suggestion}")
        else:
            report.append("  âœ… ì „ë°˜ì ìœ¼ë¡œ ì¢‹ì€ ë‹µë³€ êµ¬ì¡°ë¥¼ ë³´ì—¬ì£¼ì…¨ìŠµë‹ˆë‹¤!")
        
        report.append("\n" + "="*60)
        report.append("ğŸ“‹ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
        report.append("="*60)
        
        return '\n'.join(report)


def main(): # í”„ë¡œê·¸ë¨ì˜ ë©”ì¸ ë¡œì§ì„ ë‹´ëŠ” í•¨ìˆ˜
    print("AI ë©´ì ‘ ì‹œìŠ¤í…œì„ ì‹œì‘í•©ë‹ˆë‹¤")

    # í™˜ê²½ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•´ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´ë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ RAG(ê²€ìƒ‰ ì¦ê°• ìƒì„±) ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”
    CONNECTION_STRING = os.getenv("POSTGRES_CONNECTION_STRING")
    
    if not CONNECTION_STRING:
        print("âš ï¸ ê²½ê³ : POSTGRES_CONNECTION_STRING í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   .env íŒŒì¼ì— ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    
    # ê°ì²´ ì´ˆê¸°í™”: ResumeRAG í´ë˜ìŠ¤ ë‚´ë¶€ì—ì„œ SQLAlchemyë¥¼ í†µí•´ PostgreSQL(PGVector)ì— ì ‘ì†
    # ì§€ì›ìì˜ ì´ë ¥ì„œ ë°ì´í„°ë¥¼ ì¡°íšŒí•  ì¤€ë¹„ë¥¼ ë§ˆì¹œë‹¤.
    try:
        rag = ResumeRAG(connection_string=CONNECTION_STRING)
    except Exception as e:
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
        print("   Docker ì»¨í…Œì´ë„ˆê°€ ì‹¤í–‰ ì¤‘ì¸ì§€, ì—°ê²° ì •ë³´ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    # ì´ë ¥ì„œ íŒŒì¼ í™•ì¸
    resume_path = os.path.join(current_dir, "resume.pdf")
    if os.path.exists(resume_path):
        print(f"'{resume_path}' íŒŒì¼ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        do_index = input("ì´ë ¥ì„œë¥¼ DBì— ìƒˆë¡œ ì¸ë±ì‹±í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n, default: n): ").strip().lower()
        if do_index == 'y':
            rag.clear_collection() # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ì¤‘ë³µ ë°©ì§€)
            # PDF íŒŒì¼ì„ ì½ì–´ì„œ í…ìŠ¤íŠ¸ë¡œ ìª¼ê°  ë’¤, ë²¡í„°(ìˆ«ì)ë¡œ ë³€í™˜í•˜ì—¬ DBì— ì €ì¥
            rag.load_and_index_pdf(resume_path)
    else:
        print(f"Warning: '{resume_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. RAG ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("CSH í´ë”ì— 'resume.pdf'ë¥¼ ë°°ì¹˜í•´ì£¼ì„¸ìš”.")

    # ì¸ë±ì‹±(Indexing)ë˜ì–´ DBì— ì €ì¥ëœ ë°©ëŒ€í•œ ë°ì´í„° ì¤‘, ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ ìˆëŠ” ë‚´ìš©ì„ ê³¨ë¼ë‚´ëŠ” 'ê²€ìƒ‰ê¸°'ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì½”ë“œ
    retriever = rag.get_retriever()
    
    # LLM ì´ˆê¸°í™” (Ollama ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©)
    try:
        llm = ChatOllama(model=DEFAULT_LLM_MODEL, temperature=DEFAULT_LLM_TEMPERATURE)
        print(f"âœ… LLM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {DEFAULT_LLM_MODEL}")
    except Exception as e:
        print(f"âŒ LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print("   Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”: 'ollama serve'")
        return

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: ë©´ì ‘ê´€ì˜ í˜ë¥´ì†Œë‚˜ ì„¤ì •
    system_prompt = """ë‹¹ì‹ ì€ IT ê¸°ì—…ì˜ 30ë…„ì°¨ ìˆ˜ì„ ê°œë°œì ë©´ì ‘ê´€ì…ë‹ˆë‹¤.
ì§€ì›ìì˜ ì´ë ¥ì„œ ë‚´ìš©ê³¼ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ ê¸°ìˆ  ìŠ¤íƒê³¼ ê²½í—˜ì— ëŒ€í•´ ì‹¬ë„ ìˆëŠ” ì§ˆë¬¸ì„ ë˜ì§€ì„¸ìš”.
ì œê³µëœ 'ì°¸ê³ ìš© ì´ë ¥ì„œ ë‚´ìš©'ì„ ì ê·¹ í™œìš©í•˜ì—¬ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•˜ì„¸ìš”.

[ì¤‘ìš” ê·œì¹™]
1. ë‹µë³€ì´ ë¶€ì‹¤í•˜ë©´ êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ ìš”êµ¬í•˜ê±°ë‚˜ ê¼¬ë¦¬ ì§ˆë¬¸ì„ í•˜ì„¸ìš”.
2. ê¼¬ë¦¬ ì§ˆë¬¸ì€ ì£¼ì œë‹¹ ìµœëŒ€ 2ë²ˆê¹Œì§€ë§Œ í—ˆìš©í•©ë‹ˆë‹¤. 
3. ë™ì¼í•œ ê¸°ìˆ ì  ì£¼ì œì— ëŒ€í•´ 2ë²ˆì˜ ë‹µë³€ì„ ë“¤ì—ˆë‹¤ë©´, "ì•Œê² ìŠµë‹ˆë‹¤. ë‹¤ìŒì€..."ì´ë¼ë©° ì£¼ì œë¥¼ ì „í™˜í•˜ì„¸ìš”.
4. ì§ˆë¬¸ì€ í•œ ë²ˆì— í•˜ë‚˜ë§Œ í•˜ì„¸ìš”.

ì§ˆë¬¸ì„ í•  ë•Œ ë„ˆë¬´ ê³µê²©ì ì´ì§€ ì•Šê²Œ, ì •ì¤‘í•˜ì§€ë§Œ ë‚ ì¹´ë¡œìš´ íƒœë„ë¥¼ ìœ ì§€í•˜ì„¸ìš”.
ë©´ì ‘ì€ ìê¸°ì†Œê°œë¡œ ì‹œì‘í•©ë‹ˆë‹¤."""

    # ëŒ€í™” ê¸°ë¡ ê´€ë¦¬
    chat_history = [
        SystemMessage(content=system_prompt)
    ]

    print(f"\n[{'='*30} AI ë©´ì ‘ ì‹œì‘ {'='*30}]")
    initial_greeting = "ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ ë©´ì ‘ì„ ì§„í–‰í•˜ê²Œ ëœ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ë¨¼ì € ê°„ë‹¨í•œ ìê¸°ì†Œê°œë¥¼ ë¶€íƒë“œë¦½ë‹ˆë‹¤."
    print(f"AI ë©´ì ‘ê´€: {initial_greeting}")
    chat_history.append(AIMessage(content=initial_greeting))

    while True: # ë¬´í•œ ë£¨í”„
        try:
            user_input = input("\nì§€ì›ì (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥): ")
            if user_input.lower().strip() in ["exit", "ì¢…ë£Œ", "quit"]:
                print("\nAI ë©´ì ‘ê´€: ë©´ì ‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤.")
                
                # ë©´ì ‘ ì¢…ë£Œ í›„ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
                generate_report = input("\nğŸ“Š ë©´ì ‘ ê²°ê³¼ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n, default: y): ").strip().lower()
                if generate_report != 'n':
                    report_generator = InterviewReportGenerator(llm)
                    report = report_generator.generate_report(chat_history)
                    print(report)
                    
                    # ë¦¬í¬íŠ¸ íŒŒì¼ë¡œ ì €ì¥ ì—¬ë¶€ í™•ì¸
                    save_report = input("\nğŸ’¾ ë¦¬í¬íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n, default: n): ").strip().lower()
                    if save_report == 'y':
                        try:
                            report_filename = f"interview_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
                            report_path = os.path.join(current_dir, report_filename)
                            with open(report_path, 'w', encoding='utf-8') as f:
                                f.write(report)
                            print(f"âœ… ë¦¬í¬íŠ¸ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {report_path}")
                        except IOError as e:
                            print(f"âŒ ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
                
                break
            
            if not user_input.strip():
                continue

            # ì‚¬ìš©ìì˜ ì§ˆë¬¸(user_input)ì„ ë°”íƒ•ìœ¼ë¡œ DBì—ì„œ ê´€ë ¨ ìˆëŠ” ë¬¸ì„œ ì¡°ê°ë“¤ì„ ì‹¤ì œë¡œ ê°€ì ¸ì˜¨ë‹¤
            # ì§ˆë¬¸ì„ ë²¡í„°(ìˆ«ì)ë¡œ ë°”ê¾¼ ë’¤, DBì— ì €ì¥ëœ ì´ë ¥ì„œ ì¡°ê°ë“¤ ì¤‘ ìˆ«ìê°€ ê°€ì¥ ë¹„ìŠ·í•œ ê²ƒë“¤ì„ ê³¨ë¼ë‚¸ë‹¤
            # ê²°ê³¼ê°’ì¸ retrieved_docsëŠ” ë¬¸ì„œ ê°ì²´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸(List) í˜•íƒœì´ë‹¤ (ì˜ˆ: [ë¬¸ì„œ1, ë¬¸ì„œ2, ë¬¸ì„œ3])
            retrieved_docs = retriever.invoke(user_input)
            # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ë¬¸ì„œë“¤ì„ AIê°€ ì½ê¸° í¸í•˜ë„ë¡ í•˜ë‚˜ì˜ ê¸´ í…ìŠ¤íŠ¸ë¡œ í•©ì¹˜ëŠ” ê³¼ì •
            context_text = "\n".join([doc.page_content for doc in retrieved_docs])
            
            # ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆë‹¤ë©´ í”„ë¡¬í”„íŠ¸ì— ì£¼ì…
            # context_messageë¼ëŠ” ë³€ìˆ˜ë¥¼ ìƒì„±í•˜ê³  ì´ˆê¸°ê°’ì„ Noneìœ¼ë¡œ ì„¤ì •. ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë³€ìˆ˜ë¥¼ ë¯¸ë¦¬ ì´ˆê¸°í™”í•´ë‘ëŠ” ê³¼ì •.
            context_message = None
            # context_text: ë²¡í„° DB ë“±ì—ì„œ ê²€ìƒ‰í•´ì˜¨ í…ìŠ¤íŠ¸ ë°ì´í„°
            if context_text:
                context_message = SystemMessage(content=f"--- [RAG System] ì°¸ê³ ìš© ì´ë ¥ì„œ ê´€ë ¨ ë‚´ìš© ---\n{context_text}\n------------------------------------------")

            # ì‚¬ìš©ìì˜ ì§ˆë¬¸(user_input)ê³¼ ì´ì „ ëŒ€í™” ê¸°ë¡(chat_history)ì„ í•©ì³ì„œ AI ëª¨ë¸ì—ê²Œ ì „ë‹¬í•  ìµœì¢… ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“œëŠ” ê³¼ì •
            messages_for_inference = list(chat_history)
            messages_for_inference.append(HumanMessage(content=user_input))
            
            # AI ëª¨ë¸ì€ [ì´ì „ ëŒ€í™” ë‚´ì—­ + í˜„ì¬ ì§ˆë¬¸]ì— ë”í•´ [ì°¸ê³ í•´ì•¼ í•  ì´ë ¥ì„œ ë°ì´í„°]ê¹Œì§€ í•œêº¼ë²ˆì— ì „ë‹¬ ë°›ê²Œ ëœë‹¤
            if context_message:
                messages_for_inference.append(context_message)

            # LLM ì‘ë‹µ ìƒì„±
            print("\n(AIê°€ ìƒê° ì¤‘ì…ë‹ˆë‹¤... ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...)")
            response = llm.invoke(messages_for_inference)
            
            # AIê°€ ìƒì„±í•œ ë‹µë³€ ì¤‘ í…ìŠ¤íŠ¸ ë‚´ìš©(content)ë§Œ ì¶”ì¶œí•˜ì—¬ í™”ë©´ì— ì¶œë ¥
            print(f"\nAI ë©´ì ‘ê´€: {response.content}")

            # ì‹¤ì œ ëŒ€í™” ê¸°ë¡ì—ëŠ” User Inputê³¼ AI Responseë§Œ ì €ì¥ (ContextëŠ” ì¤‘ë³µ ì €ì¥ ì•ˆ í•¨)
            # ë°©ê¸ˆ ë‚˜ëˆˆ ëŒ€í™”ë¥¼ ë©”ëª¨ë¦¬(ëŒ€í™” ê¸°ë¡)ì— ì €ì¥í•˜ì—¬, ë‹¤ìŒ ì§ˆë¬¸ì„ í–ˆì„ ë•Œ AIê°€ ì•ì„  ë‚´ìš©ì„ ê¸°ì–µí•  ìˆ˜ ìˆê²Œ ë§Œë“œëŠ” ê³¼ì •
            chat_history.append(HumanMessage(content=user_input))
            chat_history.append(response)

        except KeyboardInterrupt:
            print("\n\në©´ì ‘ì´ ê°•ì œë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            # ê°•ì œ ì¢…ë£Œ ì‹œì—ë„ ë¦¬í¬íŠ¸ ìƒì„± ì˜µì…˜ ì œê³µ
            try:
                generate_report = input("\nğŸ“Š ë©´ì ‘ ê²°ê³¼ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n, default: n): ").strip().lower()
                if generate_report == 'y':
                    report_generator = InterviewReportGenerator(llm)
                    report = report_generator.generate_report(chat_history)
                    print(report)
            except (EOFError, KeyboardInterrupt):
                print("\në¦¬í¬íŠ¸ ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\nì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            import traceback
            traceback.print_exc()  # ë””ë²„ê¹…ì„ ìœ„í•œ ìƒì„¸ ì—ëŸ¬ ì¶œë ¥
            break
    
    print("\në©´ì ‘ ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤! ğŸ‘‹")

if __name__ == "__main__":
    main()
