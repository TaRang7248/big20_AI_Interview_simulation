"use client";
import { useState, useEffect, useRef, useCallback } from "react";
import { useRouter } from "next/navigation";
import { useAuth } from "@/contexts/AuthContext";
import Header from "@/components/common/Header";
import EventToastContainer from "@/components/common/EventToast";
import InterviewReportCharts, { ReportData } from "@/components/report/InterviewReportCharts";
import { sessionApi, interviewApi, ttsApi, interventionApi } from "@/lib/api";
import { Mic, MicOff, Camera, CameraOff, PhoneOff, SkipForward, Volume2, Loader2, FileText, Download, LayoutDashboard } from "lucide-react";

/* Web Speech API íƒ€ì… (ë¸Œë¼ìš°ì € ì „ìš©) */
type SpeechRecognitionType = typeof window extends { SpeechRecognition: infer T } ? T : unknown;
declare global {
  interface Window {
    SpeechRecognition: new () => SpeechRecognition;
    webkitSpeechRecognition: new () => SpeechRecognition;
  }
  interface SpeechRecognition extends EventTarget {
    lang: string; continuous: boolean; interimResults: boolean;
    start(): void; stop(): void; abort(): void;
    onresult: ((ev: SpeechRecognitionEvent) => void) | null;
    onerror: ((ev: Event) => void) | null;
    onend: (() => void) | null;
  }
  interface SpeechRecognitionEvent extends Event {
    readonly resultIndex: number;
    readonly results: SpeechRecognitionResultList;
  }
  interface SpeechRecognitionResultList { readonly length: number; item(index: number): SpeechRecognitionResult; [index: number]: SpeechRecognitionResult; }
  interface SpeechRecognitionResult { readonly length: number; readonly isFinal: boolean; item(index: number): SpeechRecognitionAlternative; [index: number]: SpeechRecognitionAlternative; }
  interface SpeechRecognitionAlternative { readonly transcript: string; readonly confidence: number; }
}

type Phase = "setup" | "interview" | "coding" | "whiteboard" | "report";
type Status = "ready" | "listening" | "speaking" | "processing";

export default function InterviewPage() {
  const { user, token } = useAuth();
  const router = useRouter();

  // ìƒíƒœ
  const [phase, setPhase] = useState<Phase>("setup");
  const [status, setStatus] = useState<Status>("ready");
  const [sessionId, setSessionId] = useState("");
  const [messages, setMessages] = useState<{ role: "ai" | "user"; text: string }[]>([]);
  const [currentQuestion, setCurrentQuestion] = useState("");
  const [questionNum, setQuestionNum] = useState(0);
  const totalQuestions = 9;
  const [sttText, setSttText] = useState("");
  const [micEnabled, setMicEnabled] = useState(true);
  const [camEnabled, setCamEnabled] = useState(true);
  const [interviewStarted, setInterviewStarted] = useState(false);
  const [reportData, setReportData] = useState<ReportData | null>(null);
  const [reportLoading, setReportLoading] = useState(false);

  // Refs
  const videoRef = useRef<HTMLVideoElement>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const wsRef = useRef<WebSocket | null>(null);
  const recognitionRef = useRef<SpeechRecognition | null>(null);
  const chatEndRef = useRef<HTMLDivElement>(null);
  const interventionTimerRef = useRef<ReturnType<typeof setInterval> | null>(null);
  const pushEventRef = useRef<((raw: Record<string, unknown>) => void) | null>(null);

  // ì¸ì¦ í™•ì¸
  useEffect(() => {
    if (!token) router.push("/");
  }, [token, router]);

  // ë¦¬í¬íŠ¸ ë°ì´í„° ë¡œë“œ
  useEffect(() => {
    if (phase !== "report" || !sessionId) return;
    setReportLoading(true);
    interviewApi
      .getReport(sessionId)
      .then((data) => setReportData(data as ReportData))
      .catch((err) => console.error("ë¦¬í¬íŠ¸ ë¡œë“œ ì‹¤íŒ¨:", err))
      .finally(() => setReportLoading(false));
  }, [phase, sessionId]);

  // ì±„íŒ… ìë™ ìŠ¤í¬ë¡¤
  useEffect(() => { chatEndRef.current?.scrollIntoView({ behavior: "smooth" }); }, [messages]);

  // í´ë¦°ì—…
  useEffect(() => {
    return () => {
      streamRef.current?.getTracks().forEach(t => t.stop());
      wsRef.current?.close();
      recognitionRef.current?.stop();
      if (interventionTimerRef.current) clearInterval(interventionTimerRef.current);
    };
  }, []);

  // ========== ë©´ì ‘ ì‹œì‘ ==========
  const startInterview = async () => {
    if (!user) return;
    try {
      // ì¹´ë©”ë¼ ì´ˆê¸°í™”
      const stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: true });
      streamRef.current = stream;
      if (videoRef.current) videoRef.current.srcObject = stream;

      // ì„¸ì…˜ ìƒì„±
      const res = await sessionApi.create({ user_email: user.email, interview_type: "technical" });
      setSessionId(res.session_id);

      // WebSocket ì—°ê²°
      const protocol = window.location.protocol === "https:" ? "wss:" : "ws:";
      const wsToken = sessionStorage.getItem("access_token");
      const ws = new WebSocket(`${protocol}//${window.location.host}/ws/interview/${res.session_id}?token=${encodeURIComponent(wsToken || "")}`);
      ws.onmessage = (e) => {
        try {
          const data = JSON.parse(e.data);
          if (data.type === "stt_result" && data.is_final) {
            setSttText(prev => prev + " " + data.transcript);
          }
          // EventBus ì´ë²¤íŠ¸ â†’ ì‹¤ì‹œê°„ í† ìŠ¤íŠ¸ ì•Œë¦¼
          if (data.type === "event" && pushEventRef.current) {
            pushEventRef.current(data);
          }
        } catch { /* ignore */ }
      };
      wsRef.current = ws;

      // ìŒì„±ì¸ì‹ ì´ˆê¸°í™” (Web Speech API í´ë°±)
      initSpeechRecognition();

      setPhase("interview");
      setInterviewStarted(true);

      // ì²« ì§ˆë¬¸ ìš”ì²­
      await getNextQuestion(res.session_id, "[START]");
    } catch (err) {
      console.error("ë©´ì ‘ ì‹œì‘ ì‹¤íŒ¨:", err);
      alert("ë©´ì ‘ ì‹œì‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì¹´ë©”ë¼/ë§ˆì´í¬ ê¶Œí•œì„ í™•ì¸í•´ì£¼ì„¸ìš”.");
    }
  };

  // ========== ìŒì„± ì¸ì‹ (Web Speech API) ==========
  const initSpeechRecognition = () => {
    const SR = window.webkitSpeechRecognition || window.SpeechRecognition;
    if (!SR) return;
    const recognition = new SR();
    recognition.lang = "ko-KR";
    recognition.continuous = true;
    recognition.interimResults = true;

    recognition.onresult = (e: SpeechRecognitionEvent) => {
      let final = "";
      for (let i = e.resultIndex; i < e.results.length; i++) {
        if (e.results[i].isFinal) final += e.results[i][0].transcript;
      }
      if (final) setSttText(prev => prev + " " + final);
    };

    recognition.onend = () => { if (interviewStarted && micEnabled) recognition.start(); };
    recognitionRef.current = recognition;
    recognition.start();
  };

  // ========== ì§ˆë¬¸ ìš”ì²­ ==========
  const getNextQuestion = async (sid: string, message: string) => {
    setStatus("processing");
    try {
      const res = await interviewApi.chat({ session_id: sid, message, mode: "interview" });
      const q = res.response;
      setCurrentQuestion(q);
      setQuestionNum(res.question_number || questionNum + 1);
      setMessages(prev => [...prev, { role: "ai", text: q }]);
      await speakQuestion(q);
      setStatus("listening");

      // ê°œì… ì²´í¬ ì‹œì‘
      startInterventionCheck(sid);
    } catch { setStatus("ready"); }
  };

  // ========== TTS ë°œí™” ==========
  const speakQuestion = async (text: string) => {
    setStatus("speaking");
    try {
      const blob = await ttsApi.speak(text, "professional");
      const url = URL.createObjectURL(blob);
      const audio = new Audio(url);
      await new Promise<void>((resolve) => {
        audio.onended = () => resolve();
        audio.onerror = () => resolve();
        audio.play().catch(() => resolve());
      });
      URL.revokeObjectURL(url);
    } catch {
      // TTS ì‹¤íŒ¨ ì‹œ Web Speech API í´ë°±
      try {
        const utterance = new SpeechSynthesisUtterance(text);
        utterance.lang = "ko-KR";
        speechSynthesis.speak(utterance);
      } catch { /* ignore */ }
    }
  };

  // ========== ê°œì… ì²´í¬ ==========
  const startInterventionCheck = (sid: string) => {
    if (interventionTimerRef.current) clearInterval(interventionTimerRef.current);
    interventionApi.startTurn(sid).catch(() => {});
    interventionTimerRef.current = setInterval(async () => {
      try {
        const res = await interventionApi.check(sid, sttText);
        if (res.should_intervene && res.message) {
          setMessages(prev => [...prev, { role: "ai", text: `ğŸ’¡ ${res.message}` }]);
          await speakQuestion(res.message);
        }
      } catch { /* ignore */ }
    }, 3000);
  };

  // ========== ë‹µë³€ ì œì¶œ ==========
  const submitAnswer = async () => {
    if (!sttText.trim()) return;
    const answer = sttText.trim();
    setSttText("");
    setMessages(prev => [...prev, { role: "user", text: answer }]);

    // ê°œì… íƒ€ì´ë¨¸ ì •ì§€
    if (interventionTimerRef.current) clearInterval(interventionTimerRef.current);
    interventionApi.endTurn(sessionId, answer).catch(() => {});

    // í‰ê°€
    setStatus("processing");
    try {
      await interviewApi.evaluate({
        session_id: sessionId,
        question: currentQuestion,
        answer,
        question_number: questionNum,
      });
    } catch { /* ignore */ }

    // ë‹¤ìŒ ì§ˆë¬¸ or ì¢…ë£Œ
    if (questionNum >= totalQuestions) {
      endInterview();
    } else {
      await getNextQuestion(sessionId, answer);
    }
  };

  // ========== ë©´ì ‘ ì¢…ë£Œ ==========
  const endInterview = async () => {
    setInterviewStarted(false);
    recognitionRef.current?.stop();
    if (interventionTimerRef.current) clearInterval(interventionTimerRef.current);
    setPhase("coding");
  };

  // ========== ë§ˆì´í¬/ì¹´ë©”ë¼ í† ê¸€ ==========
  const toggleMic = () => {
    const track = streamRef.current?.getAudioTracks()[0];
    if (track) { track.enabled = !track.enabled; setMicEnabled(track.enabled); }
  };
  const toggleCam = () => {
    const track = streamRef.current?.getVideoTracks()[0];
    if (track) { track.enabled = !track.enabled; setCamEnabled(track.enabled); }
  };

  if (!user) return null;

  // ========== ë Œë”ë§ ==========
  return (
    <div className="min-h-screen flex flex-col">
      <Header />
      {/* ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ì•Œë¦¼ (EventBus â†’ WebSocket) */}
      <EventToastContainer onPushEvent={(handler) => { pushEventRef.current = handler; }} />

      {/* ë©´ì ‘ ì¤€ë¹„ í™”ë©´ */}
      {phase === "setup" && (
        <main className="flex-1 flex items-center justify-center p-6">
          <div className="glass-card max-w-lg w-full text-center">
            <h1 className="text-3xl font-bold gradient-text mb-4">AI ëª¨ì˜ë©´ì ‘</h1>
            <p className="text-[var(--text-secondary)] mb-8">
              ì¹´ë©”ë¼ì™€ ë§ˆì´í¬ê°€ ì¤€ë¹„ë˜ì—ˆëŠ”ì§€ í™•ì¸í•œ í›„<br />ë©´ì ‘ì„ ì‹œì‘í•´ì£¼ì„¸ìš”.
            </p>
            <div className="rounded-xl overflow-hidden bg-black aspect-video mb-6">
              <video ref={videoRef} autoPlay muted playsInline className="w-full h-full object-cover" />
            </div>
            <button onClick={startInterview} className="btn-gradient text-lg px-12 py-4 rounded-2xl">
              ğŸ¤ ë©´ì ‘ ì‹œì‘
            </button>
          </div>
        </main>
      )}

      {/* ë©´ì ‘ ì§„í–‰ í™”ë©´ */}
      {phase === "interview" && (
        <main className="flex-1 flex flex-col p-4 max-w-[1400px] mx-auto w-full">
          {/* ìƒíƒœ ë°” */}
          <div className="flex items-center justify-between mb-4">
            <div className="flex items-center gap-3">
              <span className={`px-4 py-1.5 rounded-full text-sm font-semibold ${
                status === "ready" ? "bg-[rgba(0,255,136,0.2)] text-[var(--green)]" :
                status === "listening" ? "bg-[rgba(255,193,7,0.2)] text-[var(--warning)]" :
                status === "speaking" ? "bg-[rgba(0,217,255,0.2)] text-[var(--cyan)]" :
                "bg-[rgba(156,39,176,0.2)] text-purple-300"
              }`}>
                {status === "ready" && "ëŒ€ê¸°"}
                {status === "listening" && "ğŸ¤ ë“£ëŠ” ì¤‘..."}
                {status === "speaking" && "ğŸ”Š ë°œí™” ì¤‘..."}
                {status === "processing" && "â³ ì²˜ë¦¬ ì¤‘..."}
              </span>
              <span className="text-sm text-[var(--text-secondary)]">ì§ˆë¬¸ {questionNum}/{totalQuestions}</span>
            </div>
            <button onClick={endInterview} className="px-4 py-2 text-sm rounded-lg bg-[rgba(244,67,54,0.2)] text-[var(--danger)] border border-[rgba(244,67,54,0.3)] hover:bg-[rgba(244,67,54,0.3)] transition">
              ë©´ì ‘ ì¢…ë£Œ
            </button>
          </div>

          {/* ì§„í–‰ ë°” */}
          <div className="flex gap-1 mb-6">
            {Array.from({ length: totalQuestions }, (_, i) => (
              <div key={i} className={`h-1.5 flex-1 rounded-full transition-all ${
                i < questionNum ? "bg-gradient-to-r from-[var(--cyan)] to-[var(--green)]" :
                i === questionNum ? "bg-[var(--cyan)] animate-pulse" : "bg-[rgba(255,255,255,0.1)]"
              }`} />
            ))}
          </div>

          {/* 2ì—´ ë ˆì´ì•„ì›ƒ */}
          <div className="grid grid-cols-1 lg:grid-cols-2 gap-4 flex-1">
            {/* AI ë©´ì ‘ê´€ */}
            <div className="glass-card flex flex-col">
              <h3 className="text-sm font-semibold mb-3 flex items-center gap-2">
                <Volume2 size={16} className="text-[var(--cyan)]" /> AI ë©´ì ‘ê´€
              </h3>
              <div className="flex-1 rounded-xl bg-gradient-to-br from-[#1e3a5f] to-[#0d2137] flex items-center justify-center min-h-[200px] relative">
                <div className={`w-48 h-48 rounded-full border-4 ${
                  status === "speaking" ? "border-[var(--green)] shadow-[0_0_30px_rgba(0,255,136,0.5)]" : "border-[var(--cyan)]"
                } bg-gradient-to-br from-[#2a4a6b] to-[#1a3050] flex items-center justify-center text-6xl transition-all`}>
                  ğŸ¤–
                </div>
                <span className="absolute bottom-3 left-3 text-xs bg-black/60 px-2 py-1 rounded">AI ë©´ì ‘ê´€</span>
              </div>
            </div>

            {/* ì±„íŒ…/ë¹„ë””ì˜¤ */}
            <div className="glass-card flex flex-col">
              {/* ì‚¬ìš©ì ë¹„ë””ì˜¤ (ì‘ê²Œ) */}
              <div className="rounded-xl overflow-hidden bg-black h-32 mb-3">
                <video ref={videoRef} autoPlay muted playsInline className="w-full h-full object-cover" />
              </div>

              {/* ì±„íŒ… ë¡œê·¸ */}
              <div className="flex-1 overflow-y-auto space-y-3 mb-3 min-h-[200px] max-h-[400px] pr-2">
                {messages.map((m, i) => (
                  <div key={i} className={`flex ${m.role === "user" ? "justify-end" : "justify-start"}`}>
                    <div className={`max-w-[85%] px-4 py-3 rounded-2xl text-sm leading-relaxed ${
                      m.role === "user"
                        ? "bg-gradient-to-r from-[rgba(0,217,255,0.15)] to-[rgba(0,255,136,0.1)] rounded-br-md"
                        : "bg-[rgba(255,255,255,0.06)] rounded-bl-md"
                    }`}>
                      {m.text}
                    </div>
                  </div>
                ))}
                <div ref={chatEndRef} />
              </div>

              {/* STT ì¸ì‹ í…ìŠ¤íŠ¸ */}
              {status === "listening" && (
                <div className="bg-[rgba(255,193,7,0.08)] border border-[rgba(255,193,7,0.2)] rounded-xl p-3 mb-3">
                  <p className="text-xs text-[var(--warning)] mb-1">ğŸ¤ ìŒì„± ì¸ì‹ ì¤‘...</p>
                  <p className="text-sm">{sttText || "ë§ì”€í•´ì£¼ì„¸ìš”..."}</p>
                </div>
              )}

              {/* ì»¨íŠ¸ë¡¤ */}
              <div className="flex items-center justify-center gap-4">
                <button onClick={toggleMic} className={`w-12 h-12 rounded-full flex items-center justify-center transition ${
                  micEnabled ? "bg-[rgba(0,255,136,0.2)] text-[var(--green)]" : "bg-[rgba(255,82,82,0.2)] text-[var(--danger)]"
                }`}>
                  {micEnabled ? <Mic size={20} /> : <MicOff size={20} />}
                </button>
                <button onClick={toggleCam} className={`w-12 h-12 rounded-full flex items-center justify-center transition ${
                  camEnabled ? "bg-[rgba(0,255,136,0.2)] text-[var(--green)]" : "bg-[rgba(255,82,82,0.2)] text-[var(--danger)]"
                }`}>
                  {camEnabled ? <Camera size={20} /> : <CameraOff size={20} />}
                </button>
                <button onClick={submitAnswer} disabled={!sttText.trim() || status !== "listening"}
                  className="btn-gradient !rounded-full w-12 h-12 flex items-center justify-center disabled:opacity-40">
                  <SkipForward size={20} />
                </button>
                <button onClick={endInterview} className="w-12 h-12 rounded-full bg-[rgba(244,67,54,0.8)] text-white flex items-center justify-center hover:bg-[rgba(244,67,54,1)] transition">
                  <PhoneOff size={20} />
                </button>
              </div>
            </div>
          </div>
        </main>
      )}

      {/* ì½”ë”© í…ŒìŠ¤íŠ¸ Phase */}
      {phase === "coding" && (
        <main className="flex-1 flex items-center justify-center p-6">
          <div className="glass-card max-w-lg text-center">
            <h2 className="text-2xl font-bold gradient-text mb-4">ğŸ’» ì½”ë”© í…ŒìŠ¤íŠ¸</h2>
            <p className="text-[var(--text-secondary)] mb-6">
              í™”ìƒ ë©´ì ‘ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì½”ë”© í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ?
            </p>
            <div className="flex gap-4 justify-center">
              <button onClick={() => router.push(`/coding?session=${sessionId}`)} className="btn-gradient px-8 py-3">
                ì½”ë”© í…ŒìŠ¤íŠ¸ ì‹œì‘
              </button>
              <button onClick={() => setPhase("whiteboard")} className="px-8 py-3 rounded-xl border border-[rgba(0,217,255,0.4)] text-[var(--cyan)] hover:bg-[rgba(0,217,255,0.1)] transition">
                ê±´ë„ˆë›°ê¸°
              </button>
            </div>
          </div>
        </main>
      )}

      {/* í™”ì´íŠ¸ë³´ë“œ Phase */}
      {phase === "whiteboard" && (
        <main className="flex-1 flex items-center justify-center p-6">
          <div className="glass-card max-w-lg text-center">
            <h2 className="text-2xl font-bold gradient-text mb-4">ğŸ¨ ì•„í‚¤í…ì²˜ ì„¤ê³„</h2>
            <p className="text-[var(--text-secondary)] mb-6">
              í™”ì´íŠ¸ë³´ë“œì— ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ë¥¼ ì„¤ê³„í•´ë³´ì„¸ìš”.
            </p>
            <div className="flex gap-4 justify-center">
              <button onClick={() => router.push(`/whiteboard?session=${sessionId}`)} className="btn-gradient px-8 py-3">
                ì„¤ê³„ ì‹œì‘
              </button>
              <button onClick={() => setPhase("report")} className="px-8 py-3 rounded-xl border border-[rgba(0,217,255,0.4)] text-[var(--cyan)] hover:bg-[rgba(0,217,255,0.1)] transition">
                ê²°ê³¼ ë³´ê¸°
              </button>
            </div>
          </div>
        </main>
      )}

      {/* ë¦¬í¬íŠ¸ Phase */}
      {phase === "report" && (
        <main className="flex-1 overflow-y-auto p-6">
          <div className="max-w-5xl mx-auto space-y-6">
            {/* ë¡œë”© ìƒíƒœ */}
            {reportLoading && (
              <div className="flex flex-col items-center justify-center py-20">
                <Loader2 className="w-10 h-10 text-[var(--cyan)] animate-spin mb-4" />
                <p className="text-[var(--text-secondary)]">ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤â€¦</p>
              </div>
            )}

            {/* ì°¨íŠ¸ ë¦¬í¬íŠ¸ */}
            {!reportLoading && reportData && (
              <InterviewReportCharts report={reportData} />
            )}

            {/* ë°ì´í„° ì—†ì„ ë•Œ */}
            {!reportLoading && !reportData && (
              <div className="glass-card text-center py-12">
                <h2 className="text-2xl font-bold gradient-text mb-4">ğŸ“Š ë©´ì ‘ ì™„ë£Œ!</h2>
                <p className="text-[var(--text-secondary)]">ë¦¬í¬íŠ¸ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.</p>
              </div>
            )}

            {/* í•˜ë‹¨ ì•¡ì…˜ ë²„íŠ¼ */}
            <div className="flex gap-4 justify-center flex-wrap pb-8">
              <button
                onClick={() => window.open(`/api/report/${sessionId}`, "_blank")}
                className="flex items-center gap-2 px-6 py-3 rounded-xl bg-[rgba(0,217,255,0.15)] border border-[rgba(0,217,255,0.4)] text-[var(--cyan)] hover:bg-[rgba(0,217,255,0.25)] transition"
              >
                <FileText className="w-4 h-4" /> JSON ì›ë³¸
              </button>
              <button
                onClick={() => {
                  const tk = localStorage.getItem("token");
                  fetch(`/api/report/${sessionId}/pdf`, {
                    headers: { Authorization: `Bearer ${tk}` },
                  })
                    .then((res) => {
                      if (!res.ok) throw new Error("PDF ìƒì„± ì‹¤íŒ¨");
                      return res.blob();
                    })
                    .then((blob) => {
                      const url = URL.createObjectURL(blob);
                      const a = document.createElement("a");
                      a.href = url;
                      a.download = `interview_report_${sessionId?.slice(0, 8)}.pdf`;
                      a.click();
                      URL.revokeObjectURL(url);
                    })
                    .catch((err) => alert(err.message));
                }}
                className="flex items-center gap-2 btn-gradient px-6 py-3"
              >
                <Download className="w-4 h-4" /> PDF ë‹¤ìš´ë¡œë“œ
              </button>
              <button
                onClick={() => router.push("/dashboard")}
                className="flex items-center gap-2 px-6 py-3 rounded-xl border border-[rgba(0,217,255,0.4)] text-[var(--cyan)] hover:bg-[rgba(0,217,255,0.1)] transition"
              >
                <LayoutDashboard className="w-4 h-4" /> ëŒ€ì‹œë³´ë“œë¡œ
              </button>
            </div>
          </div>
        </main>
      )}
    </div>
  );
}
