"""
Whisper ì˜¤í”„ë¼ì¸ STT í´ë°± ì„œë¹„ìŠ¤
=================================
Deepgram(í´ë¼ìš°ë“œ) ì—°ê²° ë¶ˆê°€ ì‹œ ë¡œì»¬ Whisper ëª¨ë¸ë¡œ ìŒì„±â†’í…ìŠ¤íŠ¸ ë³€í™˜.

ì£¼ìš” ê¸°ëŠ¥:
- faster-whisper ê¸°ë°˜ ê³ ì† ë¡œì»¬ ì¶”ë¡  (CPU/GPU)
- í•œêµ­ì–´ ìµœì í™” (language="ko")
- word-level íƒ€ì´ë°/confidence ì§€ì› (SpeechAnalysisService ì—°ë™)
- ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ë²„í¼ë§ + ì£¼ê¸°ì  ë³€í™˜ (VAD ê¸°ë°˜)
- Deepgram API ì¥ì•  ì‹œ ìë™ í´ë°±
- pykospacing ë„ì–´ì“°ê¸° ë³´ì • ì—°ë™

ì‚¬ìš©:
    service = WhisperSTTService()
    service.start_session("session_id")
    service.feed_audio(session_id, pcm_bytes)
    results = service.flush(session_id)  # ì¦‰ì‹œ ë³€í™˜
"""

import os
import io
import time
import wave
import struct
import asyncio
import threading
from typing import Optional, Dict, List, Any, Callable
from dataclasses import dataclass, field
from collections import deque
from concurrent.futures import ThreadPoolExecutor

# ========== faster-whisper ë¡œë“œ ==========
_WHISPER_AVAILABLE = False
_WhisperModel = None

try:
    from faster_whisper import WhisperModel as _FasterWhisperModel
    _WhisperModel = _FasterWhisperModel
    _WHISPER_AVAILABLE = True
except ImportError:
    pass

# openai-whisper í´ë°±
if not _WHISPER_AVAILABLE:
    try:
        import whisper as _openai_whisper
        _WHISPER_AVAILABLE = True
    except ImportError:
        _openai_whisper = None


def is_whisper_available() -> bool:
    """Whisper STT ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
    return _WHISPER_AVAILABLE


# ========== ë°ì´í„° ëª¨ë¸ ==========

@dataclass
class WhisperSegment:
    """Whisper ë³€í™˜ ê²°ê³¼ ì„¸ê·¸ë¨¼íŠ¸"""
    text: str
    start: float  # ì´ˆ
    end: float    # ì´ˆ
    confidence: float  # í‰ê·  í™•ë¥ 
    words: Optional[List[Dict]] = None  # word-level: [{"word", "start", "end", "confidence"}]


@dataclass
class WhisperResult:
    """Whisper ë³€í™˜ ì „ì²´ ê²°ê³¼"""
    transcript: str
    segments: List[WhisperSegment]
    language: str = "ko"
    duration: float = 0.0
    is_final: bool = True
    words: Optional[List[Dict]] = None  # ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ì˜ word í†µí•©


@dataclass
class _SessionBuffer:
    """ì„¸ì…˜ë³„ ì˜¤ë””ì˜¤ ë²„í¼"""
    audio_chunks: deque = field(default_factory=deque)
    total_bytes: int = 0
    last_feed_time: float = 0.0
    is_active: bool = True
    sample_rate: int = 16000
    channels: int = 1
    sample_width: int = 2  # 16bit = 2 bytes
    # VAD: ìŒì„± í™œë™ ê°ì§€ìš© ì—ë„ˆì§€ ì¶”ì 
    silence_start: Optional[float] = None
    last_transcript: str = ""


class WhisperSTTService:
    """
    Whisper ê¸°ë°˜ ì˜¤í”„ë¼ì¸ STT ì„œë¹„ìŠ¤

    Deepgram í´ë¼ìš°ë“œ STT ë¶ˆê°€ ì‹œ ë¡œì»¬ Whisper ëª¨ë¸ë¡œ í´ë°±.
    - faster-whisper (ìš°ì„ ) ë˜ëŠ” openai-whisper ì‚¬ìš©
    - ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ ë²„í¼ë§í•˜ê³  ì¼ì • ë‹¨ìœ„(VAD ê¸°ë°˜)ë¡œ ë³€í™˜
    """

    # ëª¨ë¸ í¬ê¸°: tiny < base < small < medium < large
    DEFAULT_MODEL_SIZE = "base"
    # ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ ëª¨ì•„ì„œ ë³€í™˜í•˜ëŠ” ìµœì†Œ ë‹¨ìœ„ (ì´ˆ)
    MIN_AUDIO_DURATION = 1.5
    # ì¹¨ë¬µ ê°ì§€ í›„ ìë™ flush ì‹œê°„ (ì´ˆ)
    SILENCE_FLUSH_SECONDS = 1.0
    # ì—ë„ˆì§€ ê¸°ë°˜ ì¹¨ë¬µ ì„ê³„ê°’ (RMS)
    SILENCE_RMS_THRESHOLD = 300
    # ìµœëŒ€ ë²„í¼ í¬ê¸° (ì´ˆ) â€” ë©”ëª¨ë¦¬ ë³´í˜¸
    MAX_BUFFER_SECONDS = 30
    # ë³€í™˜ ìŠ¤ë ˆë“œí’€ í¬ê¸°
    WORKER_THREADS = 2

    def __init__(
        self,
        model_size: str = None,
        device: str = "auto",
        compute_type: str = "auto",
        language: str = "ko",
    ):
        """
        Args:
            model_size: Whisper ëª¨ë¸ í¬ê¸° (tiny/base/small/medium/large-v3)
            device: "cpu", "cuda", "auto"
            compute_type: "int8", "float16", "float32", "auto"
            language: ì–¸ì–´ ì½”ë“œ
        """
        self.model_size = model_size or os.getenv("WHISPER_MODEL_SIZE", self.DEFAULT_MODEL_SIZE)
        self.device = device
        self.compute_type = compute_type
        self.language = language

        self._model = None
        self._model_lock = threading.Lock()
        self._model_loaded = False
        self._use_faster_whisper = _WhisperModel is not None

        # ì„¸ì…˜ë³„ ë²„í¼
        self._sessions: Dict[str, _SessionBuffer] = {}
        self._sessions_lock = threading.Lock()

        # ë³€í™˜ ìŠ¤ë ˆë“œí’€
        self._executor = ThreadPoolExecutor(
            max_workers=self.WORKER_THREADS,
            thread_name_prefix="whisper-stt"
        )

        # ë„ì–´ì“°ê¸° ë³´ì •ê¸°
        self._spacing_corrector = None
        try:
            from stt_engine import KoreanSpacingCorrector
            self._spacing_corrector = KoreanSpacingCorrector()
            if not self._spacing_corrector.is_available:
                self._spacing_corrector = None
        except ImportError:
            pass

        # ì½œë°±: ê²°ê³¼ë¥¼ ì™¸ë¶€ë¡œ ì „ë‹¬ (session_id, WhisperResult)
        self.on_result: Optional[Callable] = None

        print(f"ğŸ”§ [WhisperSTT] ì´ˆê¸°í™”: model={self.model_size}, "
              f"engine={'faster-whisper' if self._use_faster_whisper else 'openai-whisper'}, "
              f"device={self.device}")

    # â”€â”€â”€â”€â”€â”€â”€â”€ ëª¨ë¸ ê´€ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€

    def _ensure_model(self):
        """Lazy loading: ì²« ë³€í™˜ ì‹œ ëª¨ë¸ ë¡œë“œ"""
        if self._model_loaded:
            return

        with self._model_lock:
            if self._model_loaded:
                return

            print(f"â³ [WhisperSTT] ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_size} ...")
            start = time.time()

            if self._use_faster_whisper:
                # faster-whisper: CTranslate2 ê¸°ë°˜ ê³ ì† ì¶”ë¡ 
                device = self.device
                compute = self.compute_type

                if device == "auto":
                    try:
                        import torch
                        device = "cuda" if torch.cuda.is_available() else "cpu"
                    except ImportError:
                        device = "cpu"

                if compute == "auto":
                    compute = "float16" if device == "cuda" else "int8"

                self._model = _WhisperModel(
                    self.model_size,
                    device=device,
                    compute_type=compute,
                )
            else:
                # openai-whisper í´ë°±
                self._model = _openai_whisper.load_model(self.model_size)

            elapsed = time.time() - start
            self._model_loaded = True
            print(f"âœ… [WhisperSTT] ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({elapsed:.1f}ì´ˆ)")

    # â”€â”€â”€â”€â”€â”€â”€â”€ ì„¸ì…˜ ê´€ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€

    def start_session(self, session_id: str, sample_rate: int = 16000):
        """ì„¸ì…˜ ì˜¤ë””ì˜¤ ë²„í¼ ì´ˆê¸°í™”"""
        with self._sessions_lock:
            self._sessions[session_id] = _SessionBuffer(
                sample_rate=sample_rate,
                last_feed_time=time.time(),
            )
        print(f"ğŸ™ï¸ [WhisperSTT] ì„¸ì…˜ {session_id[:8]}... ì‹œì‘")

    def end_session(self, session_id: str) -> Optional[WhisperResult]:
        """ì„¸ì…˜ ì¢…ë£Œ: ë‚¨ì€ ë²„í¼ flush í›„ ì •ë¦¬"""
        result = self.flush(session_id)
        with self._sessions_lock:
            self._sessions.pop(session_id, None)
        return result

    # â”€â”€â”€â”€â”€â”€â”€â”€ ì˜¤ë””ì˜¤ ì…ë ¥ â”€â”€â”€â”€â”€â”€â”€â”€

    def feed_audio(self, session_id: str, pcm_bytes: bytes):
        """
        PCM ì˜¤ë””ì˜¤ ë°ì´í„°(16-bit, mono, 16kHz)ë¥¼ ë²„í¼ì— ì¶”ê°€.
        ì¼ì • ë¶„ëŸ‰ì´ ìŒ“ì´ê±°ë‚˜ ì¹¨ë¬µì´ ê°ì§€ë˜ë©´ ìë™ flush.
        """
        with self._sessions_lock:
            buf = self._sessions.get(session_id)
            if not buf or not buf.is_active:
                return

        buf.audio_chunks.append(pcm_bytes)
        buf.total_bytes += len(pcm_bytes)
        buf.last_feed_time = time.time()

        # RMS ì—ë„ˆì§€ë¡œ ì¹¨ë¬µ ê°ì§€
        rms = self._compute_rms(pcm_bytes)
        if rms < self.SILENCE_RMS_THRESHOLD:
            if buf.silence_start is None:
                buf.silence_start = time.time()
            elif (time.time() - buf.silence_start) >= self.SILENCE_FLUSH_SECONDS:
                # ì¹¨ë¬µì´ ì¶©ë¶„íˆ ì§€ì†ë¨ â†’ flush
                duration = self._buffer_duration(buf)
                if duration >= self.MIN_AUDIO_DURATION:
                    self._async_flush(session_id)
                    buf.silence_start = None
        else:
            buf.silence_start = None

        # ìµœëŒ€ ë²„í¼ ì´ˆê³¼ ì‹œ ê°•ì œ flush
        if self._buffer_duration(buf) >= self.MAX_BUFFER_SECONDS:
            self._async_flush(session_id)

    @staticmethod
    def _compute_rms(pcm_bytes: bytes) -> float:
        """16-bit PCMì˜ RMS ì—ë„ˆì§€ ê³„ì‚°"""
        if len(pcm_bytes) < 2:
            return 0.0
        n_samples = len(pcm_bytes) // 2
        samples = struct.unpack(f"<{n_samples}h", pcm_bytes[:n_samples * 2])
        if not samples:
            return 0.0
        sq_sum = sum(s * s for s in samples)
        return (sq_sum / n_samples) ** 0.5

    @staticmethod
    def _buffer_duration(buf: _SessionBuffer) -> float:
        """ë²„í¼ì— ìŒ“ì¸ ì˜¤ë””ì˜¤ ê¸¸ì´ (ì´ˆ)"""
        bytes_per_second = buf.sample_rate * buf.channels * buf.sample_width
        return buf.total_bytes / bytes_per_second if bytes_per_second > 0 else 0.0

    # â”€â”€â”€â”€â”€â”€â”€â”€ ë³€í™˜ â”€â”€â”€â”€â”€â”€â”€â”€

    def flush(self, session_id: str) -> Optional[WhisperResult]:
        """
        ë²„í¼ì˜ ëª¨ë“  ì˜¤ë””ì˜¤ë¥¼ ì¦‰ì‹œ Whisperë¡œ ë³€í™˜.
        ë™ê¸° í˜¸ì¶œ â€” ê²°ê³¼ë¥¼ ì§ì ‘ ë°˜í™˜.
        """
        buf = self._sessions.get(session_id)
        if not buf or buf.total_bytes == 0:
            return None

        # ë²„í¼ì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ ë° ì´ˆê¸°í™”
        chunks = list(buf.audio_chunks)
        buf.audio_chunks.clear()
        buf.total_bytes = 0

        pcm_data = b"".join(chunks)
        if len(pcm_data) < buf.sample_rate * buf.sample_width:
            # 0.5ì´ˆ ë¯¸ë§Œì€ ë¬´ì‹œ
            return None

        return self._transcribe(pcm_data, buf.sample_rate, session_id)

    def _async_flush(self, session_id: str):
        """ë¹„ë™ê¸° flush â€” ìŠ¤ë ˆë“œí’€ì—ì„œ ë³€í™˜"""
        buf = self._sessions.get(session_id)
        if not buf or buf.total_bytes == 0:
            return

        chunks = list(buf.audio_chunks)
        buf.audio_chunks.clear()
        buf.total_bytes = 0

        pcm_data = b"".join(chunks)
        if len(pcm_data) < buf.sample_rate * buf.sample_width:
            return

        def _worker():
            result = self._transcribe(pcm_data, buf.sample_rate, session_id)
            if result and self.on_result:
                self.on_result(session_id, result)

        self._executor.submit(_worker)

    def _transcribe(
        self, pcm_data: bytes, sample_rate: int, session_id: str = ""
    ) -> Optional[WhisperResult]:
        """PCM ë°ì´í„°ë¥¼ Whisperë¡œ ë³€í™˜"""
        self._ensure_model()

        # PCM â†’ WAV (in-memory)
        wav_buffer = io.BytesIO()
        with wave.open(wav_buffer, "wb") as wf:
            wf.setnchannels(1)
            wf.setsampwidth(2)  # 16-bit
            wf.setframerate(sample_rate)
            wf.writeframes(pcm_data)
        wav_buffer.seek(0)

        try:
            if self._use_faster_whisper:
                return self._transcribe_faster_whisper(wav_buffer, session_id)
            else:
                return self._transcribe_openai_whisper(wav_buffer, session_id)
        except Exception as e:
            print(f"[WhisperSTT] ë³€í™˜ ì˜¤ë¥˜: {e}")
            return None

    def _transcribe_faster_whisper(
        self, wav_buffer: io.BytesIO, session_id: str
    ) -> Optional[WhisperResult]:
        """faster-whisperë¡œ ë³€í™˜"""
        import numpy as np

        # WAV â†’ numpy array
        wav_buffer.seek(0)
        with wave.open(wav_buffer, "rb") as wf:
            frames = wf.readframes(wf.getnframes())
            audio = np.frombuffer(frames, dtype=np.int16).astype(np.float32) / 32768.0

        segments_iter, info = self._model.transcribe(
            audio,
            language=self.language,
            beam_size=5,
            word_timestamps=True,
            vad_filter=True,
            vad_parameters=dict(
                min_silence_duration_ms=500,
                speech_pad_ms=200,
            ),
        )

        segments = []
        all_words = []
        full_text_parts = []

        for seg in segments_iter:
            words_list = []
            if seg.words:
                for w in seg.words:
                    word_dict = {
                        "word": w.word.strip(),
                        "start": round(w.start, 3),
                        "end": round(w.end, 3),
                        "confidence": round(w.probability, 4),
                    }
                    words_list.append(word_dict)
                    all_words.append(word_dict)

            seg_obj = WhisperSegment(
                text=seg.text.strip(),
                start=round(seg.start, 3),
                end=round(seg.end, 3),
                confidence=round(seg.avg_log_prob if hasattr(seg, 'avg_log_prob') else 0.0, 4),
                words=words_list if words_list else None,
            )
            segments.append(seg_obj)
            full_text_parts.append(seg.text.strip())

        transcript = " ".join(full_text_parts)
        if not transcript.strip():
            return None

        # ë„ì–´ì“°ê¸° ë³´ì •
        if self._spacing_corrector:
            corrected = self._spacing_corrector.correct(transcript)
            if corrected and corrected.strip():
                transcript = corrected

        return WhisperResult(
            transcript=transcript,
            segments=segments,
            language=info.language if hasattr(info, 'language') else self.language,
            duration=round(info.duration if hasattr(info, 'duration') else 0.0, 3),
            is_final=True,
            words=all_words if all_words else None,
        )

    def _transcribe_openai_whisper(
        self, wav_buffer: io.BytesIO, session_id: str
    ) -> Optional[WhisperResult]:
        """openai-whisperë¡œ ë³€í™˜ (í´ë°±)"""
        import tempfile
        import numpy as np

        # WAVë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ (openai-whisperëŠ” íŒŒì¼ ê²½ë¡œ í•„ìš”)
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
            tmp.write(wav_buffer.read())
            tmp_path = tmp.name

        try:
            result = self._model.transcribe(
                tmp_path,
                language=self.language,
                word_timestamps=True,
                fp16=False,
            )

            segments = []
            all_words = []
            for seg in result.get("segments", []):
                words_list = []
                for w in seg.get("words", []):
                    word_dict = {
                        "word": w.get("word", "").strip(),
                        "start": round(w.get("start", 0.0), 3),
                        "end": round(w.get("end", 0.0), 3),
                        "confidence": round(w.get("probability", 0.0), 4),
                    }
                    words_list.append(word_dict)
                    all_words.append(word_dict)

                seg_obj = WhisperSegment(
                    text=seg.get("text", "").strip(),
                    start=round(seg.get("start", 0.0), 3),
                    end=round(seg.get("end", 0.0), 3),
                    confidence=round(seg.get("avg_logprob", 0.0), 4),
                    words=words_list if words_list else None,
                )
                segments.append(seg_obj)

            transcript = result.get("text", "").strip()
            if not transcript:
                return None

            # ë„ì–´ì“°ê¸° ë³´ì •
            if self._spacing_corrector:
                corrected = self._spacing_corrector.correct(transcript)
                if corrected and corrected.strip():
                    transcript = corrected

            return WhisperResult(
                transcript=transcript,
                segments=segments,
                language=result.get("language", self.language),
                duration=0.0,
                is_final=True,
                words=all_words if all_words else None,
            )
        finally:
            try:
                os.unlink(tmp_path)
            except OSError:
                pass

    # â”€â”€â”€â”€â”€â”€â”€â”€ ìƒíƒœ ì¡°íšŒ â”€â”€â”€â”€â”€â”€â”€â”€

    def get_status(self) -> Dict[str, Any]:
        """ì„œë¹„ìŠ¤ ìƒíƒœ ì •ë³´"""
        return {
            "available": _WHISPER_AVAILABLE,
            "model_loaded": self._model_loaded,
            "model_size": self.model_size,
            "engine": "faster-whisper" if self._use_faster_whisper else "openai-whisper",
            "device": self.device,
            "language": self.language,
            "active_sessions": len(self._sessions),
            "spacing_correction": self._spacing_corrector is not None,
        }

    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self._executor.shutdown(wait=False)
        self._sessions.clear()
        print("[WhisperSTT] ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


# ========== ë¹„ë™ê¸° ì–´ëŒ‘í„° (ì„œë²„ í†µí•©ìš©) ==========

async def process_audio_with_whisper(
    track,
    session_id: str,
    whisper_service: WhisperSTTService,
    broadcast_fn: Callable,
    speech_service=None,
):
    """
    aiortc ì˜¤ë””ì˜¤ íŠ¸ë™ì„ Whisper STTë¡œ ì²˜ë¦¬.
    Deepgramì˜ `_process_audio_with_stt`ì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤.

    Args:
        track: aiortc AudioStreamTrack
        session_id: ë©´ì ‘ ì„¸ì…˜ ID
        whisper_service: WhisperSTTService ì¸ìŠ¤í„´ìŠ¤
        broadcast_fn: async (session_id, data_dict) â†’ None
        speech_service: SpeechAnalysisService (Optional)
    """
    import numpy as np

    whisper_service.start_session(session_id)

    # ê²°ê³¼ ì½œë°± ì„¤ì • (ë¹„ë™ê¸° flushìš©)
    loop = asyncio.get_event_loop()

    def _on_result(sid: str, result: WhisperResult):
        """Whisper ë³€í™˜ ê²°ê³¼ë¥¼ WebSocketìœ¼ë¡œ ë¸Œë¡œë“œìºìŠ¤íŠ¸"""
        if not result or not result.transcript:
            return

        data = {
            "type": "stt_result",
            "transcript": result.transcript,
            "is_final": result.is_final,
            "timestamp": time.time(),
            "source": "whisper",
        }

        # ë°œí™” ë¶„ì„ ì„œë¹„ìŠ¤ì— ë°ì´í„° ì „ë‹¬
        if speech_service:
            try:
                words_list = result.words
                avg_confidence = None
                if words_list:
                    confs = [w.get("confidence", 0) for w in words_list]
                    avg_confidence = sum(confs) / len(confs) if confs else None

                speech_service.add_stt_result(
                    sid,
                    result.transcript,
                    result.is_final,
                    confidence=avg_confidence,
                    words=words_list,
                )
            except Exception as e:
                print(f"[WhisperSTT] SpeechAnalysis ì „ë‹¬ ì˜¤ë¥˜: {e}")

        # ì´ë²¤íŠ¸ ë£¨í”„ì— ë¸Œë¡œë“œìºìŠ¤íŠ¸ íƒœìŠ¤í¬ ì˜ˆì•½
        asyncio.run_coroutine_threadsafe(broadcast_fn(sid, data), loop)

    whisper_service.on_result = _on_result

    print(f"[WhisperSTT] ì„¸ì…˜ {session_id} ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘ (ì˜¤í”„ë¼ì¸)")

    try:
        while True:
            frame = await track.recv()
            try:
                audio_data = frame.to_ndarray()
                # 16bit PCM ë³€í™˜
                if audio_data.dtype == np.float32 or audio_data.dtype == np.float64:
                    audio_bytes = (audio_data * 32767).astype(np.int16).tobytes()
                else:
                    audio_bytes = audio_data.astype(np.int16).tobytes()

                whisper_service.feed_audio(session_id, audio_bytes)
            except Exception:
                pass
    except Exception as e:
        print(f"[WhisperSTT] ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì¢…ë£Œ: {e}")
    finally:
        # ë‚¨ì€ ë²„í¼ flush
        final_result = whisper_service.end_session(session_id)
        if final_result and final_result.transcript:
            _on_result(session_id, final_result)
