# RAG(ê²€ìƒ‰ ì¦ê°• ìƒì„±) ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  í™˜ê²½ ì„¤ì •ì„ ì¤€ë¹„
import asyncio
import hashlib  # RAG ê²€ìƒ‰ ê²°ê³¼ ìºì‹±ìš© í•´ì‹œ ìƒì„±
import json  # JSON íŒŒì¼ íŒŒì‹±ìš©
import os  # ìš´ì˜ì²´ì œì™€ ìƒí˜¸ì‘ìš©í•˜ê¸° ìœ„í•´ ì‚¬ìš©. ì‹œìŠ¤í…œì˜ í™˜ê²½ ë³€ìˆ˜ì— ì ‘ê·¼í•˜ê±°ë‚˜, íŒŒì¼ ê²½ë¡œë¥¼ ë‹¤ë£° ë•Œ í•„ìš”
import pickle  # Document ê°ì²´ ì§ë ¬í™”/ì—­ì§ë ¬í™”
import sys

# Windowsì—ì„œ psycopg3 async ëª¨ë“œ í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°
# ProactorEventLoopëŠ” psycopg3ì—ì„œ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ SelectorEventLoopìœ¼ë¡œ ë³€ê²½
if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
# ì‹¤ì œë¡œ .env íŒŒì¼ì„ ì½ì–´ì„œ ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ë¡œ ë“±ë¡í•˜ëŠ” ë„êµ¬
from dotenv import load_dotenv
from langchain_community.document_loaders import (
    PyPDFLoader,  # PDF íŒŒì¼ì„ ì½ì–´ì˜¤ê¸° ìœ„í•œ ë„êµ¬
)
from langchain_core.documents import Document  # ë¬¸ì„œ ê°ì²´ íƒ€ì…

# ì„ë² ë”© ë„êµ¬: nomic-embed-text ëª¨ë¸ ì‚¬ìš© (768ì°¨ì›, ìµœëŒ€ 8192 í† í° ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°)
# Ollamaë¥¼ í†µí•´ ë¡œì»¬ì—ì„œ ì‹¤í–‰ë˜ë©°, ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒì„ ìœ„í•´ task-prefixë¥¼ ì§€ì›í•œë‹¤.
from langchain_ollama import OllamaEmbeddings

# PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë²¡í„° ì €ì¥ì†Œë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ë„êµ¬
# V2 PGVectorStore: ë°ì´í„° ìœ í˜•ë³„ ë¬¼ë¦¬ì  í…Œì´ë¸” ë¶„ë¦¬ (resume_embeddings / qa_embeddings)
from langchain_postgres import PGEngine, PGVectorStore
from langchain_postgres.v2.vectorstores import DistanceStrategy
from langchain_text_splitters import (
    RecursiveCharacterTextSplitter,  # í…ìŠ¤íŠ¸ë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ìë¥´ëŠ” ë„êµ¬
)

# ë³´ì•ˆê³¼ ì„¤ì • ê´€ë¦¬ë¥¼ ìœ„í•´ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜
load_dotenv()

# ========== ì„ë² ë”© ëª¨ë¸ ì„¤ì • ==========
# nomic-embed-text ì‚¬ì „ ìš”êµ¬ì‚¬í•­: ollama pull nomic-embed-text
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "nomic-embed-text")
VECTOR_SIZE = 768  # nomic-embed-text ì„ë² ë”© ì°¨ì› ìˆ˜
# nomic-embed-textëŠ” 8192 í† í°(ì•½ 6000ì)ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì²­í¬ë¥¼ í¬ê²Œ ì„¤ì • ê°€ëŠ¥
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "1500"))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "300"))

# ========== í…Œì´ë¸” ì´ë¦„ ì„¤ì • ==========
RESUME_TABLE = "resume_embeddings"  # ì´ë ¥ì„œ ë²¡í„° í…Œì´ë¸”
QA_TABLE = "qa_embeddings"  # ë©´ì ‘ Q&A ë²¡í„° í…Œì´ë¸”

# ========== Redis ìºì‹± ì„¤ì • ==========
# RAG ê²€ìƒ‰ ê²°ê³¼ë¥¼ Redisì— ìºì‹±í•˜ì—¬ Ollama ì„ë² ë”© API í˜¸ì¶œì„ ê±´ë„ˆë›´
# ë™ì¼ ì¿¼ë¦¬ ë°˜ë³µ ì‹œ GPU ë¶€í•˜ ê°ì†Œ + ì‘ë‹µ ì‹œê°„ ë‹¨ì¶•
RAG_CACHE_TTL = int(os.getenv("RAG_CACHE_TTL", "1800"))  # ê¸°ë³¸ 30ë¶„ (1800ì´ˆ)
RAG_CACHE_PREFIX = "rag_cache:"  # Redis í‚¤ ì ‘ë‘ì–´
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")

# Redis í´ë¼ì´ì–¸íŠ¸ ì‹±ê¸€í†¤ (ëª¨ë“ˆ ë ˆë²¨)
_rag_redis_client = None


def _get_rag_redis():
    """RAG ìºì‹±ìš© Redis í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜ (ì‹¤íŒ¨ ì‹œ None)

    Redis ì—°ê²°ì´ ë¶ˆê°€ëŠ¥í•´ë„ RAG ê²€ìƒ‰ ìì²´ëŠ” ì •ìƒ ë™ì‘í•˜ë„ë¡
    Noneì„ ë°˜í™˜í•˜ì—¬ Graceful Degradationì„ ë³´ì¥í•©ë‹ˆë‹¤.
    """
    global _rag_redis_client
    if _rag_redis_client is not None:
        return _rag_redis_client
    try:
        import redis

        _rag_redis_client = redis.from_url(REDIS_URL, decode_responses=False)
        # ì—°ê²° í…ŒìŠ¤íŠ¸ (1ì´ˆ íƒ€ì„ì•„ì›ƒ)
        _rag_redis_client.ping()
        print("âœ… [RAG Cache] Redis ìºì‹œ ì—°ê²° ì™„ë£Œ")
        return _rag_redis_client
    except Exception as e:
        print(f"âš ï¸ [RAG Cache] Redis ìºì‹œ ë¹„í™œì„±í™” (ë¬´ì‹œ): {e}")
        _rag_redis_client = None
        return None


def _make_cache_key(table_name: str, query: str, k: int) -> str:
    """RAG ìºì‹œ í‚¤ ìƒì„± (í…Œì´ë¸”ëª… + ì¿¼ë¦¬ í•´ì‹œ + k)

    SHA-256ìœ¼ë¡œ ì¿¼ë¦¬ë¥¼ í•´ì‹œí•˜ì—¬ í‚¤ ê¸¸ì´ë¥¼ ì¼ì •í•˜ê²Œ ìœ ì§€í•©ë‹ˆë‹¤.
    í…Œì´ë¸”ëª…ê³¼ kê°’ì„ í¬í•¨í•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ ê²€ìƒ‰ ì„¤ì •ì˜ êµ¬ë¶„ì„ ë³´ì¥í•©ë‹ˆë‹¤.
    """
    query_hash = hashlib.sha256(query.encode("utf-8")).hexdigest()[:16]
    return f"{RAG_CACHE_PREFIX}{table_name}:{query_hash}:k{k}"


def _cache_get(key: str):
    """Redisì—ì„œ ìºì‹œëœ ê²€ìƒ‰ ê²°ê³¼ ì¡°íšŒ

    Returns:
        list[Document] ë˜ëŠ” None (ìºì‹œ ë¯¸ìŠ¤ ì‹œ)
    """
    r = _get_rag_redis()
    if not r:
        return None
    try:
        cached = r.get(key)
        if cached:
            return pickle.loads(cached)
    except Exception as e:
        print(f"âš ï¸ [RAG Cache] ìºì‹œ ì½ê¸° ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
    return None


def _cache_set(key: str, documents, ttl: int = RAG_CACHE_TTL):
    """RAG ê²€ìƒ‰ ê²°ê³¼ë¥¼ Redisì— ìºì‹±

    Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ pickleë¡œ ì§ë ¬í™”í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
    TTL ë§Œë£Œ ì‹œ ìë™ ì‚­ì œë˜ì–´ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
    """
    r = _get_rag_redis()
    if not r:
        return
    try:
        r.setex(key, ttl, pickle.dumps(documents))
    except Exception as e:
        print(f"âš ï¸ [RAG Cache] ìºì‹œ ì“°ê¸° ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")


def _get_connection_string() -> str:
    """DB ì—°ê²° ë¬¸ìì—´ì„ ê°€ì ¸ì˜µë‹ˆë‹¤ (psycopg3 í˜•ì‹)."""
    conn_str = os.getenv("POSTGRES_CONNECTION_STRING", "")
    if conn_str.startswith("postgresql://"):
        conn_str = conn_str.replace("postgresql://", "postgresql+psycopg://", 1)
    return conn_str


class ResumeRAG:
    """
    ì´ë ¥ì„œ(PDF)ì™€ ë©´ì ‘ Q&A ë°ì´í„°ë¥¼ PostgreSQL(pgvector)ì— ì €ì¥í•˜ê³ ,
    ê´€ë ¨ ë‚´ìš©ì„ ê²€ìƒ‰(Retriever)í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” í´ë˜ìŠ¤.

    PGVectorStore V2 ì‚¬ìš© â€” ë°ì´í„° ìœ í˜•ë³„ ë¬¼ë¦¬ì  í…Œì´ë¸” ë¶„ë¦¬:
    - resume_embeddings: ì´ë ¥ì„œ(PDF) ë²¡í„° ì €ì¥
    - qa_embeddings: ë©´ì ‘ Q&A ì°¸ì¡° ë°ì´í„° ë²¡í„° ì €ì¥

    ì„ë² ë”© ëª¨ë¸: nomic-embed-text (768ì°¨ì›, 8192 í† í° ì»¨í…ìŠ¤íŠ¸)
    - ê²€ìƒ‰ì— ìµœì í™”ëœ ì „ìš© ì„ë² ë”© ëª¨ë¸
    - search_document: / search_query: ì ‘ë‘ì‚¬ë¡œ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ
    """

    def __init__(self, table_name: str, connection_string: str = None):
        """
        Args:
            table_name: ë²¡í„° ì €ì¥ í…Œì´ë¸”ëª… (RESUME_TABLE ë˜ëŠ” QA_TABLE)
            connection_string: PostgreSQL ì—°ê²° ë¬¸ìì—´ (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)
        """
        conn_str = connection_string or _get_connection_string()
        # PGEngineì€ psycopg3 (async) ë“œë¼ì´ë²„ í•„ìš” â†’ ì—°ê²° ë¬¸ìì—´ ê°•ì œ ë³€í™˜
        if conn_str.startswith("postgresql://"):
            conn_str = conn_str.replace("postgresql://", "postgresql+psycopg://", 1)
        elif conn_str.startswith("postgresql+psycopg2://"):
            conn_str = conn_str.replace(
                "postgresql+psycopg2://", "postgresql+psycopg://", 1
            )
        self.connection = conn_str
        self.table_name = table_name

        # nomic-embed-text ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (768ì°¨ì› ë²¡í„° ìƒì„±)
        self.embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)

        # PGVectorStore V2: ë¬¼ë¦¬ì  í…Œì´ë¸” ë¶„ë¦¬
        self.engine = PGEngine.from_connection_string(url=conn_str)
        self._ensure_table(table_name)
        self.vector_store = PGVectorStore.create_sync(
            engine=self.engine,
            table_name=table_name,
            embedding_service=self.embeddings,
            distance_strategy=DistanceStrategy.COSINE_DISTANCE,
        )
        print(f"ğŸ“¦ [RAG] í…Œì´ë¸” '{table_name}' ì—°ê²°ë¨")

    def _ensure_table(self, table_name: str):
        """V2 í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            self.engine.init_vectorstore_table(
                table_name=table_name,
                vector_size=VECTOR_SIZE,
                overwrite_existing=False,
            )
            print(f"âœ… í…Œì´ë¸” '{table_name}' ì¤€ë¹„ ì™„ë£Œ")
        except Exception as e:
            # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë¬´ì‹œ
            if "already exists" in str(e).lower():
                print(f"â„¹ï¸ í…Œì´ë¸” '{table_name}' ì´ë¯¸ ì¡´ì¬í•¨")
            else:
                print(f"âš ï¸ í…Œì´ë¸” ìƒì„± ì¤‘ ê²½ê³ : {e}")

    def load_and_index_pdf(self, pdf_path: str):
        """
        PDF íŒŒì¼ì„ ì½ì–´ì„œ ì²­í¬ ë‹¨ìœ„ë¡œ ìë¥´ê³ , DBì— ë²¡í„°í™”í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.

        nomic-embed-text ìµœì í™”:
        - 8192 í† í° ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ë¥¼ í™œìš©í•˜ì—¬ í° ì²­í¬ ì‚¬ìš© (ê¸°ë³¸ 1500ì)
        - 'search_document:' ì ‘ë‘ì‚¬ë¥¼ ì¶”ê°€í•˜ì—¬ ì„ë² ë”© í’ˆì§ˆ í–¥ìƒ
        """
        if not os.path.exists(pdf_path):
            print(f"Error: {pdf_path} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return 0

        print(f"Loading PDF: {pdf_path} ...")
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()

        # nomic-embed-textì˜ ë„“ì€ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°(8192 í† í°)ë¥¼ í™œìš©í•œ ì²­í‚¹
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            separators=["\n\n", "\n", ". ", " ", ""],  # ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í•  ìš°ì„ 
        )
        splits = text_splitter.split_documents(documents)
        print(
            f"Created {len(splits)} chunks (chunk_size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP})."
        )

        # nomic-embed-text ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒì„ ìœ„í•´ 'search_document:' ì ‘ë‘ì‚¬ ì¶”ê°€
        for doc in splits:
            doc.page_content = f"search_document: {doc.page_content}"

        # DBì— ì €ì¥ (add_documents)
        print("Indexing to PostgreSQL (pgvector)...")
        self.vector_store.add_documents(splits)
        print("Indexing Complete.")
        return len(splits)

    def load_and_index_json(self, json_path: str, batch_size: int = 100):
        """
        ë©´ì ‘ Q&A JSON ë°ì´í„°ë¥¼ ì½ì–´ì„œ ë²¡í„°í™”í•˜ì—¬ DBì— ì €ì¥í•©ë‹ˆë‹¤.
        V2 ëª¨ë“œì—ì„œëŠ” ë³„ë„ ë¬¼ë¦¬ì  í…Œì´ë¸”(qa_embeddings)ì— ì €ì¥ë©ë‹ˆë‹¤.

        JSON í˜•ì‹: [{"id": 1, "question": "...", "answer": "..."}, ...]
        ê° í•­ëª©ì„ "ë©´ì ‘ ì§ˆë¬¸: {question}\\nëª¨ë²” ë‹µë³€: {answer}" í˜•íƒœì˜ Documentë¡œ ë³€í™˜ í›„ ì„ë² ë”©í•©ë‹ˆë‹¤.

        Args:
            json_path: JSON íŒŒì¼ ê²½ë¡œ
            batch_size: í•œë²ˆì— ì„ë² ë”©í•  ë¬¸ì„œ ìˆ˜ (ë©”ëª¨ë¦¬/ì†ë„ ì¡°ì ˆìš©)

        Returns:
            ì €ì¥ëœ ë¬¸ì„œ ìˆ˜
        """
        if not os.path.exists(json_path):
            print(f"Error: {json_path} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return 0

        print(f"Loading JSON: {json_path} ...")
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        if not isinstance(data, list):
            print("Error: JSON íŒŒì¼ì€ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
            return 0

        print(f"ì´ {len(data)}ê°œì˜ Q&A í•­ëª© ë°œê²¬.")

        # Q&Aë¥¼ Document ê°ì²´ë¡œ ë³€í™˜
        documents = []
        for item in data:
            q = item.get("question", "")
            a = item.get("answer", "")
            item_id = item.get("id", "")

            if not q or not a:
                continue

            # ì§ˆë¬¸ê³¼ ë‹µë³€ì„ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ê²°í•© (ê²€ìƒ‰ ì‹œ ì§ˆë¬¸ìœ¼ë¡œë„, ë‹µë³€ ë‚´ìš©ìœ¼ë¡œë„ ë§¤ì¹­ ê°€ëŠ¥)
            content = f"ë©´ì ‘ ì§ˆë¬¸: {q}\nëª¨ë²” ë‹µë³€: {a}"

            doc = Document(
                page_content=f"search_document: {content}",
                metadata={
                    "source": "interview_qa_data",
                    "qa_id": str(item_id),
                    "question": q,
                    "type": "interview_reference",
                },
            )
            documents.append(doc)

        if not documents:
            print("Warning: ë³€í™˜ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0

        # ê¸´ ë‹µë³€ì€ ì²­í¬ ë¶„í•  (nomic-embed-text 8192 í† í° ì œí•œ ê³ ë ¤)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            separators=["\n\n", "\n", ". ", " ", ""],
        )
        splits = text_splitter.split_documents(documents)
        print(f"ì²­í¬ ë¶„í•  ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ â†’ {len(splits)}ê°œ ì²­í¬")

        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ DBì— ì €ì¥ (ëŒ€ëŸ‰ ë°ì´í„° ì•ˆì •ì„±)
        total_indexed = 0
        for i in range(0, len(splits), batch_size):
            batch = splits[i : i + batch_size]
            self.vector_store.add_documents(batch)
            total_indexed += len(batch)
            print(
                f"  ì§„í–‰ë¥ : {total_indexed}/{len(splits)} ({total_indexed * 100 // len(splits)}%)"
            )

        print(
            f"âœ… JSON ì¸ë±ì‹± ì™„ë£Œ: {total_indexed}ê°œ ì²­í¬ ì €ì¥ë¨ (í…Œì´ë¸”: {self.table_name})"
        )
        return total_indexed

    def get_retriever(self, k: int = 4):
        """
        LangChain Retriever ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        nomic-embed-text ìµœì í™”:
        - MMR(Maximal Marginal Relevance) ê²€ìƒ‰ìœ¼ë¡œ ë‹¤ì–‘ì„±ê³¼ ê´€ë ¨ì„± ê· í˜•
        - fetch_kë¡œ í›„ë³´ë¥¼ ë„“ê²Œ ê°€ì ¸ì˜¨ ë’¤ kê°œë¥¼ ë‹¤ì–‘í•˜ê²Œ ì„ íƒ
        """
        return self.vector_store.as_retriever(
            search_type="mmr",
            search_kwargs={
                "k": k,  # ìµœì¢… ë°˜í™˜ ë¬¸ì„œ ìˆ˜
                "fetch_k": k * 5,  # MMR í›„ë³´ ë¬¸ì„œ ìˆ˜ (ë‹¤ì–‘ì„± í™•ë³´)
                "lambda_mult": 0.7,  # 0=ìµœëŒ€ ë‹¤ì–‘ì„±, 1=ìµœëŒ€ ìœ ì‚¬ì„± (0.7: ê´€ë ¨ì„± ìš°ì„ )
            },
        )

    def similarity_search(self, query: str, k: int = 4):
        """
        nomic-embed-textì— ìµœì í™”ëœ ìœ ì‚¬ë„ ê²€ìƒ‰ (â­ Redis ìºì‹± ì ìš©).

        1) Redis ìºì‹œ í™•ì¸ â†’ íˆíŠ¸ ì‹œ Ollama ì„ë² ë”© í˜¸ì¶œ ìƒëµ (GPU ë¶€í•˜ ê°ì†Œ)
        2) ìºì‹œ ë¯¸ìŠ¤ ì‹œ ì¿¼ë¦¬ì— 'search_query:' ì ‘ë‘ì‚¬ë¥¼ ìë™ ì¶”ê°€í•˜ì—¬ ê²€ìƒ‰ í’ˆì§ˆì„ ë†’ì„
        3) ê²°ê³¼ë¥¼ Redisì— ìºì‹± (TTL: 30ë¶„)
        """
        # â”€â”€ 1. Redis ìºì‹œ í™•ì¸ â”€â”€
        cache_key = _make_cache_key(self.table_name, query, k)
        cached_docs = _cache_get(cache_key)
        if cached_docs is not None:
            print(
                f"ğŸŸ¢ [RAG Cache] ìºì‹œ íˆíŠ¸ â€” Ollama ì„ë² ë”© ìƒëµ ({self.table_name}, k={k})"
            )
            return cached_docs

        # â”€â”€ 2. ìºì‹œ ë¯¸ìŠ¤ â†’ pgvector ê²€ìƒ‰ ì‹¤í–‰ (Ollama ì„ë² ë”© í˜¸ì¶œ) â”€â”€
        prefixed_query = f"search_query: {query}"
        results = self.vector_store.similarity_search(prefixed_query, k=k)
        # ê²°ê³¼ì—ì„œ 'search_document:' ì ‘ë‘ì‚¬ ì œê±°
        for doc in results:
            if doc.page_content.startswith("search_document: "):
                doc.page_content = doc.page_content[len("search_document: ") :]

        # â”€â”€ 3. ê²°ê³¼ë¥¼ Redisì— ìºì‹± (TTL: 30ë¶„) â”€â”€
        if results:
            _cache_set(cache_key, results)
            print(
                f"ğŸŸ¡ [RAG Cache] ìºì‹œ ì €ì¥ ì™„ë£Œ ({self.table_name}, {len(results)}ê°œ ë¬¸ì„œ, TTL={RAG_CACHE_TTL}ì´ˆ)"
            )

        return results

    def clear_table(self):
        """
        í…Œì´ë¸”ì˜ ë²¡í„° ë°ì´í„°ë¥¼ ì „ë¶€ ì‚­ì œí•˜ê³  ì¬ìƒì„±í•©ë‹ˆë‹¤.
        """
        self.engine.init_vectorstore_table(
            table_name=self.table_name,
            vector_size=VECTOR_SIZE,
            overwrite_existing=True,
        )
        print(f"âœ… í…Œì´ë¸” '{self.table_name}' ì´ˆê¸°í™” ì™„ë£Œ")
