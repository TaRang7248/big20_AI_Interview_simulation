# RAG(ê²€ìƒ‰ ì¦ê°• ìƒì„±) ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  í™˜ê²½ ì„¤ì •ì„ ì¤€ë¹„
import os # ìš´ì˜ì²´ì œì™€ ìƒí˜¸ì‘ìš©í•˜ê¸° ìœ„í•´ ì‚¬ìš©. ì‹œìŠ¤í…œì˜ í™˜ê²½ ë³€ìˆ˜ì— ì ‘ê·¼í•˜ê±°ë‚˜, íŒŒì¼ ê²½ë¡œë¥¼ ë‹¤ë£° ë•Œ í•„ìš”
import sys
import json # JSON íŒŒì¼ íŒŒì‹±ìš©
import asyncio

# Windowsì—ì„œ psycopg3 async ëª¨ë“œ í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°
# ProactorEventLoopëŠ” psycopg3ì—ì„œ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ SelectorEventLoopìœ¼ë¡œ ë³€ê²½
if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
from langchain_community.document_loaders import PyPDFLoader # PDF íŒŒì¼ì„ ì½ì–´ì˜¤ê¸° ìœ„í•œ ë„êµ¬
from langchain_text_splitters import RecursiveCharacterTextSplitter # í…ìŠ¤íŠ¸ë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ìë¥´ëŠ” ë„êµ¬
from langchain_core.documents import Document # ë¬¸ì„œ ê°ì²´ íƒ€ì…

# ì„ë² ë”© ë„êµ¬: nomic-embed-text ëª¨ë¸ ì‚¬ìš© (768ì°¨ì›, ìµœëŒ€ 8192 í† í° ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°)
# Ollamaë¥¼ í†µí•´ ë¡œì»¬ì—ì„œ ì‹¤í–‰ë˜ë©°, ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒì„ ìœ„í•´ task-prefixë¥¼ ì§€ì›í•œë‹¤.
from langchain_ollama import OllamaEmbeddings
# PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë²¡í„° ì €ì¥ì†Œë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ë„êµ¬
# V2 PGVectorStore: ë°ì´í„° ìœ í˜•ë³„ ë¬¼ë¦¬ì  í…Œì´ë¸” ë¶„ë¦¬ (resume_embeddings / qa_embeddings)
from langchain_postgres import PGVectorStore, PGEngine
from langchain_postgres.v2.vectorstores import DistanceStrategy
# ì‹¤ì œë¡œ .env íŒŒì¼ì„ ì½ì–´ì„œ ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ë¡œ ë“±ë¡í•˜ëŠ” ë„êµ¬
from dotenv import load_dotenv

# ë³´ì•ˆê³¼ ì„¤ì • ê´€ë¦¬ë¥¼ ìœ„í•´ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜
load_dotenv()

# ========== ì„ë² ë”© ëª¨ë¸ ì„¤ì • ==========
# nomic-embed-text ì‚¬ì „ ìš”êµ¬ì‚¬í•­: ollama pull nomic-embed-text
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "nomic-embed-text")
VECTOR_SIZE = 768  # nomic-embed-text ì„ë² ë”© ì°¨ì› ìˆ˜
# nomic-embed-textëŠ” 8192 í† í°(ì•½ 6000ì)ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì²­í¬ë¥¼ í¬ê²Œ ì„¤ì • ê°€ëŠ¥
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "1500"))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "300"))

# ========== í…Œì´ë¸” ì´ë¦„ ì„¤ì • ==========
RESUME_TABLE = "resume_embeddings"      # ì´ë ¥ì„œ ë²¡í„° í…Œì´ë¸”
QA_TABLE = "qa_embeddings"              # ë©´ì ‘ Q&A ë²¡í„° í…Œì´ë¸”


def _get_connection_string() -> str:
    """DB ì—°ê²° ë¬¸ìì—´ì„ ê°€ì ¸ì˜µë‹ˆë‹¤ (psycopg3 í˜•ì‹)."""
    conn_str = os.getenv("POSTGRES_CONNECTION_STRING", "")
    if conn_str.startswith("postgresql://"):
        conn_str = conn_str.replace("postgresql://", "postgresql+psycopg://", 1)
    return conn_str


class ResumeRAG:
    """
    ì´ë ¥ì„œ(PDF)ì™€ ë©´ì ‘ Q&A ë°ì´í„°ë¥¼ PostgreSQL(pgvector)ì— ì €ì¥í•˜ê³ ,
    ê´€ë ¨ ë‚´ìš©ì„ ê²€ìƒ‰(Retriever)í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” í´ë˜ìŠ¤.
    
    PGVectorStore V2 ì‚¬ìš© â€” ë°ì´í„° ìœ í˜•ë³„ ë¬¼ë¦¬ì  í…Œì´ë¸” ë¶„ë¦¬:
    - resume_embeddings: ì´ë ¥ì„œ(PDF) ë²¡í„° ì €ì¥
    - qa_embeddings: ë©´ì ‘ Q&A ì°¸ì¡° ë°ì´í„° ë²¡í„° ì €ì¥
    
    ì„ë² ë”© ëª¨ë¸: nomic-embed-text (768ì°¨ì›, 8192 í† í° ì»¨í…ìŠ¤íŠ¸)
    - ê²€ìƒ‰ì— ìµœì í™”ëœ ì „ìš© ì„ë² ë”© ëª¨ë¸
    - search_document: / search_query: ì ‘ë‘ì‚¬ë¡œ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ
    """
    def __init__(self, table_name: str, connection_string: str = None):
        """
        Args:
            table_name: ë²¡í„° ì €ì¥ í…Œì´ë¸”ëª… (RESUME_TABLE ë˜ëŠ” QA_TABLE)
            connection_string: PostgreSQL ì—°ê²° ë¬¸ìì—´ (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)
        """
        conn_str = connection_string or _get_connection_string()
        # PGEngineì€ psycopg3 (async) ë“œë¼ì´ë²„ í•„ìš” â†’ ì—°ê²° ë¬¸ìì—´ ê°•ì œ ë³€í™˜
        if conn_str.startswith("postgresql://"):
            conn_str = conn_str.replace("postgresql://", "postgresql+psycopg://", 1)
        elif conn_str.startswith("postgresql+psycopg2://"):
            conn_str = conn_str.replace("postgresql+psycopg2://", "postgresql+psycopg://", 1)
        self.connection = conn_str
        self.table_name = table_name

        # nomic-embed-text ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (768ì°¨ì› ë²¡í„° ìƒì„±)
        self.embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)

        # PGVectorStore V2: ë¬¼ë¦¬ì  í…Œì´ë¸” ë¶„ë¦¬
        self.engine = PGEngine.from_connection_string(url=conn_str)
        self._ensure_table(table_name)
        self.vector_store = PGVectorStore.create_sync(
            engine=self.engine,
            table_name=table_name,
            embedding_service=self.embeddings,
            distance_strategy=DistanceStrategy.COSINE_DISTANCE,
        )
        print(f"ğŸ“¦ [RAG] í…Œì´ë¸” '{table_name}' ì—°ê²°ë¨")
    
    def _ensure_table(self, table_name: str):
        """V2 í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            self.engine.init_vectorstore_table(
                table_name=table_name,
                vector_size=VECTOR_SIZE,
                overwrite_existing=False,
            )
            print(f"âœ… í…Œì´ë¸” '{table_name}' ì¤€ë¹„ ì™„ë£Œ")
        except Exception as e:
            # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë¬´ì‹œ
            if "already exists" in str(e).lower():
                print(f"â„¹ï¸ í…Œì´ë¸” '{table_name}' ì´ë¯¸ ì¡´ì¬í•¨")
            else:
                print(f"âš ï¸ í…Œì´ë¸” ìƒì„± ì¤‘ ê²½ê³ : {e}")

    def load_and_index_pdf(self, pdf_path: str):
        """
        PDF íŒŒì¼ì„ ì½ì–´ì„œ ì²­í¬ ë‹¨ìœ„ë¡œ ìë¥´ê³ , DBì— ë²¡í„°í™”í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
        
        nomic-embed-text ìµœì í™”:
        - 8192 í† í° ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ë¥¼ í™œìš©í•˜ì—¬ í° ì²­í¬ ì‚¬ìš© (ê¸°ë³¸ 1500ì)
        - 'search_document:' ì ‘ë‘ì‚¬ë¥¼ ì¶”ê°€í•˜ì—¬ ì„ë² ë”© í’ˆì§ˆ í–¥ìƒ
        """
        if not os.path.exists(pdf_path):
            print(f"Error: {pdf_path} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return 0

        print(f"Loading PDF: {pdf_path} ...")
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()

        # nomic-embed-textì˜ ë„“ì€ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°(8192 í† í°)ë¥¼ í™œìš©í•œ ì²­í‚¹
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            separators=["\n\n", "\n", ". ", " ", ""]  # ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í•  ìš°ì„ 
        )
        splits = text_splitter.split_documents(documents)
        print(f"Created {len(splits)} chunks (chunk_size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP}).")

        # nomic-embed-text ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒì„ ìœ„í•´ 'search_document:' ì ‘ë‘ì‚¬ ì¶”ê°€
        for doc in splits:
            doc.page_content = f"search_document: {doc.page_content}"

        # DBì— ì €ì¥ (add_documents)
        print("Indexing to PostgreSQL (pgvector)...")
        self.vector_store.add_documents(splits)
        print("Indexing Complete.")
        return len(splits)

    def load_and_index_json(self, json_path: str, batch_size: int = 100):
        """
        ë©´ì ‘ Q&A JSON ë°ì´í„°ë¥¼ ì½ì–´ì„œ ë²¡í„°í™”í•˜ì—¬ DBì— ì €ì¥í•©ë‹ˆë‹¤.
        V2 ëª¨ë“œì—ì„œëŠ” ë³„ë„ ë¬¼ë¦¬ì  í…Œì´ë¸”(qa_embeddings)ì— ì €ì¥ë©ë‹ˆë‹¤.
        
        JSON í˜•ì‹: [{"id": 1, "question": "...", "answer": "..."}, ...]
        ê° í•­ëª©ì„ "ë©´ì ‘ ì§ˆë¬¸: {question}\\nëª¨ë²” ë‹µë³€: {answer}" í˜•íƒœì˜ Documentë¡œ ë³€í™˜ í›„ ì„ë² ë”©í•©ë‹ˆë‹¤.
        
        Args:
            json_path: JSON íŒŒì¼ ê²½ë¡œ
            batch_size: í•œë²ˆì— ì„ë² ë”©í•  ë¬¸ì„œ ìˆ˜ (ë©”ëª¨ë¦¬/ì†ë„ ì¡°ì ˆìš©)
        
        Returns:
            ì €ì¥ëœ ë¬¸ì„œ ìˆ˜
        """
        if not os.path.exists(json_path):
            print(f"Error: {json_path} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return 0

        print(f"Loading JSON: {json_path} ...")
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        if not isinstance(data, list):
            print("Error: JSON íŒŒì¼ì€ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
            return 0

        print(f"ì´ {len(data)}ê°œì˜ Q&A í•­ëª© ë°œê²¬.")

        # Q&Aë¥¼ Document ê°ì²´ë¡œ ë³€í™˜
        documents = []
        for item in data:
            q = item.get("question", "")
            a = item.get("answer", "")
            item_id = item.get("id", "")
            
            if not q or not a:
                continue
            
            # ì§ˆë¬¸ê³¼ ë‹µë³€ì„ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ê²°í•© (ê²€ìƒ‰ ì‹œ ì§ˆë¬¸ìœ¼ë¡œë„, ë‹µë³€ ë‚´ìš©ìœ¼ë¡œë„ ë§¤ì¹­ ê°€ëŠ¥)
            content = f"ë©´ì ‘ ì§ˆë¬¸: {q}\nëª¨ë²” ë‹µë³€: {a}"
            
            doc = Document(
                page_content=f"search_document: {content}",
                metadata={
                    "source": "interview_qa_data",
                    "qa_id": str(item_id),
                    "question": q,
                    "type": "interview_reference"
                }
            )
            documents.append(doc)

        if not documents:
            print("Warning: ë³€í™˜ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0

        # ê¸´ ë‹µë³€ì€ ì²­í¬ ë¶„í•  (nomic-embed-text 8192 í† í° ì œí•œ ê³ ë ¤)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        splits = text_splitter.split_documents(documents)
        print(f"ì²­í¬ ë¶„í•  ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ â†’ {len(splits)}ê°œ ì²­í¬")

        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ DBì— ì €ì¥ (ëŒ€ëŸ‰ ë°ì´í„° ì•ˆì •ì„±)
        total_indexed = 0
        for i in range(0, len(splits), batch_size):
            batch = splits[i:i + batch_size]
            self.vector_store.add_documents(batch)
            total_indexed += len(batch)
            print(f"  ì§„í–‰ë¥ : {total_indexed}/{len(splits)} ({total_indexed * 100 // len(splits)}%)")

        print(f"âœ… JSON ì¸ë±ì‹± ì™„ë£Œ: {total_indexed}ê°œ ì²­í¬ ì €ì¥ë¨ (í…Œì´ë¸”: {self.table_name})")
        return total_indexed

    def get_retriever(self, k: int = 4):
        """
        LangChain Retriever ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        nomic-embed-text ìµœì í™”:
        - MMR(Maximal Marginal Relevance) ê²€ìƒ‰ìœ¼ë¡œ ë‹¤ì–‘ì„±ê³¼ ê´€ë ¨ì„± ê· í˜•
        - fetch_kë¡œ í›„ë³´ë¥¼ ë„“ê²Œ ê°€ì ¸ì˜¨ ë’¤ kê°œë¥¼ ë‹¤ì–‘í•˜ê²Œ ì„ íƒ
        """
        return self.vector_store.as_retriever(
            search_type="mmr",
            search_kwargs={
                "k": k,            # ìµœì¢… ë°˜í™˜ ë¬¸ì„œ ìˆ˜
                "fetch_k": k * 5,  # MMR í›„ë³´ ë¬¸ì„œ ìˆ˜ (ë‹¤ì–‘ì„± í™•ë³´)
                "lambda_mult": 0.7  # 0=ìµœëŒ€ ë‹¤ì–‘ì„±, 1=ìµœëŒ€ ìœ ì‚¬ì„± (0.7: ê´€ë ¨ì„± ìš°ì„ )
            }
        )

    def similarity_search(self, query: str, k: int = 4):
        """
        nomic-embed-textì— ìµœì í™”ëœ ìœ ì‚¬ë„ ê²€ìƒ‰.
        ì¿¼ë¦¬ì— 'search_query:' ì ‘ë‘ì‚¬ë¥¼ ìë™ ì¶”ê°€í•˜ì—¬ ê²€ìƒ‰ í’ˆì§ˆì„ ë†’ì…ë‹ˆë‹¤.
        """
        prefixed_query = f"search_query: {query}"
        results = self.vector_store.similarity_search(prefixed_query, k=k)
        # ê²°ê³¼ì—ì„œ 'search_document:' ì ‘ë‘ì‚¬ ì œê±°
        for doc in results:
            if doc.page_content.startswith("search_document: "):
                doc.page_content = doc.page_content[len("search_document: "):]
        return results

    def clear_table(self):
        """
        í…Œì´ë¸”ì˜ ë²¡í„° ë°ì´í„°ë¥¼ ì „ë¶€ ì‚­ì œí•˜ê³  ì¬ìƒì„±í•©ë‹ˆë‹¤.
        """
        self.engine.init_vectorstore_table(
            table_name=self.table_name,
            vector_size=VECTOR_SIZE,
            overwrite_existing=True,
        )
        print(f"âœ… í…Œì´ë¸” '{self.table_name}' ì´ˆê¸°í™” ì™„ë£Œ")
