# TASK-010 Plan: Visual Analysis (MediaPipe) Integration

## 1. 개요 (Overview)
본 문서는 과거 구현 및 검증에 성공했던 **MediaPipe 기반 Visual 분석 모듈**을 Phase 2 프로젝트 표준으로 변환 및 통합하기 위한 계획입니다.  
이 단계의 핵심은 새로운 기능의 개발이나 실험이 아니라, **"이미 검증된 성공 사례의 안전한 이식(Porting)"**입니다.

## 2. Phase 2 역할 정의 (Role Definition)

### 2.1. 비언어적 분석의 3대 축
Visual 분석은 Emotion(TASK-008), Voice(TASK-009)와 함께 면접자의 비언어적 요소를 배타적이고 상호보완적으로 분석합니다.

- **Emotion (DeepFace)**: 표정에 드러난 감정 상태 분석
- **Voice (Parselmouth)**: 음성의 물리적 특성 및 운율 분석
- **Visual (MediaPipe)**: **사용자의 물리적 존재와 자세, 시선의 안정성 분석**

### 2.2. Playground 검증 목표
Phase 2 Playground에서는 정밀한 수치보다는 서비스 가능한 **"상태 판단(State Judgment)"**을 우선합니다.
- **Presence**: 사용자가 화면 내에 존재하는가? (Face Detection)
- **Attention**: 사용자가 정면을 응시하는가? (Gaze Tracking)
- **Pose**: 사용자의 자세가 안정적인가? (Pose Landmark)

## 3. 구현 원칙 (Implementation Principles)

### 3.1. 성공 이력의 자산화 (Assetization of Success)
과거 성공했던 구현을 "Legacy"가 아닌 **"Baseline"**으로 정의합니다.
실패가 발생한다면 로직의 문제가 아니라 **"환경 및 통합의 문제"**로 간주하고 접근합니다.
- **알고리즘**: 검증된 MediaPipe 파이프라인 유지
- **데이터 구조**: 검증된 `VisualResultDTO` 스키마 유지
- **예외 처리**: "No Face" 등의 Edge Case 처리 로직 그대로 계승

### 3.2. 안전 우선 출력 (Safety First)
분석 결과는 항상 시스템이 핸들링 가능한 형태여야 합니다.
- 데이터가 없으면(No Face) `null`이나 오류를 뱉는 것이 아니라, **"부재(Absence)" 상태를 명확히 반환**합니다.
- 모든 수치 데이터는 `0.0 ~ 1.0` 또는 `True/False` 등 정규화된 형태를 지향합니다.

### 3.3. 리스크 관리 (Managed Risk)
이미 확인된 리스크(의존성 충돌, 초기화 지연 등)는 **"해결해야 할 문제"가 아니라 "관리해야 할 제약사항"**입니다.
- 이를 위해 의존성 버전과 초기화 시점을 엄격히 통제합니다.

### 3.4. 운영 안정성 제안 (Operational Stability Recommendation)
이미 성공한 이력의 재현 가능성을 보장하고, 의존성 충돌 시 신속한 복구를 위해 다음 원칙을 강력히 권장합니다.
- **환경 백업 우선**: 의존성 변경(추가/삭제/버전 변경) 전, 현재의 안정적인 환경 상태를 반드시 백업(스냅샷, 명세 추출, 환경 복제 등)합니다.
- **신속한 롤백**: 문제 발생 시 원인 분석보다 "마지막 정상 상태"로의 즉각적인 복귀를 우선합니다.
- **책임의 분리**: 이를 통해 "구현 실패"와 "환경 붕괴"를 명확히 구분하여 불필요한 디버깅 비용을 최소화합니다.

### 3.5. 성공사례 재현 기준 (Reproduction Baseline)
Phase 2의 우선 목표는 과거 성공했던 환경의 구성을 그대로 재현하는 것입니다. 이를 위해 다음 버전 스펙을 "재현 기준선"으로 고정합니다.

**[Core Dependencies Checkpoint]**
- `protobuf==4.25.8` (tensorflow-intel 의존성 자동 선택)
- `mediapipe==0.10.5`
- `tensorflow-cpu==2.13.0` + `tensorflow-intel==2.13.0`
- `deepface==0.0.98`
- `typing_extensions==4.15.0` (pydantic 호환성 보장)

**[재현 원칙]**
1. 이 기준선(성공사례 버전 스펙)에서 먼저 재현을 시도합니다.
2. 이 기준선에서 재현이 실패할 경우에만, 범위 축소 또는 대안 검토를 논의합니다.

## 4. 범위 설정 (Decision Making)

### 4.1. Phase 2에서 확정하는 역할
- **단일 프레임 분석**: 실시간 스트림이 아닌, Playground 요청 단위(Image)의 처리
- **기초 메타데이터 제공**: 얼굴 좌표, 눈 깜빡임, 고개 끄덕임 등의 기초 데이터 생성

### 4.2. 무엇을 '더 만들지 않기로' 결정했는가 (Out of Scope)
1. **고수준 행동 인식**: "긴장해서 손을 떤다" 등의 주관적 해석은 Phase 2 범위 제외
2. **독자적 감정 판단**: MediaPipe를 통한 감정 추론은 Emotion 모듈과 충돌하므로 배제
3. **실시간성 최적화**: 1fps 수준의 처리를 기준으로 하며, 그 이상의 고성능 튜닝은 수행하지 않음

### 4.3. Phase 3 확장 여지
- 시계열적 데이터 누적을 통한 "면접 태도 점수" 산출
- 제스처 인식을 통한 "적극성/소극성" 지표 확장

## 5. 결론: "재현 성공"의 의미
우리가 이 Plan에서 "재현 성공"을 유일한 성공 기준으로 삼는 이유는 명확합니다.
**"이미 작동함이 증명된 코드는 디버깅의 대상이 아니라, 시스템 통합의 대상이기 때문입니다."**

이 Plan은 TASK-010이 기술적 탐구(Research)가 아닌, **시스템 통합(Integration)** 작업임을 명시합니다.
